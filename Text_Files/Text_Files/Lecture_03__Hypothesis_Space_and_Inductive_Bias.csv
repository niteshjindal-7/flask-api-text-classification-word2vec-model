,content,topics
0,,0_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
1,today we will have the first module of machine learning part c i will talk about hypothesis space and inductive bias will give a brief introduction to this so that when we talk about different machine learning out with us we can refer to this,1_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
2,that in inductive learning,2_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
3,aur production,3_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
4,we are given example,4_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
5,on date,5_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
6,and examples are of the four as we have seen x,6_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
7,x for a particular instance X comprises of the values of the different features of that instance and why is the route attribute and we can also think of that has been given its and effect,7_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
8,so issue as you that the output of an instance is a function of the input vector input feature vector and this is a function that we are trying to learn we are given xfx pairs as examples and we want to learn x for classification problem,8_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
9,in the class which of the about two types of supervised learning problems classification and regression depending on whether the output attribute type is a discrete valued or continuous value in classification problem this function effects is described,9_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
10,in regression,10_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
11,the function f x,11_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
12,continuous,12_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
13,we can also about some classification and regression in some cases we may want to find out the probability of a particular value of Y so for those problems where will look at probability estimation,13_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
14,,14_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
15,our effects,15_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
16,if the probability of,16_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
17,,17_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
18,25 types of inductive learning problems that you are looking at why do we call this inductive learning we are given some data and we are trying to do induction to try to identify a function which can explain the data induction as opposed to detect,18_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
19,unless we can see all the instances all the possible data points for we make some restricted as a certain about the language in which type of this is express or some buyers this problem is not well defined so that's why it's called an inductor,19_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
20,,20_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
21,in the last class which talked about features,21_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
22,so when you say we have to learn a function if a function of the features so instances are described in terms of features so features are property,22_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
23,describe each in,23_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
24,and each instance can be described in a quantitative manager using features and often we have multiple features so we have what we use what we call a feature vector,24_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
25,example for a particular instance we may be of particular task we may be describing all the instances in terms of 10 features to the feature that there will be a one dimensional vector of size 10 now based on this week and define of features space,25_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
26,suppose for simplicity let us assume that there are two features and we can,26_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
27,the features are X1 and X2 in general we can have a number of features we have two features the features define a two dimensional space if you have any features that define an n-dimensional space if you take a particular instance that Ascend D1 is an instance and fodder 1 X 1 equal to 2 x 2 equal to 5,27_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
28,sex one is to hear and experience 5 year so this is device ODI one can be thought of as a point in this feature space point in the two-dimensional feature space are you can think of it as a vector in this space to eat instance is a point in the feature space now let's look at a classification problem and Let Us C say that it is a two class classification problem so we are given a number of instances are examples some of which belong to class 1 the others belong to class 2 Class classification problem we have two types of instances those belonging to class 1 and those belonging to class to you are given a training set which comprises a subset of the instances some of them are marks class 1 some of the month plus two and we can say that class one is yours it is and class two as negative so we find that we can map different points in this feature space let us say these are the positive point,28_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
29,and we have some points in this feature space which are negative,29_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
30,now what you want to do if we want to learn a function so that based on the function we want to We want a function to predict whether a new instance which is given to you suppose this is a new instance which is given to you we want to know whether this should be positive and negative in order to do this we have to learn a function or the function could be a particular curve of a line which separates the positive from the negative instances for example the function that we learn put this function and we can say that any point which lies to the side of the function is positive any point which lies to this site of the function is negative and some point lies to the left of the function it is negative inductive learning is about,30_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
31,solar just look at the slide in the slide which have taken from my flight by Jessica Davis of University of Washington we can see a feature space is described in terms of the positive and negative examples the green Plaza the positive points the red - are the negative,31_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
32,this is a particular instance for which X 1.5 x 2 is 2.8 and the label of this instance is,32_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
33,the question mark points at the test points and we are asked to find out what should be the class of those points should be positive and negative in the prediction problem so in order to answer the prediction problem we have to come up with the function for example let's say we come up with this pink function pink line and we say lines points that lie to the right of the pink line is negative points which lie to the left of the pink line is positive so this is so in this case this point and this point will be marked positive and this point and this point will be mark,33_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
34,what is the function that we have come up it and so this is a hypothesis of function that we use to do our predict,34_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
35,we could have instead of this particular line we could have hypothesized other functions so all these are possible functions which we could have found and the set of all such legal functions that we could have come up with their define the hypothesis space in a particular learning problem you Foster find the hypothesis space that is the class of functions that you are going to consider then given the data points you try to come up with the best hypothesis given the data,35_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
36,what about in the last module about how a function is represented so as we have discussed a function,36_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
37,in terms of features,37_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
38,,38_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
39,things that we need in order to describe a function we have to decide the features of the vocabulary and we have to decide the function class or the type of function the language of the function that we will have to be will be,39_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
40,based on the features and the language we can define our hypothesis space,40_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
41,types of Representation have been considered for making predictions for example we just so that we could have a linear function to act as a discriminator between two classes,41_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
42,subsequent class we will look at our representation by using a structure which we called the decision tree where at a decision tree is a tree that every note,42_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
43,decision based on the value of an attribute,43_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
44,and based on that we go to different branches so at every node,44_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
45,condition based on the value of an attribute and every leaf node is labelled by the value of Y decision tree is an type of Representation linear function is one type of Representation you could also have a multivariate linear function you can have neural networks these are some examples of Representation and we have some examples in the slide here a decision tree a linear function,45_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
46,multivariate linear function as single layer perceptron the basic unit of a neural network a multilayer neural network these are some of the Representation that we will talk about later than this,46_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
47,once you have chosen the features and the language or the,47_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
48,functions what you have is a hypothesis space,48_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
49,hypothesis space is the space of All legal hypothesis of set of All legal hypothesis that you can describe using the features that you have chosen and the language that you have chosen and this is the set from which the learning algorithm will pick hypothesis so hypothesis space we may represent the hypothesis space by 8 and the learning algorithm outputs a hypothesis 8 belonging to age this is the output of a learning,49_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
50,2 notes All legal hypothesis all possible output by the learning out of the given the training set given the particular data points the learning algorithm will come up with one of the hypothesis of the hypothesis space which type of this is it comes up it will depend on the data and it also will depend on what type of restrictions are biases that we have imposed which we will describe later so supervised learning we can think of is a device which explores the hypothesis space or which searches for hypothesis space in order to find out one of the hypothesis which satisfy certain,50_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
51,Tamil MP4,51_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
52,we have already talked about an example,52_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
53,xy,53_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
54,value of the input and the value of the output x y pair,54_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
55,training data,55_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
56,is the set of examples,56_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
57,collection of example,57_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
58,which has been observed for the learning outcome of which is input to the learning algorithm we have instant space,58_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
59,aur feature space which describes all possible instances so if we have two features X1 and X2,59_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
60,and that affects one text value between 0 and hundred its two text value between 0 and 50,60_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
61,and all points in this plane can describe an instance this is the instance space for instance space is the set of all possible objects that can be described by the features,61_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
62,,62_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
63,,63_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
64,trying to learn a concept C plus think of a classification problem where we have a particular class that we are trying to learn so let's think of a two class classification problem we can define one of the classes of positive those negative we can think of the positive examples of the concept which we are trying to learn so out of all possible objects that we can describe in the instance space a subset of those objects are positive that is they belong to the concept so the concept,64_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
65,a subset of the instance space X so which define the positive points she is unknown to US and this is what we are trying to find out in order to find out Si we are trying to find a function,65_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
66,is what we're trying to learn what is f f is a function which maps every input.exe to an output why now what is the difference between C and F F is used to is a function used to describe the concept that may be same they may be different because as is Defined by the language and the features that you have chosen as a subtle difference between f and C now what you are trying to do in learning is given a hypothesis space a,66_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
67,trying to come up with the hypothesis,67_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
68,small age belonging to the hypothesis age that approximately if you want to find it,68_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
69,approximate,69_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
70,based on the training data that you have been given now the set of hypothesis that can be produced,70_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
71,for that by specifying a language by the hypothesis space defines all possible set of hypothesis you can restrict,71_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
72,hypothesis,72_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
73,by defining somewhere,73_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
74,specify some constraints on the language or some preferences so bias is of two types,74_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
75,pious can be in terms of constants are the buyers can be in terms of preference,75_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
76,find them more precisely soon but what we mean by constant se suppose you are features are Boolean variable now if you say that you want to consider only Boolean functions which are conjunctions of monomials so that is providing a base of the language if you say that you want a function which is simpler than you are putting a preference,76_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
77,I will talk about this,77_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
78,so given visa definition in a learning problem the input is a training set,78_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
79,s.s. is a subset of the instance space access the instance space which comprises of all possible instances and the training examples that you are given is a subset of this and output you are required to output,79_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
80,hypothesis small age belonging to the hypothesis space capital is so this is for a classification,80_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
81,let me love you,81_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
82,,82_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
83,,83_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
84,,84_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
85,,85_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
86,,86_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
87,let us look at our hypothesis space now suppose we look at functions to take an example we take features which are good,87_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
88,suppose X1 X2 X3 X4 4 features and their Boolean features the value of the features at true or false now if they 4 billion features,88_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
89,how many possible instances can you,89_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
90,particular instance expand can be true or false 0162 can be 0183 can be 02164 can be 0 or 1 so there are 2 to the power 4 9 16 possible in,90_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
91,so number of possible instance,91_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
92,,92_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
93,Deepavali 4 or 6,93_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
94,how many possible functions are there how many Boolean functions are,94_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
95,function function will classify some of the points as positive others as negative out of the 16 points so that means the number of functions is the number of possible subsets of this 16,95_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
96,so how many possible subsets of the there are 2 to the power 16 subsets of 2 to the power 2 to the power 4 subsets in Boolean variable size features if you had and wooden features then the number of possible instances will be 2 to the power and and number of possible functions will be 2 to the power 2 to the power so this is the size of the hypothesis space as you can see the hypothesis space is very large and it is not possible to look at every hypothesis individually in order to select the best hypothesis that you,96_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
97,you put some restrictions on the hypothesis space you can put some restrictions for you select the hypothesis language,97_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
98,language may be an unrestricted language for example all possible Boolean functions or may be restricted language we have seen already some examples of hypothesis languages a decision tree linear functions neural networks at 17 are there could be polynomial function linear function are there could be conjunction Boolean formulas cnf and formulas and restricted bullion formulas so you choose a hypothesis language the hypothesis language,98_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
99,you restrict type of this is language,99_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
100,hypothesis of language reflects abayas,100_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
101,this reflects a bias or inductive bias,101_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
102,,102_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
103,lao Tzu latest define formerly what is inductive,103_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
104,when we choose a hypothesis space we need to make some assumptions and,104_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
105,two types of assumptions that you can make you can put a restrictions on the type of functions that is we can say instead of considering all Boolean formulas we are going to consider only conjunctive Puliya,105_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
106,say that for regression problem you can say that we are looking at linear functions or you can say that we can looking at 4th degree polynomials for any degree polynomial or we can say we look at any polynomial so specifying the form of the function is called restriction bias the second type of bias that you can use this preference buyers were given a particular language that you have chosen you say that,106_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
107,I'm considering all possible polynomials but I will prefer polynomials of lower degree so you can say that I am considering all possible Boolean functions that I want a Boolean function which can be described in small size so you can put different types of bias on your learning,107_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
108,inductive learning means general function from training examples given some training examples you want to generalize so you construct a hypothesis age you are given some training examples which comes from a concept c and you want to find out hypothesis age you can come up with the hypothesis that is consistent with all the training examples given then such type of sources are called consistent,108_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
109,sometimes not possible to come up with a hypothesis for consistent hypothesis and sometimes we will not come up with answer type of the source but even when you are coming up with the consistent hypothesis given a hypothesis space and given a training data multiple possible consistent hypothesis can be there and you are to select which one of them you want to output based on your preference shares the hypothesis that you want to output is most often you are guided by you want to come up with the hypothesis that generalizes well over the unseen,109_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
110,new formula hypothesis based on the training data but you want to come up with an hypothesis that doesn't just do well on the training data path is likely to do well on unseen,110_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
111,inductive learning is an important problem if you do not look at all suppose your hypothesis space is all will inform you,111_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
112,if you do not look at all the to the power n possible examples if you look at a subset of those examples multiple possible hypothesis a possible and they have their will behave differently with the rest of the example so you cannot come up with the correct hypothesis,112_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
113,by logical deals by in a great which is guaranteed to be true without seeing all the training examples of learning is a problem you are looking for generalisation guided by some buyers or some kind,113_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
114,so why you are being able to generalize is based on the assumption we call this assumption the inductive learning,114_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
115,states that a hypothesis age is found to approximate the target function Si well over a sufficiently large set of training examples of if you come up with the hypothesis which has a low training error over a sufficiently large training set you expect that hypothesis to do well on unseen examples so this is the inductive learning High Court,115_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
116,looked upon as searching through the hypothesis space,116_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
117,based on the training examples and the bias that you have imposed there are different types of wires for example 1 classical bias is a bias called occam's Razor occam's Razor states that you will prefer the simple,117_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
118,the principal officers of philosophical principle that if something can be described in a short in a soil in a short language that hypothesis is to be preferred over a more Complex hypothesis and there are other types of inductive bias like minimum description length like maximum margin at 17 which will be only a which will can be explained when we talk about the specific algorithms such processes used in machine learning you have to come up with a good hypothesis space yet to find an algorithm that works well with the hypothesis space if the come up in the hypothesis that works everything that works well with the hypothesis space and outputs an hypothesis that is expected to do well over future data points and you have to understand what is the confidence that you have won the hypothesis and these are the things that we will discuss so machine learning coming up with a function is all about doing generalization and when you doing generalisation you can make some errors and the errors are of two types bias errors and variance Ellis sobai Eso is a restriction on the hypothesis space of the preference in choosing hypothesis by deciding a particular hypothesis you impose a bias so this is error due to incorrect assumptions or restrictions on the hypothesis space the error introduced by that is called bias variance errors introduced when you have a small test set so reinsurer means the model that you estimate from different training sets will differ from each other if you come up with a model from some 50 training 50 data points and you take another 50 letter points on the distribution I come up with a very different model than we say that there is a variance,118_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
119,and this point we will discuss later when we talked about different learning alphabets this is a very important concept but we will talk about when we talk about this is overfitting and underfitting you may come up with the hypothesis that does well over the training examples but does very poorly over the test examples then we say overfitting has occurred overfitting comes from using very complex functions are using too few training data when the reverse of overfitting and underfitting if you have a very simple function then it cannot capture all the nuances of the data so we'll talk about details of overfitting and underfitting when we talk about specific outcomes with this we come to the end of this mode,119_Lecture_03__Hypothesis_Space_and_Inductive_Bias.wav
