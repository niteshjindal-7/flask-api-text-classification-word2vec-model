,content,topics
0,,0_Lecture_33__Deep_Neural_Network.wav
1,good morning so now we have in the last two classes in neural network we have looked at single layer and multilayer neural network and the backpropagation algorithm today we will talk briefly about Deep neural networks are so this is a very new topic and with a lot of interest and I will just give you a brief exposure to certain very basic deep neural network architectures and tell you what it is all about so,1_Lecture_33__Deep_Neural_Network.wav
2,years there have been break they have been a lot of excitement with deep learning deep neural networks and their happen breakthrough results in various domains including starting with image classification then in speech recognition and finally in natural language processing like machine translation multi-modal learning developing text from speech text from images automatically and many such and the excitement continents I'll talk briefly about what is a deep neural network so and deep neural network in a very simply speaking is a network which is Deep that is which has many hidden layers we have seen that in a we have feedforward that works with hidden layers normally in a normal that work what use to happen is that it we also talked about in the last class that a network with two hidden layers is enough to represent any function but it may not be easy to learn learn all Complex functions using a network into layers as you increase the layers in the network it is easier to be able to represent complex function in terms of simple function,2_Lecture_33__Deep_Neural_Network.wav
3,you can have a network with many hidden layers and you can call this a deep network why was it not an attractive proposition to start with people found that when you have a deep that work the number of the error surface becomes very rough and there is a lot of local cinema so you cannot always converts the convergence is extremely slow slow slow training and very important that was one condition suppose you have this input and you have the different hidden layers 81 82 83 84 and then you have out now we have connection from this layer to this layer this layer to this layer this back propagation the error output was propagated here and this error the backpropagation error was propagated here here now there are a few things most of the error based on this and these weights rising changed as we go further here,3_Lecture_33__Deep_Neural_Network.wav
4,they would be less in a less updates of the weights in the initial and one of the reasons was diffusion of gradient when you do gradient Descent you find the error gradient and gradient Descent part the gradient can get very diffused get activated as you go deeper from the output input when you use a function like the sigmoid function in look at the sigmoid function in the sigmoid function in most of the region's gradient is almost close to zero only in this region are the gradient has the reason that most of the regions the gradient is close to zero is a small value and as you are taking proper getting them back was a gradient becomes attenuated and very close to zero and very little very little change was happening here they can also be in certain cases and can increase very much they are also there is a problem which can be handled by simpler method that great and clipping diffusion of gradient is a problem and the slowness of convergence is a problem when you use a deep,4_Lecture_33__Deep_Neural_Network.wav
5,and prove that came in using deep layer network is because people came up with methods which could handle this problem in various ways some of which we will discuss today for letter C Y keep representation is good in many tasks hierarchical representation help represent Complex functions for example in natural language processing we have an input sentence and we can look at first sight and define the words we can look at the part of speech on morphology of the word which can then identify chunks for small phrases than larger Phrases and clauses and then the sentence when we can process it in this order in the NLP pipeline when you doing image processing you can initially find out the ages in the image on the pixels find the ages then the text on the motives and parts then objects and if you do it had done quickly the process may become simpler 20 planning we are basically learning a hierarchy of internal representation and think of that the notes that in every hidden layer are representing certain features that this is the input these are some low level features and these features are composed of this low level features a for the composed for the composer for the Corpus so you have a hierarchy of low-level to higher level features and the learnt internal representation can be thought of as features and we can say that these hidden layers are acting as feature extract deep learning can be looked upon as a feature learning and one way of look cooking at it is that we have the input and here we have a trainable feature its,5_Lecture_33__Deep_Neural_Network.wav
6,call feature extractor,6_Lecture_33__Deep_Neural_Network.wav
7,and finally at the output we have a class,7_Lecture_33__Deep_Neural_Network.wav
8,if you have a classification problem,8_Lecture_33__Deep_Neural_Network.wav
9,one of the ways in which deep networks can be trained is by using unsupervised pretend there are several ways one is that you could try to setup the units so that the gradient diffusion or gradient blowing a problem can get reduced and there are some ways of using activation functions other than sigmoid function for example people use a function called which will see which avoids the problem of gradient diffusion The Other approaches using unsupervised pre training so instead of learning and deep Network at one go we learn the network one step at a time using a not to,9_Lecture_33__Deep_Neural_Network.wav
10,and by growing the layers one at a time we can avoid the problem which deep networks have if you look at the slide are we show the schematic of doing great deal a device pre-training this is the input the first learn the first hidden,10_Lecture_33__Deep_Neural_Network.wav
11,by using some mechanism one of the mechanism is auto-encoder which I will discuss briefly today and other is by using restricted Boltzmann machine by several such approaches is possible to first learn this layer when you learn this layer you fix this layer after you fix this layer in terms of this you learn the next then you fix it then you learn the next year and so on so here you are fixing the parameters of the previous hidden layers and you are assuming that these are different features and then you are trying to find the next place so this is the idea of green tea layer-wise free training after have learnt the layers individually finally you can look at the classification problem and based on the training example you can set up the classified at the last layer of the output layer and then consume the whole network backpropagation initially you do great deal Airways training finally you put the classification problem using the training examples and then tune the entire network so this is one scheme which is followed by stacked restricted Boltzmann machine and start auto,11_Lecture_33__Deep_Neural_Network.wav
12,neural network so what we will look at is feed forward neural network stacked autoencoder or which will talk about briefly statue restricted Boltzmann machine will not talked about in this class that's another approach and very briefly will talk about convolutional neural,12_Lecture_33__Deep_Neural_Network.wav
13,example of a deep architecture a multilayer perceptron we have the output layer this is a input layer comprising of RAW sensory input these are three hidden layers which learn more abstract representations as you go upwards and finally this is the,13_Lecture_33__Deep_Neural_Network.wav
14,now we have seen a neural network desert back propagation of error talked about it and when you train the blood works then there are some problems the early layers of the network to not get trained well because there is a diffusion of gradient error at invites as it propagates to earlier layers it has very slow training and the error too early LS drops quickly as the top players was please solve the task so not much update happens in the beginning layers and that is often not enough label date,14_Lecture_33__Deep_Neural_Network.wav
15,if you have a deep network it will have many parameters when you have we have seen that if you have too many parameters to Hue training example then it may lead to overfitting however you can take advantage of unlabeled data instead of using only labelled data which may be restricted in number you can take advantage of unlabeled data and this is what some of these approaches to Deep networks tend to have more local minima problems than Shallow networks so these are some problems with it,15_Lecture_33__Deep_Neural_Network.wav
16,now as I said that you talked about the sigmoid function is also the tanh function relu function softplus function gives us some other activation function so this blue line is the sigmoid function which have discussed is between 0 and 1 that and each function is also an at step function similar to the sigmoid function its ranges from -1,16_Lecture_33__Deep_Neural_Network.wav
17,the release function the red function value function is zero when the input is less than zero and after that it is a linear,17_Lecture_33__Deep_Neural_Network.wav
18,this is the function and its blue light blue function is the softmax function which is a smooth function similar to the relu function and these functions are unbounded when the input is high then becomes very large and these functions have recently been used to overcome the diffusion of gradient problem in deep neural that it's so that and it's sigmoid function be about the same tanh function is given by this equation so this is that energy function this is the sigmoid function and this is a value function which is given by Maxim 0x it simplifies that propagation makes learning fast,18_Lecture_33__Deep_Neural_Network.wav
19,and this is often preferred option when you have many hidden layers,19_Lecture_33__Deep_Neural_Network.wav
20,briefly talk about the autoencoder suppose you have an label state trading examples X1 X2 X3,20_Lecture_33__Deep_Neural_Network.wav
21,which are all real valued better now in autoencoder what we will do is that we will learn the initial hidden layers based on the unlabeled units and labelled input only how to reset it up suppose this is the input x i this is my hidden,21_Lecture_33__Deep_Neural_Network.wav
22,player H1 penal setup problem where excise the input and its output so we will set up a network to learn the identity,22_Lecture_33__Deep_Neural_Network.wav
23,and we will have connections from here to here to hear so you can see that this age one will act as a representation of the input because some H1 the input can be reconstructed now you may be tempted to think that we can have a very simple trivial function with just takes each unit and transfers in there but we want to prevent it using some country we want to put something strange so that each one Learns some interesting function at the hidden and not a trivial function into this functions will be a representation of the input but they will be non-trivial by using some constitutes so we'll discuss about this but once you learn this auto-encoder we have basically a neural network with one hidden layer this is easy to train and then we fix the weight so we fix this weights w1 now be removed and now with think of given the input we have this value of 81 and the setup and other learning problem where we have the hidden layer h2so this 81 we have fixed this w1b have fixed this we have fixed and we set up H2 and we put H1 at the output so every training example we find the value of each one and the same value we put here we generate the stunning example and then we learn these weights with letters call them w2 and after that we will fix this year and this wait and we will remove this so we will learn layer-by-layer similarly we will learn 83 and,23_Lecture_33__Deep_Neural_Network.wav
24,finally after we have finished learning with unsupervised layers in a layer wise approach now with respect to each training example will find the value at age 3 and will bring in the output,24_Lecture_33__Deep_Neural_Network.wav
25,40 output will train at Ulla neural network or some other function like SVM order linear regression will train sum function and then we will get these wastes w4,25_Lecture_33__Deep_Neural_Network.wav
26,up to this point using only unlabeled data here we bring in the labelled it then we will get a deep neural network this feeding in the input output from here so we have an example we set the target values to be equal to the input X1 Hera X 1 x x x year and then which train the first hidden layer so that the hidden layer represents now as I said the solution to this auto-encoder maybe trivial and we want to put some constraints in order to prevent it one of the constraints could be that in order to prevent the trivial mapping we can set the number of nodes in the hidden layer to be much smaller than the number of inputs and weekend constraint that support the input has 100 dimensions we can put 20 dimension in the hidden layer and its function can be represented by 20 dimension that is a non-trivial function so that could be one constraints we put another very popular constraint that we use is the sparsity country ine sparsity constraint for sparsity constant first of all with respect to one neurone we say that the neurone is active if its output value is closed,26_Lecture_33__Deep_Neural_Network.wav
27,if output is close to one now among all the training examples for some of the training examples in urine will we can put a constraint on the fraction of training examples on which the neurone can be active so that is we constrain the neurone to be inactive most of the time and to be active only for some training exam which have certain commonalities so we can impose the sparsity constraint on the this network and we can learn the weights which satisfy the sparsity constraint thus preventing the trivial mapping to the lord this is another way of imposing a country and we are as we saw that we are starting different layers it has been found that the sparse autoencoder can be stacked easily where is the previous constraint very constrain the number of hidden units are they do not after they know they are not so much am unable to start right so we can add such constraints and then we can put the input your input your learn the first aid and their we can fix the way from the input to the first hidden layer and then we can after fixing the weights then at the end we can learn the classified of course we can do more than one,27_Lecture_33__Deep_Neural_Network.wav
28,and by stacking autoencoder we can use more than one hidden layer this is the first aid and then there is a second hidden there should be true supervisors So we do layer by layer using unlabelled examples finally we do supervisors training on the outside output layer and then we tune the entire network that auto-encoder and this is one way of learning and deep network where we avoid the problems for standard problems that were there with the network,28_Lecture_33__Deep_Neural_Network.wav
29,ok there are other as I said we have stacked RBM where we use energy minimization to learn layer by layer using bi-directional during that work that we will not discuss in this class we will be briefly talk about another approach to Deep neural networks using convolutional,29_Lecture_33__Deep_Neural_Network.wav
30,convolutional neural network has been very popular for working with images and now it has also been applied to other areas including Natural Language Processing so convolutional neural network consists of a number of layers and these layers have some specific functions there are two types of players which are very popular convolutional layer and subsequently or pooling layer that inputs to convolutional layer is an image in pixels and for each pixel we may have a number of channels for example if it's an RGB image there are three channels for the number of inputs corresponding to the image is M bi m y and the convolutional layer we use ke filters so we use ke function,30_Lecture_33__Deep_Neural_Network.wav
31,and each of these functions,31_Lecture_33__Deep_Neural_Network.wav
32,will do a mapping for letters explain how these functions convolution function works for simplicity let us assume we have a n y m and there is only one solution what we will to is that it will take a rectangle with 4 by 4 sub,32_Lecture_33__Deep_Neural_Network.wav
33,then we can take different 4 by 4 subject and,33_Lecture_33__Deep_Neural_Network.wav
34,the corresponding to this sub rectangle if we have 16 inputs and this 16 input,34_Lecture_33__Deep_Neural_Network.wav
35,feeds to this can this is Keval is one feature and if there are 112 w 1616 weight this is an output now what we will do is that for other other Sab rectangles also will go to this function K1 and they will also share the same width w1 w2 W3 so what we do is that in convolution for every note we will for every rectangle we will so for a particular kernel that we are using for every rectangle we will find the function and dysfunction the weights of this function will be the same width as we are using for all this subject and so this is the convolution so we are taking a mask and fun valve thing it to every position of the image and we get the illusion now because the weights are shared the number of parameters is less so you can think of your trying to find a particular pattern in the pattern could be in this place of the image of this disorder and the output of the solution there is any by and by so I can be equal to M or less than and q is the number of channels here are a number of Janu and he can have instead of looking at the Sab rectangle we can think of a volume we can look at Vol 2 the Conclusion and get the output which is and by and by 2 and we can have several search filters which we call coronavirus this is one function ke one we can add another function Kiya To which will have a difference,35_Lecture_33__Deep_Neural_Network.wav
36,,36_Lecture_33__Deep_Neural_Network.wav
37,so we can look at this picture here so we have this sum rectangle for some volume and for one convolution layer via computing this function and this function will share the wait between the different local,37_Lecture_33__Deep_Neural_Network.wav
38,what we can do is that we can do a pulling we want to know if this particular pattern occurs anywhere in the image so we can take for K1 we will have output from different parts of the image then some of these outputs can be combined using maths or average,38_Lecture_33__Deep_Neural_Network.wav
39,Max fitness Max Pune suppose we take this region at this filter for response to a fun if age is there now that it can be here in the image here in the image image techmax it means that the edge occur anywhere,39_Lecture_33__Deep_Neural_Network.wav
40,so we can do some pulling and so that we can take this and by and by q and reduce it,40_Lecture_33__Deep_Neural_Network.wav
41,pulling up sampling what it does it take to output of a function and combines them into smaller value so convolutional layer consists of rectangular grid of neurones each neurone text input from a rectangular section of the previous player the weights of the rectangular section at the same for each neurone in the convolutional,41_Lecture_33__Deep_Neural_Network.wav
42,pulling we use features obtained after convolution the pooling layer takes small rectangular blocks from the convolutional layer and Sab sample set to produce a single input from that blocked for example after convolution these are the values that you get now we can divide it into small Sab rectangle and for each rectangle we can do a pulling for example from this sab rectangle with by Max pooling biggest six from this Max pooling uses 8 from this Max pooling is S3 from this Max pooling resources for sore after pulling we reduce the image size from this to,42_Lecture_33__Deep_Neural_Network.wav
43,I know this is digits learning in recognition of handwritten digits task we have any way we do a convolution and we use 5 features an input images 32b 32b get 528 by 28 images after convolution then we do to into to sab sampling so we take it into two neighbouring rectangle doing Max pooling and then reduce it to 14 by 14 then we again Jo convolution 5 by 5 convolution and we get 10 by 10 feature maps and again to Saptami so we can have a convolution and Sab sampling layers and then they can be start together we can also have optionally some layers which do fully connected feedforward neural network like we're finally finally we have the classification of this is one of the architecture one of the of a configuration of a convolutional neural network which consists of convolution layer pooling layer fully connected layer of course there will be there and that this place will be start together pulling your may not be there and the fully connected layer,43_Lecture_33__Deep_Neural_Network.wav
44,some properties of convolutional neural network the convolutional neural network takes advantage of the substructure of the input which is achieved with local connections and weights so look for locally the convolution is over a local rectangle so it looks at some feature in a local region and the same feature it tries to find in different regions and this is achieved by time the weights for using the same weights for this rectangle as well as this time we use the same weight,44_Lecture_33__Deep_Neural_Network.wav
45,so this results in translation in variant features convolutional neural networks are easier to train and they have many fewer parameters than the corresponding fully connected network because the weights are shared ok so this makes convolutional neural network easier to learn,45_Lecture_33__Deep_Neural_Network.wav
46,many other deep neural network architectures of course will not be able to cover everything I will just mention the recurrent neural network which are very useful for representing temporal sequence,46_Lecture_33__Deep_Neural_Network.wav
47,as in speech or sequences of words in a sentence videos at 17 in recurrent neural network what we have is that we have a input and output and then the hidden,47_Lecture_33__Deep_Neural_Network.wav
48,we want to capture the input from the previous time steps now one of the ways in which people who is that we could have the input at xt60 -1 itzy -2 -3 pic size window and treat it as the input and then we have a hidden and we could have the output these constraints are from taking input from a sliding window alternately we could take the input theoretically from any time in the past infinitely back in the past by having a connection from H2,48_Lecture_33__Deep_Neural_Network.wav
49,producers XT uses HT and we can have a connection from h to itself which corresponds to if you look at this picture which corresponds to an unfolded and have this is extra -1 XT XT plus one and this is the hidden state at the minus one,49_Lecture_33__Deep_Neural_Network.wav
50,and these weights are shared this this is the common wait and await you this and this is the recurrent neural network and you can unfolded infinity to get a very deep neural network and you can use backpropagation the particular term we use backpropagation over time to find the value of the weights now recurrent neural network have similar problem as deep neural networks have with respect to back propagation and one of the very nice ideas people have come up with is used some,50_Lecture_33__Deep_Neural_Network.wav
51,units instead of using simple perceptrons we use certain get by which one can store which can act as a memory where one can store some information so that that information stays till when you want to using so information which is long back in the past can be stored using this gated units and there are various such units which have been used a less CM which stands for long short term memory for G are you with X forget it recurrent your it is a sum of the units which are used in recurrent neural network to enable them to work effectively and fortunately or today we do not have much scope in this class to talk about this and this is a little more Complex a topic which we will not have time to go into detail but I just wanted to give you a glimpse into this so that in future you can read this entire topic of today's is a little bit and once but we wanted to tell you a little bit about this there are many are very nice architectures of the neural networks like encoder-decoder architectures and there are different models using models which use external memory and is a very exciting and they have been used for solving extremely interesting tasks I hope that he will have some interest and be able to study them later thank,51_Lecture_33__Deep_Neural_Network.wav
