,content,topics
0,,0_Lecture_13__Feature_Selection.wav
1,today we will cover up in the today we are in the first third module which is on instance based learning and feature reduction we have the part B of this modern where we will talk,1_Lecture_13__Feature_Selection.wav
2,class we have looked at instance based learning and we have seen that in instance based learning given that test data instance we have to find out the nearby instances for this we need the distance function this distance function is computed in terms of the features if the number of features is large there is a problem because the distance that you get is not may not be representative of the actual distance through this is the reason why feature reduction become,2_Lecture_13__Feature_Selection.wav
3,we have seen that are you know that information about the target so features can't,3_Lecture_13__Feature_Selection.wav
4,about the,4_Lecture_13__Feature_Selection.wav
5,function classification function is defined in terms of the features so you may be tempted to think that more features means better information or more,5_Lecture_13__Feature_Selection.wav
6,better discriminative power or better classification,6_Lecture_13__Feature_Selection.wav
7,is this really the case always we will see that this is not always but this may not hold always that just because your more features does not mean you have more information or better class,7_Lecture_13__Feature_Selection.wav
8,,8_Lecture_13__Feature_Selection.wav
9,one typical scenario as if you keep the number of training examples fixed and the training set is not extremely large then typically what me,9_Lecture_13__Feature_Selection.wav
10,number of features initially the classified performance Megha and then the classified,10_Lecture_13__Feature_Selection.wav
11,why why can it be so the reason is features can be,11_Lecture_13__Feature_Selection.wav
12,,12_Lecture_13__Feature_Selection.wav
13,K nearest neighbour please use elephant features introduce noise and,13_Lecture_13__Feature_Selection.wav
14,trying to find which instances are close together this is relevant features or noise features will make this make the,14_Lecture_13__Feature_Selection.wav
15,you may have read an,15_Lecture_13__Feature_Selection.wav
16,training examples and redundant features which do not contribute additional information they may lead to degradation in performance of the learner,16_Lecture_13__Feature_Selection.wav
17,elephant features and easy Dun redundant features can confuse and especially when you have limited training documents limited free,17_Lecture_13__Feature_Selection.wav
18,computation,18_Lecture_13__Feature_Selection.wav
19,,19_Lecture_13__Feature_Selection.wav
20,space by positive space will be larger and searching may take more time depending on the,20_Lecture_13__Feature_Selection.wav
21,with Limited training examples you cannot work with a large number of features because we have seen that it leads to,21_Lecture_13__Feature_Selection.wav
22,these things contributes to a phenomena which we called the Curse of dimensionality,22_Lecture_13__Feature_Selection.wav
23,when you have too many features too many diamond,23_Lecture_13__Feature_Selection.wav
24,lead to degradation of the learning out more computational time and distance omina is called because,24_Lecture_13__Feature_Selection.wav
25,Curse of dimensionality we want to do feature reduction two types of feature reduction one is called feature selection that is called feature extraction,25_Lecture_13__Feature_Selection.wav
26,,26_Lecture_13__Feature_Selection.wav
27,election what we do is that given an initial set of features as equal to X 1 X 2 X and we are given initially and number of features we want to find the subset of prime which is a subset of f equal to,27_Lecture_13__Feature_Selection.wav
28,,28_Lecture_13__Feature_Selection.wav
29,Prime One X Prime,29_Lecture_13__Feature_Selection.wav
30,,30_Lecture_13__Feature_Selection.wav
31,find a subset of those features so that it optimises certain criteria so what do you want to optimise,31_Lecture_13__Feature_Selection.wav
32,show me the first talk about the next method of feature reduction which is called feature x,32_Lecture_13__Feature_Selection.wav
33,feature extraction in the next class but what feature extraction does is,33_Lecture_13__Feature_Selection.wav
34,transforms or projects the original set of features into a new substance which has smaller,34_Lecture_13__Feature_Selection.wav
35,dial it,35_Lecture_13__Feature_Selection.wav
36,,36_Lecture_13__Feature_Selection.wav
37,M less than end,37_Lecture_13__Feature_Selection.wav
38,feature selection you select a subset of the features will talk about feature extraction in the next class so the import these cases what you are seeking to optimise if you want to either improve or maintain,38_Lecture_13__Feature_Selection.wav
39,location accuracy,39_Lecture_13__Feature_Selection.wav
40,5 classified,40_Lecture_13__Feature_Selection.wav
41,Vishwas which showed that as you increase the number of each,41_Lecture_13__Feature_Selection.wav
42,classification accuracy initially increases and then reduces it could also be the case that the classification accuracy increases and then remains the same so you want to find the small number of features which either improve classification accuracy or maintains the same acuris accuracy and it simplifies the complexity of the classified today weather reasons why we do feature selection now that feature selection we want to select a subset of the original features set now let's see how we can select that some,42_Lecture_13__Feature_Selection.wav
43,you can see that if we have any teachers then number of subsets,43_Lecture_13__Feature_Selection.wav
44,,44_Lecture_13__Feature_Selection.wav
45,new Marathi each of this possible subsets and check how good it is because the number of subsets is exponential so we need a method which works in reasonable time the method that we can use for feature subset selection can be optimum method,45_Lecture_13__Feature_Selection.wav
46,,46_Lecture_13__Feature_Selection.wav
47,,47_Lecture_13__Feature_Selection.wav
48,we can use optimum method if the hypothesis space for the feature subset space has a structure so that we can have a optimum algorithm which works polynomial time otherwise we can use accurate sticker greedy algorithm or some type of Run,48_Lecture_13__Feature_Selection.wav
49,which considers the different feature subset will also have some mechanism to evaluate,49_Lecture_13__Feature_Selection.wav
50,,50_Lecture_13__Feature_Selection.wav
51,types of method,51_Lecture_13__Feature_Selection.wav
52,in detail unsupervised methods and superb,52_Lecture_13__Feature_Selection.wav
53,methods we are,53_Lecture_13__Feature_Selection.wav
54,over,54_Lecture_13__Feature_Selection.wav
55,content in some way in an unsupervised way these methods are called filter,55_Lecture_13__Feature_Selection.wav
56,is method also called to wrapper method,56_Lecture_13__Feature_Selection.wav
57,what the features of set,57_Lecture_13__Feature_Selection.wav
58,physical supervised methods or wrapper method,58_Lecture_13__Feature_Selection.wav
59,latest look,59_Lecture_13__Feature_Selection.wav
60,so let's look at as I said the feature selection is an Optimisation problem you have a search algorithm have an objective function and you are trying to optimise,60_Lecture_13__Feature_Selection.wav
61,here the search algorithm will select a feature subset and score it on the objective function and find the goodness of the feature success based on this it will decide which part of the search space,61_Lecture_13__Feature_Selection.wav
62,this module is completed you get a final feature subset and this final feature subset is used by your machine learning or pattern,62_Lecture_13__Feature_Selection.wav
63,optimal on near optimal with respect to the object,63_Lecture_13__Feature_Selection.wav
64,,64_Lecture_13__Feature_Selection.wav
65,by two methods in supervised methods or wrapper methods We trained using the selected subset and we estimate the error on the validation set in unsupervised of filter methods will look at only the input and we select the substance which has the,65_Lecture_13__Feature_Selection.wav
66,types of methods illustrated,66_Lecture_13__Feature_Selection.wav
67,,67_Lecture_13__Feature_Selection.wav
68,comes up with a feature subset that is evaluated for information contact and based on that search algorithm process and finally this module gives a fine,68_Lecture_13__Feature_Selection.wav
69,method of supervised method the search algorithm outputs a feature subset which is again used with a pattern recognition and machine learning algorithm and the prediction accuracy is obtained which is fed to the surcharge and after the phone module is completed you have a feature subset,69_Lecture_13__Feature_Selection.wav
70,different methods of two different frameworks of feature selection now how do you do the feature selection after so first of all what you can do is,70_Lecture_13__Feature_Selection.wav
71,features of redundant you may not use all the features so you can find an cold,71_Lecture_13__Feature_Selection.wav
72,some features and then when you try to introduce the new feature you do not take the teacher if that feature is highly correlated with another feature which already contains the information about that feature so you select only uncorrelated,72_Lecture_13__Feature_Selection.wav
73,so you have to select encoder,73_Lecture_13__Feature_Selection.wav
74,also have to eliminate retarded,74_Lecture_13__Feature_Selection.wav
75,that we can use to frame words for your district a simple heuristic algorithm for feature subset selection that is forward selection algorithm and backwards,75_Lecture_13__Feature_Selection.wav
76,,76_Lecture_13__Feature_Selection.wav
77,forward selection algorithm you start with empty pictures set and we add feature one at a time so you start from an empty set of features that lets first write about the forward select,77_Lecture_13__Feature_Selection.wav
78,,78_Lecture_13__Feature_Selection.wav
79,start with empty features,79_Lecture_13__Feature_Selection.wav
80,Aaj features one by,80_Lecture_13__Feature_Selection.wav
81,try each of the remaining fees,81_Lecture_13__Feature_Selection.wav
82,them you estimate that you,82_Lecture_13__Feature_Selection.wav
83,the classification or regression,83_Lecture_13__Feature_Selection.wav
84,each specific,84_Lecture_13__Feature_Selection.wav
85,teacher,85_Lecture_13__Feature_Selection.wav
86,gives maximum in,86_Lecture_13__Feature_Selection.wav
87,improvement you can use the validation set and not the training set as we have discussed earlier so you can stop when there is no significant,87_Lecture_13__Feature_Selection.wav
88,MPPSC set at features one by one when you add a feature you try to evaluate the result of adding all the features and select that features whose addition next the maximum improvement and you continue this process until adding a feature does not give rise to,88_Lecture_13__Feature_Selection.wav
89,half backward search,89_Lecture_13__Feature_Selection.wav
90,backward search what should you start with in backward search you start with the full features,90_Lecture_13__Feature_Selection.wav
91,you try removing,91_Lecture_13__Feature_Selection.wav
92,try removing feature,92_Lecture_13__Feature_Selection.wav
93,features that you have to find that feature whose removal gives rise to maximum improvement in of,93_Lecture_13__Feature_Selection.wav
94,you try removing it drop the feature,94_Lecture_13__Feature_Selection.wav
95,you dropped the feature with the smallest,95_Lecture_13__Feature_Selection.wav
96,smallest impact,96_Lecture_13__Feature_Selection.wav
97,for backward search to find the feature,97_Lecture_13__Feature_Selection.wav
98,selection Method can we univariate method which look at one feature at a time or can we multivariate feature in univariate features methods so why we who is that we look at each feature independently of the other features for this week and look for some different methods for example Pearson,98_Lecture_13__Feature_Selection.wav
99,4 types ware signal-to-noise ratio mutual information at 17 and based on the method that will select we will write the features by importance and the user will select the cutoff and we will take the top and features about the cutoff for top features top few features of features which have rank above the cutoff I will just talked about one or two up,99_Lecture_13__Feature_Selection.wav
100,method basically what they do is that the measures some type of correlation between two random variables in this case a measuring the correlation between or feature between a particular feature and the target variable and the key features which have higher,100_Lecture_13__Feature_Selection.wav
101,correlation Coefficient measures the correlation correlation is given by the following,101_Lecture_13__Feature_Selection.wav
102,Sigma over all the training examples,102_Lecture_13__Feature_Selection.wav
103,random variable exercise which is a feature why bar is the random variable average of the random,103_Lecture_13__Feature_Selection.wav
104,value is between + 1 and -1 + 5 means perfect positive correlation -1 means perfect negative correlation both Plus and minus 1 + 2 + 1 and -1 means highly correlated closer to zero means no correlation so you want to select features switch off hai positive for hai,104_Lecture_13__Feature_Selection.wav
105,,105_Lecture_13__Feature_Selection.wav
106,,106_Lecture_13__Feature_Selection.wav
107,points correlation is -1 for this point correlation is between 0 and -1 for these points for relation is between,107_Lecture_13__Feature_Selection.wav
108,and for 30 points,108_Lecture_13__Feature_Selection.wav
109,relation Coefficient briefly mention another method for signal to noise ratio which measures the difference in means divided by the difference in standard deviation between the two classes and the formula is given by X - 2y divided by x minus 1 by and large values indicate a strong correlation so for every feature you can find the value of a seminar on with Y and then sort them according to this value and select the largest values or those values which are ab,109_Lecture_13__Feature_Selection.wav
110,to the univariate selection methods we also have multivariate selection methods in multivariate selection methods they consider all the features for example we have talked about linear regression for Linear regression will find a set of weights we are in our class we refer to those with beta 0 Beta 1 Beta 2 Beta in which are the coefficients of the n attributes on the value of the bias the classification of a point is given by,110_Lecture_13__Feature_Selection.wav
111,mission of the line and the coefficient matrix is given by w transport you look at the values of this Matrix if w is small the corresponding feature if you have w i x l and w small excise has less contribution to Wi-Fi but if w is large XI has large contribution to buy effexor w i small letters,111_Lecture_13__Feature_Selection.wav
112,example if w values are 10.01 -9 the ones whose the X features of coefficients at 10 and -9 they have high contribution to the output but the one with Coefficient point zero one has low,112_Lecture_13__Feature_Selection.wav
113,,113_Lecture_13__Feature_Selection.wav
114,,114_Lecture_13__Feature_Selection.wav
115,the w can be obtained by any of linear classifier for example we have seen a method in our class a variant of this multivariate feature selection approach is called recursive feature elimination in recursive feature elimination to compute the w on all the features and you remove the feature with the smallest,115_Lecture_13__Feature_Selection.wav
116,w on the new state university and features you remove the creature with the smallest w you are left with n -1 features on this and -1 features you recompute w by invoking algorithm again and recursively you keep doing this until the store,116_Lecture_13__Feature_Selection.wav
117,multivariate feature selection with us we come to the end of today's lecture in the next volume which part we will talk about feature extraction,117_Lecture_13__Feature_Selection.wav
