,content,topics
0,,0_Lecture_20___Bayesian_Network.wav
1,good morning today we will start with the fourth module of our lecture on probability and bayesian learning we have looked at the basics of probability and we have talked about bayesian approach to learning you have talked about base map classification base of the well classified and we have also look that nice now as we have seen the base optimal classifier is not practical to apply it because one has to apply all the classifiers to a particular problem also if we look at the entire joint distribution of the probabilities involving all the variables the problem becomes intractable because the number of joint distribution is too large to learn or to represent we have seen that nice base makes very very strict active assumptions but it is a very simple very fast easy to use learning out and we have also considered that it works well under certain situations but what we are going to study today is someone in the middle which is called the base network approach,1_Lecture_20___Bayesian_Network.wav
2,base of belief networks is a type of graphical model in fact it is a type of directed graphical model 13 also other types of graphical model which represents certain Independence relations are conditional Independence relationships between the different variables in that open this helps us in to make tractable inference in many cases where all the variables are not completely connected or connect completely dependent with each other this is a vast topic and we will just introduce what is bayesian network and we'll talk a little bit about that but for more details you have to study on your own or take up more advanced course now so what is network represents is that we have variables propose X1 X2 X3 and other variables of interest in the dormant why is also can be included in as one of the various we have the different variables in the domain and we want to look at that types of dependence on other Independence is between them and so we can think of these variables are represented by notes so we use a graphical notation to represent the relations especially the Independence relations between the variable these variables are the notes and we have arcs,2_Lecture_20___Bayesian_Network.wav
3,these are present relations between the variables the lack of arcsde not the lack of relation between the variables in certain ways,3_Lecture_20___Bayesian_Network.wav
4,we also have we have notes we have arts and they represent conditional dependence conditional Independence relationships,4_Lecture_20___Bayesian_Network.wav
5,,5_Lecture_20___Bayesian_Network.wav
6,it is not essential we can represent in bayesian networks causality causality can be represented so among the different variables in the domain some of these variables may be causes of the Other some may be effects of the Other so if you represent this causality we get a compact bayesian network structure we can still have a bayesian network without representing causality but the representation of causality makes the structure of the bayesian network more efficient so what we will do is that they will know that and example of a bayesian network so let's say that,6_Lecture_20___Bayesian_Network.wav
7,this is a note corresponding to the variable late wake up this is a known corresponding to late for work,7_Lecture_20___Bayesian_Network.wav
8,this is unknown corresponding to accident on the highway,8_Lecture_20___Bayesian_Network.wav
9,this is a note corresponding to a rainy day,9_Lecture_20___Bayesian_Network.wav
10,this is a note corresponding to traffic jam,10_Lecture_20___Bayesian_Network.wav
11,this is a note corresponding to meeting postponed,11_Lecture_20___Bayesian_Network.wav
12,note corresponding to Lake for Neet,12_Lecture_20___Bayesian_Network.wav
13,suppose these are the variables which are of our interests and we want to see what is the relation between the various lete se waking up late influences whether you are late for work if there is accident it is more likely that there is a traffic jam if it's a rainy day it's like to be our traffic jam and maybe we can say if it is a rainy day is a higher likelihood for an axe,13_Lecture_20___Bayesian_Network.wav
14,you are caught in a traffic jam its effects whether you are late for work if you are late for work it affects whether you are late for a meeting and if the meeting is postponed that also influences whether you are late for the meeting now please ask that I have drawn the note causal relationships between this variables and from this structure that we get which is a graph or more specifically directed acyclic graph,14_Lecture_20___Bayesian_Network.wav
15,directed acyclic graph which represents the notes and the relationships between them and from this network we can read different conditional Independence relationship example by whether you have woken up late but if you are late for work that directly influences whether you are late for meeting and therefore we can say that if you wake up late if you wake up late that affects whether you are late for the meeting however if you know that,15_Lecture_20___Bayesian_Network.wav
16,person is late for work not suppose you are not late for work in spite of waking up late if you are not late for work in spite of waking up late then whether your work in up late or not does not influence whether you are left for the meeting if you know whether you are late for work or not so waking up late and being late for meeting are normally not independent but they are conditionally independent given that you know whether you are late for work,16_Lecture_20___Bayesian_Network.wav
17,again makeup look at,17_Lecture_20___Bayesian_Network.wav
18,accident whether an Accident happened and whether you woke up,18_Lecture_20___Bayesian_Network.wav
19,so you think of weather at present happens weather cuplate these two variables are independent if you are woken up late,19_Lecture_20___Bayesian_Network.wav
20,that is independent of whether an Accident happened but if you know that you are late for work then these two variables do not remain independent if the value of this not for example you know that you are late for work you did not wake up then reason why you are late for work so these two variables waking up late and accident are not independent if you are given the value of this,20_Lecture_20___Bayesian_Network.wav
21,,21_Lecture_20___Bayesian_Network.wav
22,so similar late for work and waiting for spot that independent but they are not independent if this values of this particular graphical representation and codes certain conditional Independence relationships Tera three specific conditions of The Separation I will not covered them in detail we will just look at some examples in this class will look at formally what a bayesian network,22_Lecture_20___Bayesian_Network.wav
23,bayesian network is a graphical representation which represents efficiently,23_Lecture_20___Bayesian_Network.wav
24,so this is an efficient representation of the joint probability distribution of the variable,24_Lecture_20___Bayesian_Network.wav
25,,25_Lecture_20___Bayesian_Network.wav
26,any probability of conditional probability of interest can be computed if you know the full joint probability distribution but as we have discussed the presenting the entire joint probability distribution is it taxable if you have a base network representation we can more compactly and efficiently represent the joint probability distribution especially if this graph does not have too many edges if this graph has less the number of edges that means these variables many of them are conditional independent of others give exam evidence and we can represent the joint distribution board officiated at work is a set of nodes let us say it is a set of nodes X equal to X 1 X 2 X these are the notes of the variable then ask represent,26_Lecture_20___Bayesian_Network.wav
27,probabilistic dependence on Independence among variables,27_Lecture_20___Bayesian_Network.wav
28,,28_Lecture_20___Bayesian_Network.wav
29,in fact absence of a 8 denotes Independence or conditional Independence the network structure is a directed acyclic graph the network is a directed acyclic graph for that and at every note we keep the local probability,29_Lecture_20___Bayesian_Network.wav
30,artist note we keep local probability distributions,30_Lecture_20___Bayesian_Network.wav
31,which is also called the conditional probability table so CPT or conditional probability table is associated with each note now let's try to look at you know what are the values what are the probability distributions that we need to fully specified this space for simplicity letters as you all this notes correspond to variable switch and Boolean so we have 1234567 we have 7 variables so that all bullion so there are 2 to the power 7 possible combinations and for each one of them we can find the probability of find the probability,31_Lecture_20___Bayesian_Network.wav
32,if you follow this model at the conditional probability table will see we have to keep the probability distribution of a node given the value of its not this not does not have any parent so what we need to keep here is a need one value what is the probability of late wake up so we require one rainy day also doesn't have any parents we need one probability value to be associated accident has one parent rainy day so what is the probability of accident been to given that it is a rainy day and accident being true given that it is a not a rainy day so we have one parent we need to give two values traffic jam has to accident and rainy day so accident can be true that they can be true but can be false one can be true one can be false and so on so there are four possible combinations of the parents for each of them we can find out what is the probability of traffic jam happening the probability of traffic jam not happening can be computed by 1 minus source of this not weakness for that similarly late for work also be required for values meeting postponed one value for value for late for meeting so the total number of probability values that we have to keep his 1234 8:12 1370 so be required to store 17 probability values in contrast to the fully connected bayesian network where we have to keep 2 to the power 7 actually 2 to the power 7 - 1 bus network is a compact representation of the joint probability distribution,32_Lecture_20___Bayesian_Network.wav
33,what you will like to do is look at an example of this example of the base network that we have look that and corresponding to this example we have conditional probability table associated with each note the number of values of the conditional probability table given that all the variables are bullion I have already written here so you eat note is asserted to be conditionally independent of its non descendants within its immediate right disturb so the best network says that each node is conditionally independent,33_Lecture_20___Bayesian_Network.wav
34,,34_Lecture_20___Bayesian_Network.wav
35,office non defendants,35_Lecture_20___Bayesian_Network.wav
36,given the value of its immediate,36_Lecture_20___Bayesian_Network.wav
37,so late for meeting is independent of late wake up given whether you are late for work,37_Lecture_20___Bayesian_Network.wav
38,meeting postponed is independent of late for work given the value of late wake up and traffic jam so these are examples of conditional Independence that we can read of based on this statement that I have,38_Lecture_20___Bayesian_Network.wav
39,when we have a bayesian network so 1st represent the local probability tables we can use the base net for making in forest,39_Lecture_20___Bayesian_Network.wav
40,so inference means you want to compute certain probabilities of certain probability distribution of certain variables given certain evidences you know the values of some variables and you want to find out the probability distribution of some other variables which are of your Indus inference you compute posterior probability is given some evidence evidence means some of the notes you know the values of the notes,40_Lecture_20___Bayesian_Network.wav
41,and in order to do inference efficiently you can exploit the probability conditional Independence that is encoded in the belief,41_Lecture_20___Bayesian_Network.wav
42,Chandra in general exact inference,42_Lecture_20___Bayesian_Network.wav
43,intractable,43_Lecture_20___Bayesian_Network.wav
44,for arbitrary basement the exact inference in interactive for certain special types of base network which have some special structure exact inferences tractor but even when exact inferences intractable there are certain methods by which you can do inference in a tractable,44_Lecture_20___Bayesian_Network.wav
45,Sofia approximate techniques,45_Lecture_20___Bayesian_Network.wav
46,for inference iji Monte Carlo methods will not talk about these methods are in fact and inference method specifically in this class,46_Lecture_20___Bayesian_Network.wav
47,can also be hard for certain types of bayesian network but in practice many of these methods are useful but you can have efficient algorithms,47_Lecture_20___Bayesian_Network.wav
48,,48_Lecture_20___Bayesian_Network.wav
49,that leverage,49_Lecture_20___Bayesian_Network.wav
50,the structure of the graph,50_Lecture_20___Bayesian_Network.wav
51,in the last class we talked about nice and we say that nice base represents Watan Independence conditional Independence assumptions we can draw nice place the probabilities in the variables involved in life is in this graphical structure and for that particular structure inferences efficient there are other types of structures like qualities which are more general structures for which inferences efficient but it is not true for other structures but there are different algorithms for dealing with poly trees and also certain variations of quality but those details they will not enough of about in this class,51_Lecture_20___Bayesian_Network.wav
52,let us look at what are the situation what are some standard situation where we can apply bayesian so bayesian networks can be applied in many scenarios for example it can be applied for diagnosis suppose you know the symptoms of a disease you want to identify what is the possible cause for example suppose a cancer causes a tumor and then there are other causes for a tube,52_Lecture_20___Bayesian_Network.wav
53,,53_Lecture_20___Bayesian_Network.wav
54,cancer may also cause weight loss and so on so if you are given the symptoms you can try to in for what is the possible cause we saw an example of this type of inference when will look that using base your so from the symptoms you can find out what is the probability of specific causes social late for work what is the probability that there is a traffic jam so give you want to find out what is the what is the possible cause this is for diagnosis you want to diagnose what happened given that you can see the symptom suppose up some machine is malfunctioning used to observe certain things you want to understand what is the possible cause this is one time network can also be used for prediction if you have cancer what is the probability of weight loss what is the probability that you have a tumor so it can be used for production given the cause find the probability of the symptoms can be used for production find the probability of class of a class given the data given the training data you find the probability of class this is how it is used for basic supervised machine learning to find the probability of a class is in the data it can also be used for decision making if you are given different cost functions you want to take different decisions are so that your utility is maximized and for such applications also based network can be used so let's look at how to define find a base network for many as we have already said the structure of the graph denotes the conditional Independence relationships in general the joint distribution of X 1 X 2 x,54_Lecture_20___Bayesian_Network.wav
55,can be written as the product of probability of each node given it spent in general in the most general case if we have to find out probability X1 X2 X3 M we can apply the chain Rule By which protects this is equal to probability of X 1 X probability of its two given X 1 X ability of X3 given X1 X2 that the probability of X and given X 1 X 2 x minus 1 is the normal chain rule applied toward joint probability distribution but in a base situation you can say that this is equal to the product of probability of each node given just the,55_Lecture_20___Bayesian_Network.wav
56,use of it this way it is a compact representation of the joint distribution for the grass is required to be a cyclic so that two components to a bayesian network the graph structure and the numerical probabilities or,56_Lecture_20___Bayesian_Network.wav
57,the conditional probability table associated with each note now we will look at some examples of bayesian network so here is a situation the first example we have three variables A B and C and then there are no edges so there completely independent therefore probability the joint distribution of probability of ABC is simply probability of a x l b x British see this is the simplest case that there is no relation among the variables that completely,57_Lecture_20___Bayesian_Network.wav
58,the second example we have a suppose a is a disease be is one symptom of the disease C is another symptom of the disease,58_Lecture_20___Bayesian_Network.wav
59,conditional independence of the base network means the joint distribution probability of a b c is given by probability be given a x b l c given a x probability of A B and C are conditionally independent give,59_Lecture_20___Bayesian_Network.wav
60,example we have A and B are two causes for C suppose she is late for what is traffic jam these latter wake up so late wake up and traffic jam are independent causes of being late so if so traffic jam and let wake up independent if you do not know whether it is late but they are not independent if you know whether it is because one can if you are late one of them can explain the reason for the other so if you're a late and there is a traffic jam the probability that you have woken up latest less but if you are late and there was no traffic jam its higher highly more highly probable that you have woken up and The Fourth example we have this three variables ABC and we have no relation from A to B from b2c and this represents mark of dependency so she is independent of a given the fancy occur at consecutive times Death at time T minus one be at time T C at time T we can say that the probability of C depends only on the current state be it is independent of the previous test this is the mark of assumption which is used for examples of bayesian that and we have already talked about the naive Bayes model in the earlier class where we have certain attributes his attributes and noted here as Y1 Y2 y3 n and c is the class so Ennai space we assume that Y1 Y2 y3 are in all of them there was an independent of each other given the class and the class determines the probability of my meniscus is one the probability of say that the probability of even given classes 1 probability of Y1 given classical 20 these are the particular relationships that you have to study in a life base model and lastly this is a model for hidden Markov model so hidden Markov model is another graphical model we will not unfortunately we will not study it in the detailed in this class but in a hidden Markov model the there is the underlying state of the system which follows a Markov model so S1 S2 S3 SM at the states of the system at different time instance at the mark of assumption means that is 3 is independent of S1 given is to SN is independent of the previous dates given as a -1 and these states are hidden but you have an observation and the observation what you can observe depends on the current state so the hidden Markov model is depicted by this type of graphical model and their official outcomes for inference and learning of hidden Markov models they have widespread application in speech recognition part of speech tagging gene sequence model and other such area,60_Lecture_20___Bayesian_Network.wav
61,where does learning how does learning how is learning help by beliefnet so beautiful mention that there are three cases the ways in which you can use belief networks for learning in case one the network structure is known to you the network structure is given in advance and from your training examples you can learn the conditional probability table second estimate the product structure the variables that given to you and the training example values of all the variables are known to you and you have to estimate the conditional probabilities and after that you can use the base network for making in forest in the second case the network structure is given but only the values of some of the variables are given to you in the training data so when the network structure is given but you know the values of some of the various so here you have to also estimate the probabilities corresponding to the hidden variable for that some Framework like gradient Descent 17 can be used again will not talked about the details in class with hard case is weather network structure is not known hearing to learn the network structure using some dualistic search for constant this technique this is likely advance we will not talk about this with this we come to the end of this lecture and this module thank you very much,61_Lecture_20___Bayesian_Network.wav
