,content,topics
0,,0_Lecture_39___Introduction_to_Ensembles.wav
1,good morning today we will start the 8 module where we talked about when we will talk about ensemble learning in ensemble learning we look at multiple classifiers and combining the output of the multiple classifiers in order to get better prediction for classification accuracy and under certain conditions where the classified outputs are independent of each other and make errors in an independent manner it is possible that by combining the outputs of several classified as we get a resulting classify which is better than any of the constituent classified as so today we will give a brief introduction to what is ensemble learning and what are the sum of the different ways in which the different learners can be formed so the topic of today is also some,1_Lecture_39___Introduction_to_Ensembles.wav
2,,2_Lecture_39___Introduction_to_Ensembles.wav
3,let's talk about ensemble classification so we have multiple learners,3_Lecture_39___Introduction_to_Ensembles.wav
4,given a test example these multiple learners will give different outputs the outputs maybe all sem all different or in some of them will be same some of them will be if so this multiple learners will give different decision and we want to have a way of combined name the decision output by the multiple love so what this we have to generate,4_Lecture_39___Introduction_to_Ensembles.wav
5,baseline,5_Lecture_39___Introduction_to_Ensembles.wav
6,how we get to generate a group of learners can be done in multiple waves and we will discuss about them and these different learners have to be different and there are many ways in which the learners can be different they may be using different Outlook,6_Lecture_39___Introduction_to_Ensembles.wav
7,you think of learner which uses a decision tree learning which uses and all that for a learner which uses our support vector machine and so on so we can feed the training example two different learning algorithms and combine the output but more usually which may use the same algorithm but use it such that the learners at different how so the different learners Mein use different parameters,7_Lecture_39___Introduction_to_Ensembles.wav
8,hyperparameter,8_Lecture_39___Introduction_to_Ensembles.wav
9,same learning algorithm but give different values to the parameters you get different models as output the models will be different they may have different representation,9_Lecture_39___Introduction_to_Ensembles.wav
10,the learners,10_Lecture_39___Introduction_to_Ensembles.wav
11,aur different modalities or may use different training sets,11_Lecture_39___Introduction_to_Ensembles.wav
12,talk about variance with respect to a learning research that learning outcomes may have Hai various so when they get their applied on different subsets of the data if the algorithm as high variance the model output will be different so we can try to give in learning algorithms which are we can have high variance can be fed different training set and as a result each time we will get different models which can be combined how do we set different type of parameters for example in neural network we have seen that there are certain parameters that we can set we can decide the topology of the network that is the number of hidden layers number of notes in each heat and layer the values of the weights and biases and so on so these are the different parameters by giving different values of this parameters we can get different within floral network we can get different bodies then we have looked at decision trees in decision tree is we try to come up with a strategy for deciding the attribute on which to split a particular node for split we can use different uristix functions such as entropy Gini index Excel but we could modify the decision tree algorithm so that we use different functions so that the different learners do not split on the same attributes and force the learners to form the foreign decision trees in the some of the ways in which you can get a group of learners now it is found that no learner is absolutely better than the other no one go to learn for many problems but if you have multiple learners and they know we not be good in that they may be weak but they make independent terrace so all the learners which are different then we work well in different parts of the data of the instance space and by combining is weak learners it is possible to get a very strong,12_Lecture_39___Introduction_to_Ensembles.wav
13,sample letters state the two class classification,13_Lecture_39___Introduction_to_Ensembles.wav
14,,14_Lecture_39___Introduction_to_Ensembles.wav
15,suppose we take some learners at each learner has an error so the error of each learner is website,15_Lecture_39___Introduction_to_Ensembles.wav
16,but has to be greater than 4.5 Y equal to 2.5 is a random order which does not give any information that is an example that you give it has 50% chance of being positive 50% chance of being negative the error = 2.5 does not give you any in format,16_Lecture_39___Introduction_to_Ensembles.wav
17,any learner error has to be greater than now we want to see that in ensembles we want the different constituent learners each will have an accuracy that accuracy Menon be very high but it must be greater than 4.5 but we will show that is not required if other conditions are met it is not required that that you receive has to be much higher than point why it has to be higher than 4.5 but it did not be very high but if these different base learners satisfy certain Independence conditions then it is possible to combine weak learners each having error greater than 4.5 in order to get a very strong,17_Lecture_39___Introduction_to_Ensembles.wav
18,this error has to be such that in all we have different based learning available L1 L2 LK and each has arrived at silent but the errors made must be independent,18_Lecture_39___Introduction_to_Ensembles.wav
19,make a diagram action and talk about bias and variance have already discussed bias and variance,19_Lecture_39___Introduction_to_Ensembles.wav
20,bias is a measure of how flexible the model is the model is very flexible a very powerful than the bias is low for example a decision tree is quite flexible bias is low and neural network is quite flexible bias is low but a linear classifier is not so flexible and bias is hi secondly we talked about waiting,20_Lecture_39___Introduction_to_Ensembles.wav
21,variance is high when if you give different subsets of data as training set the models output are very different then we say variance is height usually those hypothesis for which base is low they have high variance it is more flexible the more powerful the Representation is then variance will be Hai by a sweet now our objective is to reduce both AC and and we will see that one of the ways we can achieve both low bias and low variance is by on something,21_Lecture_39___Introduction_to_Ensembles.wav
22,low bias error and low variance error so we have by using ensembles,22_Lecture_39___Introduction_to_Ensembles.wav
23,which use multiple,23_Lecture_39___Introduction_to_Ensembles.wav
24,,24_Lecture_39___Introduction_to_Ensembles.wav
25,,25_Lecture_39___Introduction_to_Ensembles.wav
26,assume that these models have low by a sun hi variance by combining the output we can get low variance while maintaining your mobile,26_Lecture_39___Introduction_to_Ensembles.wav
27,even if the buyers is high to start with by combining we get a classifier which has,27_Lecture_39___Introduction_to_Ensembles.wav
28,because the individual hypothesis may have high bias but when we combine them we get a new hypothesis which may be out of the hypothesis class but we get a new potential hypothesis which has low bias so by using ensemble it is possible to get low bias and low variance,28_Lecture_39___Introduction_to_Ensembles.wav
29,if we achieve low by a slow variance what does it give us we will get less overfitting I will not have to worry about stopping criteria when to stop at 17 if we can achieve low bias and variance so the offence of ensemble by which it achieves this is by combining different baseless so and these learners Mein biwi,29_Lecture_39___Introduction_to_Ensembles.wav
30,combining week,30_Lecture_39___Introduction_to_Ensembles.wav
31,error cannot be higher than 4.5 error must be,31_Lecture_39___Introduction_to_Ensembles.wav
32,but if there is no is not very low we can say it is a week or so in order to combine weak learners we need certain conditions so the main condition that we required is independent independent,32_Lecture_39___Introduction_to_Ensembles.wav
33,,33_Lecture_39___Introduction_to_Ensembles.wav
34,and independent learners and letters a it has accuracy,34_Lecture_39___Introduction_to_Ensembles.wav
35,70% each with array,35_Lecture_39___Introduction_to_Ensembles.wav
36,so we have an independent learners each with points 3 error aur point 7 accuracy now you apply the learners on some test data so it can be that all the learners give the same up if all the learners get the same output that is a possibility now,36_Lecture_39___Introduction_to_Ensembles.wav
37,,37_Lecture_39___Introduction_to_Ensembles.wav
38,,38_Lecture_39___Introduction_to_Ensembles.wav
39,then what is your confidence on the test example because the learner has accuracy point 7 your confidence is used one learn your confidence is points but if you have 10 learners all of them saying positive then what is your confidence and the resulting ensemble so you could say that you are 1 - 1 - point 7 to the power 10,39_Lecture_39___Introduction_to_Ensembles.wav
40,if all 10 learners used and learners all of them give the same output your confidence on the learners is 1 - 1 - 7 to the power 10 which is one minus point 3 to the power 10 so you can find out the value of this expression but it will be quite Hai very close to one when you do this so you are getting higher confidence with all the learners agree on the,40_Lecture_39___Introduction_to_Ensembles.wav
41,practice we cannot always expect that all the learners Nagari if the learners have accuracy points11 it is unlikely that all of them will agree on the so some of them will say class 1 some of them will say class 2 and we will our strategy will be taking the majority class so it is unlikely that all the learners will give the known as a independent it is unlikely that all of them will agree but some of them will agree and we will get a majority in fact If you want to get a majority for a problem instead of using 10 we can use of odd number 11 so that always be have a majority on A2 Plus,41_Lecture_39___Introduction_to_Ensembles.wav
42,in the general case is out of in learners in 1st class,42_Lecture_39___Introduction_to_Ensembles.wav
43,and into se class,43_Lecture_39___Introduction_to_Ensembles.wav
44,and without loss of generality Let Us assume that in one is greater than so that is anyone,44_Lecture_39___Introduction_to_Ensembles.wav
45,majority and therefore class one is the opinion about of the majority of the learners,45_Lecture_39___Introduction_to_Ensembles.wav
46,the probability of the output really being class 1 in such case the probability of class 1 is given by,46_Lecture_39___Introduction_to_Ensembles.wav
47,expression so 1 -,47_Lecture_39___Introduction_to_Ensembles.wav
48,binomial distribution of in the minimum of environment into in this case there was you can one get that into a minimum of an end to end the accuracy with 10 points,48_Lecture_39___Introduction_to_Ensembles.wav
49,the Binomial Distribution is given by,49_Lecture_39___Introduction_to_Ensembles.wav
50,probability of R equal to n factorial so it's actually,50_Lecture_39___Introduction_to_Ensembles.wav
51,Sia,51_Lecture_39___Introduction_to_Ensembles.wav
52,e to the power R1 - 3 to the power n minus so if you substitute here this here we get and see and do,52_Lecture_39___Introduction_to_Ensembles.wav
53,30 points,53_Lecture_39___Introduction_to_Ensembles.wav
54,point 7 to the power n,54_Lecture_39___Introduction_to_Ensembles.wav
55,point 3 to the power,55_Lecture_39___Introduction_to_Ensembles.wav
56,is the probability of this class is given by the binomial distribution and we see that combining this we classify as gives us classifier with higher accuracy now we will look at how we learn that we can give each learner equal weight so we have equal weight or an later,56_Lecture_39___Introduction_to_Ensembles.wav
57,,57_Lecture_39___Introduction_to_Ensembles.wav
58,,58_Lecture_39___Introduction_to_Ensembles.wav
59,we give different way to the different classifiers we can give different weights to the different classifiers and how can we give the weight to the class is a so the way,59_Lecture_39___Introduction_to_Ensembles.wav
60,accuracy of the class,60_Lecture_39___Introduction_to_Ensembles.wav
61,aur which country proportional to,61_Lecture_39___Introduction_to_Ensembles.wav
62,,62_Lecture_39___Introduction_to_Ensembles.wav
63,classified we can give it there are different ways in which we can with the classified then we can give them B based on their probability of being correct which is related to accuracy and this is what is done in bayesian classifier system talks about bayesian classifiers briefly we can say that the output why is taken to be if they are,63_Lecture_39___Introduction_to_Ensembles.wav
64,learners was a w y equal to 6 March 1 volt to 12,64_Lecture_39___Introduction_to_Ensembles.wav
65,w J D J and K is the weight of teacher's classify satisfy certain condition,65_Lecture_39___Introduction_to_Ensembles.wav
66,and Sigma wj equal to 1 to n,66_Lecture_39___Introduction_to_Ensembles.wav
67,conditions that the different values of wj cal now we have talked about bayesian learning earlier in the class and we have seen that in bayesian classifier we take it off,67_Lecture_39___Introduction_to_Ensembles.wav
68,,68_Lecture_39___Introduction_to_Ensembles.wav
69,given a particular training example X2 be equal to Sigma,69_Lecture_39___Introduction_to_Ensembles.wav
70,in bayesian learning we use all hypothesis we combined all hypothesis for all model so summation over all models,70_Lecture_39___Introduction_to_Ensembles.wav
71,X probability of what is given by bayesian so in bayesian learning what we are doing is that we are finding the probability of a class in terms of the prior probability of the model and the probability of class IV given the learning test and outputs,71_Lecture_39___Introduction_to_Ensembles.wav
72,different models where combining it and the weights are the prior probability of the so this is one way of ensemble but this is not a very we have talked about earlier and we'll talk again that this is often not practical because the size of the hypothesis space is so huge we cannot take get so many hypothesis and combine that there are other difficulties for example many learning and many of them that you have seen their output a class and they do not output a probability for example in decision tree we can modify the other two output a probability which is the fraction of examples at a particular leaf node but many other also those in the conventional for their output class 9 probability so we cannot directly applying that we have to modified alloy all we have to re interpret the output of the algorithm in terms of the it is not always possible to get an estimate of the prior probability of a bottle but the most important reason why this bayesian combination is not very practical because even a hypothesis space typically the number of hypothesis is so large it is not possible to numerate all of them and,72_Lecture_39___Introduction_to_Ensembles.wav
73,ok so,73_Lecture_39___Introduction_to_Ensembles.wav
74,but this is what are the base this thing so from this what we get is,74_Lecture_39___Introduction_to_Ensembles.wav
75,in the best classified we can write waste classified as the output Y equal to we can rewrite it as that class,75_Lecture_39___Introduction_to_Ensembles.wav
76,that class,76_Lecture_39___Introduction_to_Ensembles.wav
77,included in the set of classes fee for which commission over all hypothesis in the hypothesis space capital expression is maximum and what is this expression it is probability of CJ given a x x,77_Lecture_39___Introduction_to_Ensembles.wav
78,given,78_Lecture_39___Introduction_to_Ensembles.wav
79,training data or we can which is the sample that we have so probability sees a given h i x probability of training data given a child times prior to the ability to classification this is the expression that you get there is the predicted Class C is the set of all possible class and CJ is a particular class C1 C2 17 capital 8 is the hypothesis space is one element of the hypothesis space,79_Lecture_39___Introduction_to_Ensembles.wav
80,is the training date so the base of the classified will represent the hypothesis and this hypothesis is not necessarily in the hypothesis please when you combine this hypothesis we get a new hypothesis if you combine the output of the different single layer neural network to get hypothesis which may not be a single,80_Lecture_39___Introduction_to_Ensembles.wav
81,hypothesis that you can combine a set of conjunctive bullion formula to get a hypothesis which is not necessarily conjunctive fully informed the ensemble gives you a hypothesis which may be outside the hypothesis space age but such hypothesis when you use base optimal classes at the output hypothesis that you get is the optimum hypothesis in the ensemble space but as I said the based classifier cannot be practically implemented because of various factors hypothesis space to large it may be difficult to get probabilities of the probabilities of the sometimes used which is called model,81_Lecture_39___Introduction_to_Ensembles.wav
82,bayesian model averaging all possible models in the model space elevated in the probability of being correct as we do in this thing and it is optimal it is optimal hypothesis but we cannot get do that it cannot use it practically so we can use certain variations by using some sampling Monte Carlo sampling at 17 but we will not,82_Lecture_39___Introduction_to_Ensembles.wav
83,instead letter stop briefly about why are on Chromebook success so the bayesian perspective we have seen that that is equal to Sigma probability of overall models in a given MJ and x x probability of mc and we know that this gives us the optimal icon from the bayesian,83_Lecture_39___Introduction_to_Ensembles.wav
84,,84_Lecture_39___Introduction_to_Ensembles.wav
85,output may not be accurate but if the outputs an independent then the variance gets reduced and it can be shown that the variance of the output If you are using any number of learners the variance of the resulting ensemble is variance of summation over J1 by N N is the number of ensemble won by NDA,85_Lecture_39___Introduction_to_Ensembles.wav
86,result in variance which can be shown to be 1 by n square time and in 2 variants of each of the individual classified which is equal to 1 by in variance,86_Lecture_39___Introduction_to_Ensembles.wav
87,output spider and learners at independent the variance gets reduced by a factor,87_Lecture_39___Introduction_to_Ensembles.wav
88,,88_Lecture_39___Introduction_to_Ensembles.wav
89,the learners are not completely independent but they are not completely correlated we can have a modification of this so in that case we can tell if they are dependent but error increases with positive correlation we can tell that the variance of Y is still one by n square variance of this thing which can be shown to the equal to 1 by n square,89_Lecture_39___Introduction_to_Ensembles.wav
90,over all models chain variants,90_Lecture_39___Introduction_to_Ensembles.wav
91,so this is the same as the previous factors plus there is another term which is to,91_Lecture_39___Introduction_to_Ensembles.wav
92,,92_Lecture_39___Introduction_to_Ensembles.wav
93,all the best of the learners so to summation between the pairs of learners covariance of D and b,93_Lecture_39___Introduction_to_Ensembles.wav
94,so if the learners are not completely correlated we will see that even in this case the reduction in bed now what is the main challenges of developing ensemble the main challenges in developing ensemble outcomes is not to get individual goodness which you have been doing in the rest of the class but to make sure that these learners and independent and how can this be achieved this got so many different ways in which we can achieve Independence and what I told in the beginning of this class you have a training data you have a training data De,94_Lecture_39___Introduction_to_Ensembles.wav
95,XI why the training data that is given to you now one of the things that you could do you put to the learners you will teach the training data to and,95_Lecture_39___Introduction_to_Ensembles.wav
96,that two different learners to feed different training data which a sample from D so that data fed to the different learners with the same on they may be different and in the next class will look at different ways of Sampling to make this training set different step 1 to create multiple data set,96_Lecture_39___Introduction_to_Ensembles.wav
97,what you can do if you can feed this to the different learners so multiple multiple classifiers,97_Lecture_39___Introduction_to_Ensembles.wav
98,1 cm is classified as can be the same classified or can be different classifiers or same classified with different parameter ization you can make either the different of the seas different or both the front and as a result it will get output from all the combined out by using a voting mechanism and get the resulting ensemble classifier and the combination can be done by he had just talked about weighted voting and waited Hote it is also possible to use other types of combination like taking the median taking the product taking the minimum taking the maximum so some type of voting on combination,98_Lecture_39___Introduction_to_Ensembles.wav
99,class will look at bagging and boosting by which we can for these data sets to be different we have already talked about different learners and we have also talked about parameter as it is of the learners if you wish you can go through one particular ensemble which is called random forest,99_Lecture_39___Introduction_to_Ensembles.wav
100,forest,100_Lecture_39___Introduction_to_Ensembles.wav
101,defined on decision,101_Lecture_39___Introduction_to_Ensembles.wav
102,you force,102_Lecture_39___Introduction_to_Ensembles.wav
103,ensembling model to come up with different decision tree how do you force them you put some randomisation in the attribute selection face their different attributes will be selected in a random manner when you go to split a note and 20 different learners act on different subsets of the data so random forest is basically an ensemble of decision trees which do,103_Lecture_39___Introduction_to_Ensembles.wav
104,samples by bagging which is what we will talk about in the next class and their use the decision tree algorithm with different with a randomised approach to force them to produce different decision tree every time the alpha and based on this we have an ensemble classifier which is called random forest and in many examples when you test cases random forest has been found to do but better than the basic solution and it is a powerful running out with this we closed today we will take up bagging and boosting in our next class,104_Lecture_39___Introduction_to_Ensembles.wav
