,content,topics
0,,0_Lecture_14__Feature_Extraction.wav
1,so today we will start the fourth the third lecture in the module on instance based learning and features direction in the last class we have talked about feature selection in today's class we will talk about feature extraction in feature extraction what we have is that we have a n dimensional features X 1 X 2 x n and we want to make it to a lower dimensional space which is M dimensional and we want to get the features that one gets to that I'm so this is a new features where M is less than and and each of these features is some functions of the original features ss162 you know that in feature selection which we covered in the last class Wizard we take a subset of the features in C what you are doing in feature extraction is that we are taking the original features and mapping it to a lower dimensional space and each feature is obtained as a function of the feature set so it's a projection of a higher dimensional feature space to a lower dimensional features space and we want to make this projection so that the smaller the invention features that can help us to have a better classification of faster classification as we have noted in the last class need to do is that we need to find the projection Matrix Bablu,1_Lecture_14__Feature_Extraction.wav
2,,2_Lecture_14__Feature_Extraction.wav
3,such that,3_Lecture_14__Feature_Extraction.wav
4,Z equal to WTF is this event and this is so we want to find WT where is it is wpx which is a projection from n dimensional space to M dimensional space and what we expect from such a projection is that the new features there and coral,4_Lecture_14__Feature_Extraction.wav
5,and cannot be reduced further,5_Lecture_14__Feature_Extraction.wav
6,last class we noted that features can be redundant and therefore we can have larger number of features when we map into smaller space we want that the features are not redundant among themselves that they are uncorrelated and cannot be reduced for secondly we want features to have large varian,6_Lecture_14__Feature_Extraction.wav
7,variation why because if the feature takes similar values for all the instances that feature cannot be used as a discriminator so since he wants the features to be able to distinguish between the different instances we encourage larger variation on larger variance between the features because otherwise the features would not there in,7_Lecture_14__Feature_Extraction.wav
8,,8_Lecture_14__Feature_Extraction.wav
9,the following instances and there in the two-dimensional feature space that two features X1 and X2 and in that space you have these instances now you have a choice of deciding suppose you want to map this two dimensional feature space into a one-dimensional features this you are to select that access now this is a possible access now along this access you can put this on the axis and you can project each of these instances on this exercise so after projection this point will make to this point on the x-axis at this point will mat hear this point will mark here so the different points map here here here here right in contrast we could have taken that was like this and if we did that these points will map here so can you tell me how to of these two possible projections so this is letters,9_Lecture_14__Feature_Extraction.wav
10,and this is to which one would you,10_Lecture_14__Feature_Extraction.wav
11,that into there is a larger variation is a larger variance among the features so we would buy the principle that we are going by prefer this that this access to the previous,11_Lecture_14__Feature_Extraction.wav
12,topics on this what we do is that some of you may have studied mathematics and are familiar with principal components,12_Lecture_14__Feature_Extraction.wav
13,for feature extraction was principal components play a very important role so we can take the principal components of the data points that we have and take the principal components the first principal component the second principal component a search of the top tools for components as the new access and that will give a certain properties which satisfy our objectives of getting uncorrelated features and features with large variance of first let us see that if you want to given our instances in n dimensional space if you want to select one a new feature the first feature how we should select the first so we are given a sample of the observations X1 X2 X3 and for each of the way actions and for each observations we have a n dimensional vector represent in the n features now what we want to do is that we want to find this mapping,13_Lecture_14__Feature_Extraction.wav
14,present gift M dimension and letters a initially be considered to be one dimensional we want to find out the feature the one feature which will be best whatever,14_Lecture_14__Feature_Extraction.wav
15,one criteria that we can use in a because we are choosing only one feature the question of being quadrilateral uncorrelated does it so we can go by this principle we can choose that feature which has the largest various so we choose the feature such that the variance of advance is,15_Lecture_14__Feature_Extraction.wav
16,that is we find that value of the way for which the projection that we get corresponds to the largest variance of this is what we want to do and this is where the principal components come handy because mathematically the principal,16_Lecture_14__Feature_Extraction.wav
17,that vector which exactly does two ways of considering the properties of principal components one is it is that projection for which the variances maximum secondly principal component can be looked upon as if you map the original vectors to this new low dimensional vector space given a fixed size of the dimension of the principal components are such that if you want to recover the original instances from this reduced representation of the principal components of such that the Wreck instruction error is minimum so for more details you should refer to an algebra,17_Lecture_14__Feature_Extraction.wav
18,some picture in the sky,18_Lecture_14__Feature_Extraction.wav
19,we see that for the points in red desirable instances the first principal component The Princess is given by this,19_Lecture_14__Feature_Extraction.wav
20,greenline gift principal component direction one where as this is the second principal component of the first principal component is chosen based on largest various how to we choose the second principal component for the fall when we choose the second feature we want to make sure that this feature is an correlated with the first period I want to select a feature which is orthogonal to the first feature that is they do not have that do not share any information they are uncorrelated for that which was an orthogonal direction among the possible vectors in the orthogonal Direction This is the direction for which the various this,20_Lecture_14__Feature_Extraction.wav
21,component is the one which maximizes the variance the second is orthogonal to the first and with that constraint the variance is maximum the third will be orthogonal to both the first and the second and other Radiance will be maximum of course in the original feature space is two dimensional the new feature space cannot be more than two dimensional was constrained by that but you can take even a two dimensional space to a two dimensional space by making the additional property having the additional property then to news to feature dimensions,21_Lecture_14__Feature_Extraction.wav
22,now so there are two main principles for choosing the feature direction,22_Lecture_14__Feature_Extraction.wav
23,so we want to choose direction such that the total variation of the data is maximum that is we maximize total variance,23_Lecture_14__Feature_Extraction.wav
24,,24_Lecture_14__Feature_Extraction.wav
25,secondly we want to choose directions that are orthogonal so that we minimise for relay,25_Lecture_14__Feature_Extraction.wav
26,,26_Lecture_14__Feature_Extraction.wav
27,sofa dress which use,27_Lecture_14__Feature_Extraction.wav
28,action,28_Lecture_14__Feature_Extraction.wav
29,,29_Lecture_14__Feature_Extraction.wav
30,Daksha name is strictly less the name so we choose M orthogonal directions,30_Lecture_14__Feature_Extraction.wav
31,which maximize Total weight,31_Lecture_14__Feature_Extraction.wav
32,,32_Lecture_14__Feature_Extraction.wav
33,how do we do this we have an n-dimensional feature space and we have a n by an symmetric covariance matrix so we have this training data,33_Lecture_14__Feature_Extraction.wav
34,n dimensional feature space so we can have this covariance matrix,34_Lecture_14__Feature_Extraction.wav
35,of the training data points,35_Lecture_14__Feature_Extraction.wav
36,matrix from this for variance Matrix we select the em top principal components which are also called eigenvector so we select,36_Lecture_14__Feature_Extraction.wav
37,M largest eigenvectors of this covariance matrix,37_Lecture_14__Feature_Extraction.wav
38,eigenvectors the top eigenvectors of the one for which the eigenvalues and Eczema the first eigenvector will be the direction with the largest various second with the second largest variance and so what is look at the sky,38_Lecture_14__Feature_Extraction.wav
39,application of principal component analysis for Image Compression initially we have a large number of features and what we have done is that if we represent these pictures using only one principal component the first principal component after reconstruction this is a picture that we get if I use two dimensions for dimensions 8 dimension 16 dimension 32 6400 this shows the reconstructed that we get there as this is the original image so as I said that the two ways of interpreting principal components you should check the top principal components they are selected such that they are orthogonal and their variances maximum and also the can be interpreted as that selection of the factors which are orthogonal for which the reconstruction Era is this can be from the Image Compression how well,39_Lecture_14__Feature_Extraction.wav
40,think about is Principal component analysis and good criteria,40_Lecture_14__Feature_Extraction.wav
41,Supreme super component analysis if we just have the data points not considering the label the principal component gives Hai variation but if it now look at a classification problem when we do not have just the input values of the instances we also have the labour is physical component still a good way of doing that this lets look at this light the have these points and they belong to two classes Orange class and blue + now the principal component direction is given by this pink line and this pink line is a one which achieves largest variance among the instances but you see this pink line is not very good in separating the orange points on the Blue Point because if you project Orange points on the blue points on this access you will see they are coming together so even though the total spread is hai this particular access is not able,41_Lecture_14__Feature_Extraction.wav
42,the orange and blue + very good idea when you look at the classification problem when we are not taking care of class information which is very important,42_Lecture_14__Feature_Extraction.wav
43,if you have this points what would be a good criteria so what we really want is a feature that separates the classes rather than a feature with simply,43_Lecture_14__Feature_Extraction.wav
44,we want to make sure that the particular feature that we choose does separate classes if you choose this access it doesn't separate the classes because when you project Orange points and blue points on this Axis they will overlap so this is not very,44_Lecture_14__Feature_Extraction.wav
45,now Q project points on this Axis what you notice that the orange and blue points are separated so this pink access acts as a separator for the different classes rather than the,45_Lecture_14__Feature_Extraction.wav
46,climb the two classes are separated so that is to be,46_Lecture_14__Feature_Extraction.wav
47,in order to achieve this we have to see what is the information that we will use first of all we want to separate the classes so we want to maximize between class system so for this we can think of that given the points we take the centroid of the orange points and the centroid of the grains a blue point right and these two should be far away so he wants the between class distance,47_Lecture_14__Feature_Extraction.wav
48,so this is the centroid of this points is the centroid of this points we want the distance between,48_Lecture_14__Feature_Extraction.wav
49,we want to if you look at within class distance we want within class distance to be small to medium class distances achievement at a distance of an instance to the centroid of its class so within my class you want to select a feature so that upon projecting on that feature the within class distances Mall the between class distances height and linear discriminant analysis can be used to find the most disturbing and projection which maximizes between class distance and minimises within clusters,49_Lecture_14__Feature_Extraction.wav
50,this in linear discriminant this is the representation here,50_Lecture_14__Feature_Extraction.wav
51,linear discriminant analysis with define the,51_Lecture_14__Feature_Extraction.wav
52,points belonging to letters it to classes we want to find the low dimensional space such that when we project the instances on this access the classes are well,52_Lecture_14__Feature_Extraction.wav
53,,53_Lecture_14__Feature_Extraction.wav
54,we can use the following formula one is the mean of the first class of the centroid of class 1 M2 is the centroid of the instances of class 2 S1 square is the standard deviation of class 1 and is 2 square is the standard deviation of class 2 and we want the means to be as far away as possible so the objective function that we use looks at the difference between the means and we want the scatter between the points to be as small as possible so we put starter in the,54_Lecture_14__Feature_Extraction.wav
55,method for doing this is fisher linear discriminant fisher linear discriminant uses as objective function when you have two classes it objective function is m 1 - M2 whole square / S1 square + a square and it tries to maximize this objective function it tries to find w that is the projection of the feature space to the new feature space using the parameters w such that this criteria,55_Lecture_14__Feature_Extraction.wav
56,if for a two class problem this criteria can be suitably modified when you want to work with 3 class 4 class on general,56_Lecture_14__Feature_Extraction.wav
57,talk about it,57_Lecture_14__Feature_Extraction.wav
58,brings us to the end of feature detection,58_Lecture_14__Feature_Extraction.wav
59,,59_Lecture_14__Feature_Extraction.wav
