,content,topics
0,,0_Lecture_08___Overfitting.wav
1,good morning welcome to the fourth part of module 2 today we will discuss about overfitting so we mentioned overfitting when we talked about a decision tree in passing sofas we will define what is overfitting so if you learn a decision tree and you continue splitting nodes that we will become larger and larger when the tree becomes larger,1_Lecture_08___Overfitting.wav
2,if you use the tree for classified the training examples the error on the training examples with reduced but it may happen that after sometime as you grow the tree that to error will increase this is called overfitting letters formally define overfitting we say that a hypothesis a,2_Lecture_08___Overfitting.wav
3,,3_Lecture_08___Overfitting.wav
4,is that to overfit,4_Lecture_08___Overfitting.wav
5,,5_Lecture_08___Overfitting.wav
6,the training data,6_Lecture_08___Overfitting.wav
7,,7_Lecture_08___Overfitting.wav
8,if there is another hypothesis,8_Lecture_08___Overfitting.wav
9,h Prime,9_Lecture_08___Overfitting.wav
10,search that,10_Lecture_08___Overfitting.wav
11,h Prime,11_Lecture_08___Overfitting.wav
12,have,12_Lecture_08___Overfitting.wav
13,more,13_Lecture_08___Overfitting.wav
14,,14_Lecture_08___Overfitting.wav
15,Pran age,15_Lecture_08___Overfitting.wav
16,on training data,16_Lecture_08___Overfitting.wav
17,batch h Prime has,17_Lecture_08___Overfitting.wav
18,less error,18_Lecture_08___Overfitting.wav
19,dunnage on test,19_Lecture_08___Overfitting.wav
20,for example in the context of decision trees if it happens that we have a smaller decision tree and it has a high error on training data and lower error on testator compared to a larger decision tree which has smaller error on training data but I error on test data is overfitting has occurred,20_Lecture_08___Overfitting.wav
21,let us look at this slide that suppose as we are growing a decision tree and as we're splitting the notes yes that in the notes when there is positive information gain that is there is a result and production in entropy typically what we expect as we keep growing the tree the accuracy of the tree will go on increasing on the training set that is the error will go on decreasing on the training set however if you test the tree at different points in the album on a held out test set this joint SL what we typically observed is that as you grow the tree initially accuracy on the test set increases after sometime the accuracy on the test set starts to for now I and this,21_Lecture_08___Overfitting.wav
22,we want to have good generalisation performance but the training data make content noise and as we make the decision tree more and more Complex it may try to fit noise and therefore the resulting decision tree will not work well on the true population a second reason could be you have to Hue training examples and you are trying to accommodate the errors in the training exam and you too badly on the test set this is why overfitting can occur,22_Lecture_08___Overfitting.wav
23,illustrate overfitting an experiment which was Run where some positive and negative examples were taken and they were generated such that all points,23_Lecture_08___Overfitting.wav
24,with radius from 0 2.5 where was it where blue class and those having radius from the origin between point five and one where red circles and beyond one also it was blue triangle this sort of examples value used to construct a decision,24_Lecture_08___Overfitting.wav
25,country what we observed is that the error on the training set the last ka fever plotting accuracy and accuracy was going on increasing as we increase the size of the tree here plotting error error on the training set will go on reducing as we construct the decision tree but if we check the error on the test set initially the error of the test set decreases after that it increases and where is starts to increase in this region was a overfitting has occurred in this region where the tree is not big enough to say there is underfitting in this region on the left that is underfitting in the middle there is a good faith and on the right of this dash line the se overfitting has occurred underfitting occurs when the model is too simple and overfitting occurs when the model is too complex,25_Lecture_08___Overfitting.wav
26,it is another slide to illustrate overfitting in when you have overfitting what will happen is that suppose we can get another example suppose Pizza Plus is positive point and this red circles and negative point and suppose that is a noise point here right if you have this noise point which is a spurious point and if you really want to have the decision tree to have lower than the decision tree will try to include the noise point enough it includes the noise point it will induce error in the true set and overfitting can also be due to not enough exam for example if you look at the slide the bottom portion we do not have to few examples and because we do not have to few examples this green line,26_Lecture_08___Overfitting.wav
27,is the ideal this process blue process are there one class and the red solid circles is the other class and the red hollow circles are your test point because you have to not have too few to enough examples these points may be classified if in a region,27_Lecture_08___Overfitting.wav
28,in the feature space you do not have sufficient examples then you cannot form a proper hypothesis about that region and this will lead to,28_Lecture_08___Overfitting.wav
29,if you are working with decision trees overfitting results in a more Complex decision tree that is necessary and showing error is not a good predictor for test inner so what you have to do training error is not a good predictor for true error and therefore you should use a separate and same sample for all the test sample in order to get a better estimate of the true,29_Lecture_08___Overfitting.wav
30,,30_Lecture_08___Overfitting.wav
31,we have to now she know that we know that overfitting has occurred we have to look at what can be done to avoid,31_Lecture_08___Overfitting.wav
32,let us look in the context of decision trees there are two types of methods that are used to prevent overfitting so first of all you could Prove the decision tree while growing can stop early so you have tree pruning type of methods and you have post pruning type of methods,32_Lecture_08___Overfitting.wav
33,Supreme pruning is a method of,33_Lecture_08___Overfitting.wav
34,decision trees to deal with overfitting and what reapring involves is stopping stop early,34_Lecture_08___Overfitting.wav
35,and when do you stop you stop when the data split is not statistically significant what is bar with entropy measure is that we write a note on an attribute and we compute the resulting entropy so that is Asus example Siya S1 and S2 is a set of examples here and we find the gain of a switch,35_Lecture_08___Overfitting.wav
36,and we find attribute with the highest ke the attribute with the highest game should have a positive way it is a negative gain there is no reason to use the attribute but it even if there is a positive gain if the gain is not significant if they gain is not statistically significant we will stop at the not so you stop early that is stop growing stop letting,36_Lecture_08___Overfitting.wav
37,game is not,37_Lecture_08___Overfitting.wav
38,statistically significant,38_Lecture_08___Overfitting.wav
39,,39_Lecture_08___Overfitting.wav
40,latest you do not grow the full tree a second type of method of pruning decision trees is based on post,40_Lecture_08___Overfitting.wav
41,Infosys training what you do is to continue growing the tree till the tree is quite large but after that after growing a big tree you from different subtrees of the so grow the full free and remove Kannur,41_Lecture_08___Overfitting.wav
42,how to remove the mode,42_Lecture_08___Overfitting.wav
43,heuristic search has been used to decide which subtrees to run,43_Lecture_08___Overfitting.wav
44,site to say that suppose you have grown a decision trees which is quite big,44_Lecture_08___Overfitting.wav
45,this is a decision tree that you have grown and you want to decide whether you want to prove the subtree that is want to make this as a leaf not now what you can do is that you can use cross-validation so what you can do it,45_Lecture_08___Overfitting.wav
46,you can use a validation set of the original and suppose this is the subtree st1 so you find out the error you find out the error of t and error of t -,46_Lecture_08___Overfitting.wav
47,so you find out the error after removing the subtree if this error is smaller than this is a candidate for remover now when you have a decision tree any internal note is a candidate for removal you can remove this not this not this not this not this not this note is a different or in this not so these are the different candidates of trees for remover and among them you want to find out though some trees whose removal improves the test,47_Lecture_08___Overfitting.wav
48,among those if a notary whose removal improve the test error you do not true for the among those trees which are eligible for among those subtrees which are eligible for removal you choose that subtree whose removal lois the error the most so that is by using cross validation process validation there is another method based on a principal called mdn or minimum description length will not talk about this principle in detail the basic idea is you want to look at you think of having a function having a decision tree having a classified as,48_Lecture_08___Overfitting.wav
49,reducing the size of information in a suppose you have a training set and you have a function which classify the training set perfectly using the training set you can we create using this function you can recreate the training site give NXT confined why like this is the role of this function know if this function was perfect you could just get that function you put removes the labels on the training set and you could recreate that but if your function is not a perfect classify on these examples what you should do is keep this function and keep the labels of the miss-classified exam so you could think of that given a function that total information that you have to keep is the function + the miss-classified exam now you could,49_Lecture_08___Overfitting.wav
50,you could compare two different please 13 which is smaller and classify as less examples and the second free which is larger and classify is more example you could compare then based on the description length what is the total in length total number of bits required to keep the function and the MS classified example and you want to choose the tree for which this is just so that is another principle but we will not talk,50_Lecture_08___Overfitting.wav
51,let us look at this light on prepro Ne as he said tree pruning means early stopping you evaluate the space before installing them so don't install place that don't look what why when know what MI splits are there to install,51_Lecture_08___Overfitting.wav
52,so typical stopping conditions for Canada in a normal decision tree without Loni to stop if all instances belong to same class you stop if all instances belong to the have the same value and if you want to use tree pruning you introduce some more restrictive conditions for example if you stop is number of instances at that node is too small to stop if the class distributions are independent of the available feature using some statistic will not talk about it in detail thirdly you stop expanding the note if the improvements in the information measured is not suggesting,52_Lecture_08___Overfitting.wav
53,post pruning one of the proposed pruning is called reduced error pruning and we will describe that,53_Lecture_08___Overfitting.wav
54,,54_Lecture_08___Overfitting.wav
55,,55_Lecture_08___Overfitting.wav
56,introduce Tarah pruning if you look at the slide is a post pruning cross validation approach so given the training set available to us we will split the training set into a,56_Lecture_08___Overfitting.wav
57,training and validation set will split the training set in the training and validation set or we call this a gross at this part of the training set is used to grow the tree and this part is used to validate that,57_Lecture_08___Overfitting.wav
58,using the throw data we will build up complete,58_Lecture_08___Overfitting.wav
59,which it really has zero error on the feeling that otherwise also grow the tree as much as possible after that you start running the tree and as you said you look at different candidate Re Sab please for pruning and how how long will use continue cooking until you reach a situation where removing any subtree will give you increased error on the validation set so until accuracy on validation set decreases we will do this for each non-leaf nodes in the tree as he explained any non-leaf nodes and interior know this candidate for pruning for any non list node in the tree you temporal in from the tree below that not and replace that note by the majority vote test accuracy of the hypothesis on the validation set,59_Lecture_08___Overfitting.wav
60,so that is the so you find out if the resulting error is smaller than the original tree and now you do it for every attribute you prove that sum tree for which this error is lowest Soi permanently pruned the node with the greatest increasing accuracy on the validation site and you continue doing this until accuracy on the validation set does not increase,60_Lecture_08___Overfitting.wav
61,post pruning methods of you cons of this method because you are dividing the data into row is set and validate set you have less number of data on which to grow that,61_Lecture_08___Overfitting.wav
62,now if you look at this slide it shows and the effect of Host pruning on a particular training set the solid line in the top this is the training error on the training data as you grow the tree trees grown 2012 ten hundred up to 100 no trees grow and as you see the training accuracy is going on increasing where that test data on the validation set that ulysses reducing after initially it is increasing after sometime it is reducing that is overfitting has occurred now after I have grown the full tree we are using the reduced error pruning algorithm proposed prone and as your postponing the number of nodes in the trees decreasing and we see that accuracy is increasing up to this point when we have about 18 notes that USA she has increased after that further pruning does not give you better accuracy so we stop here so this is the effect of post pruning on the accuracy of a disease,62_Lecture_08___Overfitting.wav
63,now we have discussed earlier that learning involves general ise ation for induction and it is an ill posed problem because you do not have sufficient data so we need some buyers to restrict the hypothesis space or put some preferences so that you want to prevent over,63_Lecture_08___Overfitting.wav
64,tradeoffs Pita in Pol,64_Lecture_08___Overfitting.wav
65,state of includes these three factors the complexity of the hypothesis a more Complex hypothesis can is more flexible it can accommodate more powerful functions but more Complex hypothesis are mainly to overfitting the second is train inside if training size is too small then using the training set you cannot come up with a wood function then the third is generalization error that is the true error the error on a on the population or unseen insta so we want to look at the trade-offs involving these three factors the complexity of the hypothesis that size of the training set and the generalization error we find out that as n increases as train insights increases the error decreases as complexity of hypothesis increases or decreases then the error increases as we saw in the decision tree as we are going the decision tree initially error is decreasing then it is increasing and as complexity of hypothesis increases that training a decreases for some time and after sometime it may become zero it becomes starting with the test error of the true error first increases and then increases and this is when error starts increasing this is the region where we say overfitting this is where overfitting has overfitting is happening when a model captures the it used in traces of the data rather than the general is in the data if you have too many parameters if too many notes on the decision tree if you are too many parameters overfitting can occur for example we discussed about regression in an earlier class if you have linear regression you have few parameters with h u m b h m to but if you have second degree polynomial Swift degree polynomial 10th degree polynomial have many more parameters and overfitting is more likely to happen because an another order polynomial can intersect any and plus one data points if you have to show data points you should use a function which has fewer parameters because otherwise overfitting can occur we have seen how overfitting can be dealt with in decision tree in regression also overfitting that occurred and in regression on linear regression overfitting can occur at if you want to deal with overfitting there are some standard ways of dealing with overfitting first of all in any for any learning Urdu If you can get more data that is one way of dealing with overfitting the second is a diary of cross validation using a validation set to decide what is happening to this is a very general approach to deal with overfitting and then we will let a look at the bayesian approach to overfitting but one approach will just want to spend of 2 minutes on is regularization regularization is a method of dealing with overfitting in algorithms like linear regression now in a linear regression model typically overfitting is characterized by large weights of the coefficients the coefficients Babita Babita Babita to ascertain the to use and linear regression is overfitting others normally these values become too large so to deal with overfitting so she looked at this slide hear this shows the values of the different coefficients at different instances and as you see for this is for this is for 103 degree polynomial this is one degree polynomial 3 degree polynomial 9th degree polynomial when we fit the data with a 90 degree polynomial we are seeing that the weights are very coefficients are very large and this is an indicator of overfitting so in order to prevent overfitting one of the methods that we can do with polynomial regression or even a linear equation is used regularization and one way of regularization is to penalize large weight so earlier we have seen that the error function which we try to optimise is the sum of squared errors that is the value of Y minus y hat whole square sum over all the example when is the target value and this is the value of the predicted by now if you want to deal with overfitting in addition to this in the error function you can add another term based on the coefficients the the size of the coefficient so there are different types of regularization L1 regularization L2 regularization IT sector for Linear regression are there are two very popular types of regularization method L2 regularization or Ridge regression,65_Lecture_08___Overfitting.wav
66,it as a factor Lambda by 2w square and square is the L2 norm of the coefficients and in L1 regularization all are so we used the L1 not so this is added to the penalty function and this is a function that you try to optimise so you prefer the way so that the weights were small and the error is not if you do that it can help in preventing overfitting in linear regression with this we come to an end to this class,66_Lecture_08___Overfitting.wav
