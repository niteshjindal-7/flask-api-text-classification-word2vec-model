,content,topics
0,,0_Lecture_04__Evaluation_and_Cross-Validation.wav
1,morning so now we will start part the of module 1 in this module will talk about how to evaluate learning outcomes will be a preliminary lecture on evaluation and how to use cross validation for the evaluation this will be the topic of the current lecture so when you have a learning algorithm you have to understand out so as we saw in the last module that given the hypothesis space age given a training data is your learning algorithm comes up with age,1_Lecture_04__Evaluation_and_Cross-Validation.wav
2,belonging to Capital age now it is important to understand how good ages write so you wanted to evaluate the performance of learning and you can come up with experimental,2_Lecture_04__Evaluation_and_Cross-Validation.wav
3,metric by which evaluate different metrics can be used for example you can have some sort of error metric you can find out what is the errors made if you as you ate as the funk,3_Lecture_04__Evaluation_and_Cross-Validation.wav
4,cat accuracy,4_Lecture_04__Evaluation_and_Cross-Validation.wav
5,Kannu Kat procession Re,5_Lecture_04__Evaluation_and_Cross-Validation.wav
6,and some of these things we will define,6_Lecture_04__Evaluation_and_Cross-Validation.wav
7,and in order to evaluate the error you can evaluate you can find out the error accuracy Precision recall et cetera on a samp,7_Lecture_04__Evaluation_and_Cross-Validation.wav
8,you can evaluate the error,8_Lecture_04__Evaluation_and_Cross-Validation.wav
9,other parameters on the training set but since you are using the training set to come up with the hypothesis the errors or accuracy that you get on the training set is not may not be a reflection of the two error so because of that you use at test,9_Lecture_04__Evaluation_and_Cross-Validation.wav
10,which is distant from the training set and we will talk about cross validation,10_Lecture_04__Evaluation_and_Cross-Validation.wav
11,can be used while training the algorithm in order to the algorithm how you can split the training set into train and test and still use the data that you have to your maximum advantage that can be discussed with me discuss cross,11_Lecture_04__Evaluation_and_Cross-Validation.wav
12,how to evaluate a prediction suppose that come with age and you get an example X and you want to make a prediction on it,12_Lecture_04__Evaluation_and_Cross-Validation.wav
13,you want to make a prediction on x86 and letters a 8 X equal to Y what we can say why had equal to ajax and suppose associated with its the correct value of Y is given,13_Lecture_04__Evaluation_and_Cross-Validation.wav
14,white hat is what you have predicted and why is the actual value of Y now if I had done via same then there is no errors and if that different there is an error if we had differs from why there is an error and we have to discuss how this error is measured there are different ways in which error is measured will talk about some of the absolute error,14_Lecture_04__Evaluation_and_Cross-Validation.wav
15,is measured by,15_Lecture_04__Evaluation_and_Cross-Validation.wav
16,h x minus y by x minus y is the absolute error on a single training example if you have and training examples the absolute error on a single training example if you have multiple training examples lete se and training example then you can take the average,16_Lecture_04__Evaluation_and_Cross-Validation.wav
17,sum of squares,17_Lecture_04__Evaluation_and_Cross-Validation.wav
18,squares error,18_Lecture_04__Evaluation_and_Cross-Validation.wav
19,,19_Lecture_04__Evaluation_and_Cross-Validation.wav
20,8 x minus y whole square and then you take some,20_Lecture_04__Evaluation_and_Cross-Validation.wav
21,and average of so this is sum of squares,21_Lecture_04__Evaluation_and_Cross-Validation.wav
22,absolute error and sum of squares errors are especially useful for regression problem for classification problem you can look at the number of MS class,22_Lecture_04__Evaluation_and_Cross-Validation.wav
23,which can be defined to be one by N Sigma I call 212 n delta H xy so Delta is a function which returns 18 x and y,23_Lecture_04__Evaluation_and_Cross-Validation.wav
24,and 0 if there is a number of MS classification divided by the number of examples on which,24_Lecture_04__Evaluation_and_Cross-Validation.wav
25,Rajasthan difference and measure,25_Lecture_04__Evaluation_and_Cross-Validation.wav
26,,26_Lecture_04__Evaluation_and_Cross-Validation.wav
27,special in classification problem,27_Lecture_04__Evaluation_and_Cross-Validation.wav
28,it is helpful to define a confusion Matrix confusion Matrix you can be not suppose we have a two class classification problem and,28_Lecture_04__Evaluation_and_Cross-Validation.wav
29,a set of examples on which you are testing and,29_Lecture_04__Evaluation_and_Cross-Validation.wav
30,on which side you have the true class and on this right you have the hypothesized,30_Lecture_04__Evaluation_and_Cross-Validation.wav
31,show the true class can be positive or negative,31_Lecture_04__Evaluation_and_Cross-Validation.wav
32,the hypothesized class can be positive on the,32_Lecture_04__Evaluation_and_Cross-Validation.wav
33,dost training examples for which the true classes positive and you also hypothesize positive they can be called TP the number of such examples are put in this box similarity and stands for true negative those examples where the two negative classes for also output as negative bhaiya learning will come here so these are the zones where you are learning about other products correctly but your learning algorithm can also make mistakes and there are two types of mistakes false positive mean,33_Lecture_04__Evaluation_and_Cross-Validation.wav
34,actually negative your learning algorithm is strongly classify them as positive and false negative means the learning algorithm erroneously marks as negative those examples which should have,34_Lecture_04__Evaluation_and_Cross-Validation.wav
35,confusion Matrix and you can have a confusion Matrix if you have more than two classes also for example if you have three classes as an output to a classification problem in have a 3 by 3 confusion Matrix and the diagonals diagonal entries are the ones where the learning algorithm is giving the correct result and the non diagonal entries are where the learning algorithm is giving the,35_Lecture_04__Evaluation_and_Cross-Validation.wav
36,given these entries in the confusion Matrix accuracy can be defined to,36_Lecture_04__Evaluation_and_Cross-Validation.wav
37,sotp aunty and correct result sweat ulysses T P + t and /,37_Lecture_04__Evaluation_and_Cross-Validation.wav
38,just say that some of this column is P some of this column is and so we can say this TP + tea and by people,38_Lecture_04__Evaluation_and_Cross-Validation.wav
39,accuracy we are sometimes interested in other men,39_Lecture_04__Evaluation_and_Cross-Validation.wav
40,for example procession is defined to be out of the examples that the learning out to the marquis positive how many are correct Li positive so Precision is defined to,40_Lecture_04__Evaluation_and_Cross-Validation.wav
41,PP / TP Plus,41_Lecture_04__Evaluation_and_Cross-Validation.wav
42,CP Plus SP is what this row is what the learning algorithm define,42_Lecture_04__Evaluation_and_Cross-Validation.wav
43,TP advance which actually positive defines process,43_Lecture_04__Evaluation_and_Cross-Validation.wav
44,and we have another measure called recall which measures how many of the positive examples the learning algorithm retrieves,44_Lecture_04__Evaluation_and_Cross-Validation.wav
45,taken to be PP by,45_Lecture_04__Evaluation_and_Cross-Validation.wav
46,TP Plus,46_Lecture_04__Evaluation_and_Cross-Validation.wav
47,and this is also called true positive rate similarly you can have a false positive rate these are some ways in which we can avoid with,47_Lecture_04__Evaluation_and_Cross-Validation.wav
48,briefly discuss is on what data we evaluate the,48_Lecture_04__Evaluation_and_Cross-Validation.wav
49,so we are evaluating the learning algorithm on a sample,49_Lecture_04__Evaluation_and_Cross-Validation.wav
50,and the measure of error suppose you're trying to measure error the error that we get on the sample is called sample,50_Lecture_04__Evaluation_and_Cross-Validation.wav
51,and actually later is called the true error,51_Lecture_04__Evaluation_and_Cross-Validation.wav
52,latest look at a classification problem the sample error of a hypothesis f with respect to our target function c and data sample S is the average number of misclassification so Delta effect CX Sigma of delta effects PX is the number of examples where effects and cx2 not agree this is a number of MS classification / n this is the stamp,52_Lecture_04__Evaluation_and_Cross-Validation.wav
53,the true error is defined over the distribution of instances there is a population of instances from which we assume that the samples are drawn and the true error is with respect to this distribution to the true error of a function f is the probability that on this distribution effects and CX will not agree this is the true error so we have the sample error which we measure with respect the sample and there is a true,53_Lecture_04__Evaluation_and_Cross-Validation.wav
54,what we really want to say that whether and when we take the sample error that is not now when you hypothesize a particular function the error that you get comes from different sources the error can come from the limitation in the Representation function the limitation in the hypothesis space and we have discussed that this is due to buyers in representation the error may come because given the hypothesis space the search algorithm is not exhaustive researching hypothesis space but making certain simplification that is called search by the error may be due to the limited size of the sample that you used for testing then it is called variance error and error could be because the features that you are using the vocabulary that you're using is not sufficient to capture everything think about the task then this is called this is called noise so there are different sources of error in a learning algorithm you take a sample and you find the sample error and the sample errors may be different from the true error and,54_Lecture_04__Evaluation_and_Cross-Validation.wav
55,so we have to understand what type of errors can now when you make this estimate from the sample as we saw that there could be error due to bias in the US,55_Lecture_04__Evaluation_and_Cross-Validation.wav
56,the sample error may be a poor estimator of the test error and if you choose the sample to be the training data then you are making a very bad decision about the error because the hypothesis have learnt from the training examples only so what you need to do is to split,56_Lecture_04__Evaluation_and_Cross-Validation.wav
57,exam,57_Lecture_04__Evaluation_and_Cross-Validation.wav
58,you some examples for train,58_Lecture_04__Evaluation_and_Cross-Validation.wav
59,Sanju use and destroy set of examples for,59_Lecture_04__Evaluation_and_Cross-Validation.wav
60,it really both the training set and the test set are drawn from the distribution so the training set and the training examples are used to train the learner and the test set is used to test the,60_Lecture_04__Evaluation_and_Cross-Validation.wav
61,about bias that could also be variance with estimates as we said if the test set is bol there will be variance if you take a small test said coincidentally the accuracy may be very hai aur very low on that set so if you take a larger test set the variance healthy,61_Lecture_04__Evaluation_and_Cross-Validation.wav
62,now we will look at a way of doing the evaluation with Limited training data so suppose you are given some labelled data now you can use up all the date of a train,62_Lecture_04__Evaluation_and_Cross-Validation.wav
63,able to really get a good estimate of the error because you require an independent set so what you should I really do is given the examples we will divide the example into training set and test,63_Lecture_04__Evaluation_and_Cross-Validation.wav
64,but when you do this division the size of the training set well degrees and you will see that it the size of the training set is too small it will give rise to overfitting type of error so I want to use as much of the,64_Lecture_04__Evaluation_and_Cross-Validation.wav
65,example as possible for training and also we don't want to test on a small small test I want to use all the exam,65_Lecture_04__Evaluation_and_Cross-Validation.wav
66,for that we volcker scheme for Cross validation,66_Lecture_04__Evaluation_and_Cross-Validation.wav
67,,67_Lecture_04__Evaluation_and_Cross-Validation.wav
68,social 10 line,68_Lecture_04__Evaluation_and_Cross-Validation.wav
69,given the original set of examples we can first split the data into training set and testing set so that the testing fat is completely destroyed for training during training when you are tuning the model parameters you can use some part of the training set for validation so for the training yours training and validation and after your finished tuning the whole thing all the hypothesis that you output is checked on the test,69_Lecture_04__Evaluation_and_Cross-Validation.wav
70,the standard validation procedure for validation set is used during tuning during training to tune the model parameters after the entire training is over then you check your answer check the accuracy of your hypothesis on the test set so when you do validation if you are letting your data into these parts validation fails to use all the available data so we will come up with the scheme called cross validation and let us look at what is K fold cross,70_Lecture_04__Evaluation_and_Cross-Validation.wav
71,K fold cross validation what we do is that we take the entire training data that is available to us and we split it into clay,71_Lecture_04__Evaluation_and_Cross-Validation.wav
72,split the data into K subsets write when will perform ke experiments so ke rounds of learning in Aundh,72_Lecture_04__Evaluation_and_Cross-Validation.wav
73,we will use so that this is S1 S2 S3 S4 S5 S6 in round I will use Si for,73_Lecture_04__Evaluation_and_Cross-Validation.wav
74,F - 40,74_Lecture_04__Evaluation_and_Cross-Validation.wav
75,,75_Lecture_04__Evaluation_and_Cross-Validation.wav
76,ok so at every round 1 by 1 a of the data is set apart for testing and the remaining examples are used for the train will have ke experiments and after this ke experiments over the test code that we will output is average of the error of this,76_Lecture_04__Evaluation_and_Cross-Validation.wav
77,so we're using the different parts of the data for testing in the different rounds so this is called K fold cross validation this is elastic and by this pictured here this is round 1 round 2 round 3 round 10 Tarik 10 splits of the data in this picture and in round 1 suppose the accuracy is 93% round to 90% round 3 91% and so on till round 10 95% and the final accuracy is the average of the curious in all the round this is called K fold cross validation which makes use of limited data so that the training and testing can be done at all,77_Lecture_04__Evaluation_and_Cross-Validation.wav
78,in machine learning as we will see now and subsequently there is always a treat of is a trade-off between Complex hypothesis Complex representation and simpler hypothesis Complex hypothesis can fit the training data well there more flexible but they have a tendency to overfit the training data and they were poorly on the testator especially the size of the training data is on the other hand simpla hypothesis Mein generalize better but they may not be able to represent complex function if your training data is large then the generalization error will decrease and you can go for complex,78_Lecture_04__Evaluation_and_Cross-Validation.wav
79,we come to the end of the introduction module and with this brief introduction and the different issues that we have touched upon in the next board you will start with the description of some of the learning alphabets,79_Lecture_04__Evaluation_and_Cross-Validation.wav
