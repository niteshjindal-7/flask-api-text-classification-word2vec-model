,content,topics
0,good morning so today we will talk about part 3 of the module and bayesian learning today's topic is nice base in the last class we looked at the base theorem to recapitulate it says the posterior probability of a hypothesis given that data is given by probability of the given age X probability of the hypothesis / the likelihood of the,0_Lecture_19__Naive_Bayes.wav
1,if you think of beehive you are trying to find out letters apply based your Rinku classification you want to find out the classification why given the input x so if you apply Bayes Theorem probability y by x is proportional to as we have seen that for different hypothesis the likelihood of the data is identical bye bye bye given X is proportional to probability of X given y x probability of y,1_Lecture_19__Naive_Bayes.wav
2,that is now X is the input instance and it can be a vector of features so we can write it as probability ex 1X 2x mollis smolin is the number of features,2_Lecture_19__Naive_Bayes.wav
3,bi y x fever now this expression probability X1 X2 X3 given why is a joint Probability and joint probability is difficult to learn and represent because if there are n features even if the features of Boolean there are 2 to the power n possible combinations of the features and you have to store the values corresponding the probability values corresponding to all of them and this is a intractable problem now in nice base which we will talk about today,3_Lecture_19__Naive_Bayes.wav
4,the make a simplifying assumption is that individual are independent given why so in general we can write this as this part of probability ex 1 given y x probability X2 given X 1 by X probability ex and given X1 X2 X3 - 11 x probability of my now in the nice Samson we say that probability of,4_Lecture_19__Naive_Bayes.wav
5,XI,5_Lecture_19__Naive_Bayes.wav
6,given its A Y is equal to Probability excise invoice or its and exchange are independent given by and based on that assumption we can rewrite this as probability of its one given by X probability its to give NY times probability of XM given y x probability of white rice based exam,6_Lecture_19__Naive_Bayes.wav
7,,7_Lecture_19__Naive_Bayes.wav
8,so beer and grooming conditional Independence among the individual attributes X 1 X 2 x,8_Lecture_19__Naive_Bayes.wav
9,and based on this we can do the classification,9_Lecture_19__Naive_Bayes.wav
10,VRL Hue me all the input features are conditionally independent and this can be computed as probability expand given by its two given by X and given y,10_Lecture_19__Naive_Bayes.wav
11,if you look at the slide from Bayes rule if the probability of while taking a particular value by 1 a given the value of the input features X1 X2 Excel is given by probability by equal to y x y ability X1 X2 X3 given Y equal to Y Ken / denominator which is independent of by assuming conditional Independence we get it is probability Y equal to y k x product of probability exercise given why and based on this we can get a classifier the classified says that given a new example that classification why new is that why ke,11_Lecture_19__Naive_Bayes.wav
12,for which this quantity is maximized that is the product over all the training all the attributes probability XI given why of the training example of the new instance probability XI new given by given why,12_Lecture_19__Naive_Bayes.wav
13,X probability of,13_Lecture_19__Naive_Bayes.wav
14,X probability of the prior probability by School to buy this time this is maximum she want to take that classification for which the probability of people to work at times this product is,14_Lecture_19__Naive_Bayes.wav
15,the individual probabilities that we required to compute this what do we notice is that for each value of wisepos why it takes two values Plus and minus we need to know for all such cases we need to know probability of Y equal to true probability of Y and for each feature XI we need to know probability of Exide given Y equal to plus probability of exile given by 2 minus x I can have different value suppose Exide has three values for each of the values of Exide we have to find out what estimates viability exercise was to value one given Y equal to plus probability exercise equal to value to give n y equal to plus probability exercise for the value of three given by 2 + similarly probability exercise call to value one given by 2 minus answer so what we have is that the number of probabilities that we request to calculate let us assume that X 1 X 2 x n and y are binary attributes for waiver request to value is actually one of them will suffice father we can get,15_Lecture_19__Naive_Bayes.wav
16,"if each exercise has two values sofa x i will require proof for each value of Y if you know if I am truly can also get excited to fall so we have exited the true given my phone to excite a given by virtue of also two values for each excite the total 2n + 2 values to N + 1 by others will suffice to the present this probability which is very much possible and this is a simple this gives us a simple algorithm for classification called nice now let us see what is the resulting nice this when you have discrete values of x for which you can look at the slide which gives the outline of the nice base very simple so when we trained by space we take the training set and for each value by ke suppose there are n values of y k we need to estimate only and -1 parameters because we're capable to 1 by 1 1 1 2 2 Form B equal to 3 in the probabilities of some of the probabilities once we need to estimate only and minus one of the palace anyway for its value why ke we will estimate by ke as the probability of Y equal to buy this is the prior probability how do we estimate that I suppose you are given hundred training examples and y k has three values V1 V2 V3 y k equal to V1 for 70 of the examples V2 for 20 of the examples V3 for 10 of the examples then probability of Y equal to bike it would be estimated to be 7,500 for value to 20 100 for value 310 by 100 or some other estimate measure which will again talk about now for each value check size of each attribute Exide for the shared instance for each attribute Exide for each exercise we will estimate theta i j k and the parameter that probability that X equal to",16_Lecture_19__Naive_Bayes.wav
17,it's given wife,17_Lecture_19__Naive_Bayes.wav
18,,18_Lecture_19__Naive_Bayes.wav
19,so if we get its different values then this will require in minus one estimates,19_Lecture_19__Naive_Bayes.wav
20,if each x-values how many estimates required to into an estimates so small excited is the different actor different values that attribute excited and take so these probabilities we need to estimate now based on this estimate we can have the class or in the training phase will learn these estimates from the training exam and after we have learnt the estimate you look at the slide again we can classify a new instance X new as,20_Lecture_19__Naive_Bayes.wav
21,class VI new is that why K for which this expression is maximized probability Y equal to X product over I probability XI new given why was it as we have seen and in terms of the parameters simplified values of the parameters that you have written this is given by this ok so this is when I space out for the case where all the attributes are discrete valued or nominal value before we proceed let us look at an exam,21_Lecture_19__Naive_Bayes.wav
22,ab and one more thing that time forgot to tell you is that when we estimate these parameters probability of exile given why or probability why we may sometimes come across a situation where in the training example the count for computing the probability is zero then we have a problem you see in the classification we have a product that your computer you are taking the product of this Probability and the product of these probabilities now if we have insufficient training instances there may be a case where probability exercise equal to excited given Y equal to y k i know there is no telling example for a particular y k for which exercise is a particular value of excited so this value if we do nice estimation of the probabilities by frequency counting this probability will become zero and if Apne product 1 probability term becomes zero the entire product becomes zero in order to avoid that we need to do something calls moving in order to avoid such situations and so what we do is that when we do the estimating of the different parameters for example when we try to estimate by for pi ke we look at the count the number of X equal to y k divided by total number of data instances for teacher check estimate which is probability exercise equal to smaller size k y equal to y k v count the number of instances for which Capital IQ cycle small excise E and Y equal to y k divided by the number of instances for which why this is the simple formula for maximum likelihood Estimation and in this case that it is possible that is special in computing theta i j k sometimes we will get the numerator 0 and in order to avoid that we introduce moving where we we initialise some small probability to each of these values and we will see in a later slide that we can add + 12 each of the Dew bottle and compensated by Assam value added to the denominator but before that let us look at an example this is an example taken from Mitchell's book on visual learning where we have a description of different days and the attributes Outlook temperature humidity and wind is are the climate attributes of different ways and the target attribute is weather it is a good day for playing tennis and given this training examples to apply nice paste it in the training Phase II will output the probabilities so if outlook is Sunny play equal to yes given out location is to buy 9 play call to know given Outlook Assamese 3 by 5 lakeville to yes given outlook is overcast is 499 play call to know given out over cast is zero by 5 and so on so these are the values that we get by finding the maximum likelihood instance estimation from the data and these are the proprietor abilities for playing tennis and for not playing tennis and these are the values of theta is so these can be estimated using the previous maximum likelihood estimate form Allah that we have sea and this will get this,22_Lecture_19__Naive_Bayes.wav
23,now this is what happens in that training fees in the test case you are given a new instance and you have to predict its limit for example suppose the new instances outlook is Sunny temperature equal to cool humidity is high and wind is strong and paste on this probability values that we have seen in the previous page we can do the decision with the map route and we find out that probability was given exprime turns out to be point 0053 probability no given exprime is points 0206 and because probability of a given explain is less than probability no given EXP time will able explain to the no so this is a simple application of nice face was an extremely simple look at the training set you estimate estimate of the different parameters then given the test set to apply the,23_Lecture_19__Naive_Bayes.wav
24,now as I mentioned that if you are unlucky the estimate for probability exercise given vi may be zero because they may be that some particular attribute value is not represented for a particular class because we do not have sufficient training exam to alleviate the fact we can use smoothing there are many approaches for smoothing in loading many sophisticated the process but we will introduce only a simplest approach for supporting what we do is that for every,24_Lecture_19__Naive_Bayes.wav
25,probability estimates that we do we are a number of add some number that number could be one or could be a fraction L which corresponds to some imaginary instance and because while adding a small positive value to the numerator we must compensate by adding a link to our to the denominator Where are these the number of possible values of white so that the sum of the bike is become remained one similarly to estimate ke we can add l here and in the denominator we must compensate by adding l m so that the sum of the tide over a particular value of I will be equal to 1 so this is smoothing which we can apply in order to alleviate the problem due to zero probability important as I'm sure that we made in nice space is that the excise are conditionally independent given why but this is not really a valid exams and it often does not hold we can often use the right classification even when but even if this assumption does not always hold nice space is surprisingly quite effective in a given its simplicity it surprisingly quite effective in many number of cases and often it turns out that even if the assumption is not well it nice Facebook is the correct classification because my face is not we're not really using this assumption to find the exact ability but to choose between the different possible classes and in that they rise face works quite well in many cases for example and text classification nice place is a very standard algorithm is applied and the surprisingly well as fast and because even as options are not write its example now we will look at the case where the input attributes are continuous value we have so far seeing where the both the input attribute and output at to do the discrete value what is the input attribute is continuous if the input attribute a continuous valued we can assume that the conditional probability of that attribute is can be modelled by a Gaussian and based on that we can have a nice face so what is a will request you to look at the slide suppose you have continuous valued features then you can model the conditional probability probability exercise equal to X given Y equal to work as a a normal distribution or porcelain distribution which is given by the standard Formula 1 by root over to pi Sigma square it to the power minus x minus y k square divided by 2 Sigma,25_Lecture_19__Naive_Bayes.wav
26,we assume that sometimes we made you that this variant Sigma square term here is independent of Y or independent of XI or both we can assume that the same for all excited why you are you can assume that the same for all why and so on this makes the model has less number of parameters if you wish,26_Lecture_19__Naive_Bayes.wav
27,but under the circumstances we can have the gausul Ne face is in the training phase will look at the training data set and from the training data set we estimate pi ke as before 5 case probability Y equal to Y kitte prior probability of the different classes this is estimated as before but for each attribute XI vs Team at the new year and CIG mic for each exercise for a particular y k in order to find probability exercise and why we estimate new and Sigma from the data and after we have done their customers in the testing phase we can classify the new instance X new as y new is that why K for which I know this is the standard for wheel of a nice place your probability by equal to it was estimated the spike and for probability XI new given by equal to y k views on normal distribution over XI new music my music my with the parameters found in the training face so based on that we can apply the pores on face,27_Lecture_19__Naive_Bayes.wav
28,now in the Austrian nice place is used for continuous x,28_Lecture_19__Naive_Bayes.wav
29,and we can also have so this is used particularly in case where X is continuous and why is discrete and the maximum likelihood estimators via said they estimate of mew and Sigma are given by new is given by estimate of the mean of the sample and this is a standard way of doing maximum likelihood estimate and Sigma is also taken by the standard deviation of the sample As given by the slide now in soap to conclude nice is is a very simple algorithm which makes the nice as I'm sure that the different attributes at Sahi and exchange at independent given the value of the class this assumption is not always realistic but it simplifies the computations greatly and in many cases the resulting urban is quite good even though it is so simple even though the Independence exemption is not always satisfied in practice them as attributes of uncorrelated we get white good results now so but we cannot always apply nai space and as we have seen we cannot do the full joint distribution Probability x1x and given why it is it it's not tractable to really to this and to alleviate this we study bayesian networks in bayesian networks,29_Lecture_19__Naive_Bayes.wav
30,we can strike a balance we can we we need not make full Independence assumptions or full dependency example rather we denote the causal relationships and cond,30_Lecture_19__Naive_Bayes.wav
31,Independence specific conditional independence of the different attributes,31_Lecture_19__Naive_Bayes.wav
32,and in belief networks we also did not causal relationships with actual relations and actual Independence is between the attributes and based on this week and based on this we can I get different learning algorithms which are do not make as my exam tions as nai space but can capture the relationships in the Tommy and it is a very it is not once the topic and we have different types of bayesian networks we have,32_Lecture_19__Naive_Bayes.wav
33,belief networks also called,33_Lecture_19__Naive_Bayes.wav
34,direct graphical model and we also have another type of networks bayesian networks which are called undirected graphical models and these can capture different relationships but today have a finish this topic thank you very much,34_Lecture_19__Naive_Bayes.wav
