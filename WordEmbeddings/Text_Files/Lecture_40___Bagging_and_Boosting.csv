,content,topics
0,,0_Lecture_40___Bagging_and_Boosting.wav
1,good morning today we will have the second part of the lecture an ensemble learning in the last class we give introduction to the different ensemble learning methods today I will talk about to specific methods bagging and boosting which are methods for ensemble learning bagging is a method for ensemble learning batting stance for bootstrap aggregation so we saw that,1_Lecture_40___Bagging_and_Boosting.wav
2,in order to have in order to use an ensemble of learners and be able to combine the learners and get better accuracy what is needed is a set of learners,2_Lecture_40___Bagging_and_Boosting.wav
3,which make independent errors you want a set of learners which make independent errors and we discussed in the last class in order to force the learners to make independent errors you can make them use different data samples you can make them use different parameters of the earth are you can make them use different and the output can be combined by some form of boating now in boost up aggregation what is done is that first of all the samples are generated such that the samples are different from each other however suppose you have a training set as on its quality and if you are going to have in learners,3_Lecture_40___Bagging_and_Boosting.wav
4,which the training set into K partitions and use one for each other however often it is the case that the size of the training set is not large so if you make it smaller by splitting it into ke different sets the individual training sets will be small and as we have observed while talking about the foundations of machine learning is that if you use a learner on a small training example the learner 10 overfit it can have high variance then it will not generalize well so we want to be able to use,4_Lecture_40___Bagging_and_Boosting.wav
5,we will not be able to afford to have fully independent data but we can sample randomly from the data so that the and we do that by by a method of Sampling instead of dividing it into K join the groups we treat this as the pool and from this pool December,5_Lecture_40___Bagging_and_Boosting.wav
6,Diwan t 2D 3D ke different data substance which is samples from the original data have some income but they also will have a different but now as we have discussed that are so what we do is that you get the draw random samples with,6_Lecture_40___Bagging_and_Boosting.wav
7,for example if you desire that each of these will have an examples we randomly draw M samples from the with replacement that is we can have the same instance repeated several times and some instances may not appear in a particular and so this device can be different we have discussed in the last class that in order that ensembles will work we Desire that these learners may have high variance and by combining these ke learners we can reduce the variance in a in certain cases by one by so we can start with learners with high variance and we also discuss that in order that the different learners produced from hypothesis we want the law learners to be unstable stable learners in a respective of What data there to get the output will be similar that is they have low variance learners with high variance are unstable among the algorithms that we have studied decision tree then neural network they are unstable albums decision tree neural network and stable given different data they may give different where as algorithms like K nearest neighbour is more,7_Lecture_40___Bagging_and_Boosting.wav
8,when you want to use ensembles or specifically bagging we wish to use unstable learn and we use bootstrapping to generate the training sets which generate this ke training sets and we train so once we get this training sets in this study we have the instances 12345678 and we get the ones and even may have 15123 and D2 may have,8_Lecture_40___Bagging_and_Boosting.wav
9,641654 D3,9_Lecture_40___Bagging_and_Boosting.wav
10,1511 this different different data sets and V trainer learn L1 L2 LK on this different data sets and negative model and these models have to be combined by so we use bootstrapping to generate this ke learners and trainer based learning with rich and combine the output by hoti so in banking what we do is that we do sampling in order to get this individual data sets we do sampling with replacement,10_Lecture_40___Bagging_and_Boosting.wav
11,,11_Lecture_40___Bagging_and_Boosting.wav
12,inform each sample with builder bootstrap sample and if you look at each instance in the original data set what is the probability that it is included in the sun suppose a sample of size and sample of size,12_Lecture_40___Bagging_and_Boosting.wav
13,,13_Lecture_40___Bagging_and_Boosting.wav
14,I am suppose this data set is also of size,14_Lecture_40___Bagging_and_Boosting.wav
15,you should take any instance what is the probability that when you draw randomly from this data set from the sample what is the probability that a particular instance will get included in it is 1 minus 1 by and if you do then such draws so if this is an science and and each of the samples were also seisen then,15_Lecture_40___Bagging_and_Boosting.wav
16,this is the probability that a particular instance will get included in the sun,16_Lecture_40___Bagging_and_Boosting.wav
17,and this particular sample will get selected and we can see that,17_Lecture_40___Bagging_and_Boosting.wav
18,can be approximated by a inverse and we can see that if we use and random sample about 63% of the instances about 60% of the instances will occur in each of these samples 53% of the Unique instances and other instances will be repeated this you can find,18_Lecture_40___Bagging_and_Boosting.wav
19,,19_Lecture_40___Bagging_and_Boosting.wav
20,this is about packing next we come to a second ensemble algorithm which we call boosting and bagging we are generating the data samples D1 D2 from the original data set and they are all getting,20_Lecture_40___Bagging_and_Boosting.wav
21,the same probability of getting included in any of these,21_Lecture_40___Bagging_and_Boosting.wav
22,Navin boosting what we do is that again we have a spam now when we try to get the sample from what we do is that we assign a probability with each instance every instance is assigned the probability supposed to start with there are instances and initially we may have we will give equal probability to all the instances of none of them has a probability of 1 by,22_Lecture_40___Bagging_and_Boosting.wav
23,in the first iteration you choose Deewan based on this uniform probability distribution and you get now only one you training a learner and this learner you apply to all this,23_Lecture_40___Bagging_and_Boosting.wav
24,learner may have 100% accuracy on this instance if that is the case then is very good learner you know it will have you know all of the metal to connect but you most most often what will happen is that this learner will labour some of these instances in D correctly and,24_Lecture_40___Bagging_and_Boosting.wav
25,Diva,25_Lecture_40___Bagging_and_Boosting.wav
26,correctly labels is wrongly label this correctly label this correctly labels this wrongly labels is wrongly labels this correctly labels,26_Lecture_40___Bagging_and_Boosting.wav
27,what we want is we want then next learner to have independent errors and we want the next learn to do well on those instances that the concern has not done well what we will do is we will change the probability distribution and more specifically we identify those instances which were wrongly labelled by,27_Lecture_40___Bagging_and_Boosting.wav
28,we will increase the probability of these instances and those which were correctly labels within proportionately reduce the probabilities what we are going to do is that we want to hike the probability of those instances which we were not correctly classified by learner and so we will get will have to send it is it but now the probability distribution width change now we will now sampadi to according to this probability distribution and we will get a subsample and this sab sample is more likely to contain 256 than 134,28_Lecture_40___Bagging_and_Boosting.wav
29,l and D to apply learner apply the learning and get the learner,29_Lecture_40___Bagging_and_Boosting.wav
30,again we apply into on this training instances and find out which ones to does error on supposed to wrongly classified as 27 and 3 and the rest it does correctly so what you are going to do now is to increase the probability of 2 3 and 7 beyond what was the current Probability and reduced the probability of the rest to keep the sum of the probabilities equal,30_Lecture_40___Bagging_and_Boosting.wav
31,,31_Lecture_40___Bagging_and_Boosting.wav
32,based on that we are going to find D3 on which were going to try and learn like this will get the different learners different data and the different lord and then we will combine these different outputs of the different learners by voting and why voting we will assign weights to each other and the weight will be somewhat related to how good is the learner was so this is the basic boosting so in boosting is an iterative procedure we start with a uniform probability distribution on the given training instances and we adaptively change the distribution of the train,32_Lecture_40___Bagging_and_Boosting.wav
33,initially all the training instances have equal weight after each round of boosting the weights kit,33_Lecture_40___Bagging_and_Boosting.wav
34,hypothesis on a learner and we assign strength to strength and the strength is used to decide the weight of the 14 and the final classify is a linear combination of these different learning hypothesis,34_Lecture_40___Bagging_and_Boosting.wav
35,weighted by this so what is that the individual but we have said they are also the individual learners may be weak learners the unstable weak learners in fact is better than they must have more than 50% accuracy for a 2 Plus,35_Lecture_40___Bagging_and_Boosting.wav
36,there are several boosting out when most common reason for boosting is added with I will talk about in today's class so we have seen that boosting can turn week out of them into a strong learner so as a boost takes a sample has Input and lete se we call this sample,36_Lecture_40___Bagging_and_Boosting.wav
37,contents X1 Y1 X2 Y2 via training example X is a vector of the input attributes and Y is output attribute and we have a weight for each training exam initially,37_Lecture_40___Bagging_and_Boosting.wav
38,b0i is the weight of the IAF training example initially d0i equal to 1 by M for all between 1 to initially at X 0 only learners have equal weight which is equal to 1 by Andre juicer we do capital T number of iterations in the first iteration reconstruct the one on X1 X2 X3 m v construct Divas,38_Lecture_40___Bagging_and_Boosting.wav
39,and buy something from this as using the distribution,39_Lecture_40___Bagging_and_Boosting.wav
40,write a script day so this is D zero so we get demand or let me call this,40_Lecture_40___Bagging_and_Boosting.wav
41,what we do is that we update,41_Lecture_40___Bagging_and_Boosting.wav
42,,42_Lecture_40___Bagging_and_Boosting.wav
43,so for those for which from S1 we get a particular,43_Lecture_40___Bagging_and_Boosting.wav
44,so we find out which training examples L1 classify is correctly for those training examples that l oneplus 5 wrongly we increased the one the one I is increase for those examples on which wrongly so we update and from the oven and yeast reproduce or not,44_Lecture_40___Bagging_and_Boosting.wav
45,latest a when we applied the learner L1 on the error is a side and what can I learn l to an initial rest error is a silent to similarly is 3 Producers and 3 with another,45_Lecture_40___Bagging_and_Boosting.wav
46,epsolin t is the error over the current not over the original data but over the current,46_Lecture_40___Bagging_and_Boosting.wav
47,,47_Lecture_40___Bagging_and_Boosting.wav
48,latest just look at this book and the slide to the boosting so we are given as equal to X1 Y1 X2 Y2 x m y m the set of input attributes by is the output attribute and initialise the distribution to 1 by,48_Lecture_40___Bagging_and_Boosting.wav
49,initialise the distribution to 1 by and then we run the situation or equal to 1,49_Lecture_40___Bagging_and_Boosting.wav
50,so We trained the weak learner,50_Lecture_40___Bagging_and_Boosting.wav
51,using DT and get,51_Lecture_40___Bagging_and_Boosting.wav
52,how we classify,52_Lecture_40___Bagging_and_Boosting.wav
53,,53_Lecture_40___Bagging_and_Boosting.wav
54,letters college age,54_Lecture_40___Bagging_and_Boosting.wav
55,shoes Alpha,55_Lecture_40___Bagging_and_Boosting.wav
56,as the weight associated with it will see later how Alpha it is chosen and we update the distribution so we get DT plus one I am the update and distribution which is obtained from the ti,56_Lecture_40___Bagging_and_Boosting.wav
57,exponential of minus Alpha,57_Lecture_40___Bagging_and_Boosting.wav
58,,58_Lecture_40___Bagging_and_Boosting.wav
59,htx so why is the target output of the particular training example I and HT exercise is what is output by the learner HD so if they agree with this is one if that do not agree it is minus 1 divided by a normalisation factor this normalisation factor is chosen such that the sum of the probabilities some of this overall I is equal to 1 so that is a normalisation factor now we do this capital T number of times and final classified that we output,59_Lecture_40___Bagging_and_Boosting.wav
60,if given by X equal to,60_Lecture_40___Bagging_and_Boosting.wav
61,sign,61_Lecture_40___Bagging_and_Boosting.wav
62,Sigma Alpha it so H1 H2 H3 at the different classifiers set an output and HT of the different classifiers they are weighted by Alpha and the total is computer if the total is positive then the class is positive if the total is negative the classes negative ok so this is the basic adaboost algorithm you can see the slide here where the given this is our sample why is either - 1 + 1 this is the initialisation of the probability distribution and the run this algorithm for capital T number of iterations in each iteration we use the current probability distribution to generate the data and on the data which train are classified HT we find alpha t i will show you how Alpha it is computer and based on that the new probability distribution is computed which is DT plus one I and Z is the normalisation factor so to make sure that the sum of this is equal to 1 and the final classify is a taken as a Boat of the different age is weighted by alpha t,62_Lecture_40___Bagging_and_Boosting.wav
63,how it is computed first we need to compute ypsilanti which is the error of the,63_Lecture_40___Bagging_and_Boosting.wav
64,hypothesis HT with the currents and it is summation over the training examples DTI is the probability of the IAF training example in the sample and delta H T I not equal to Y is,64_Lecture_40___Bagging_and_Boosting.wav
65,why is miss-classified this is one otherwise this is 0 so what are doing is that we are taking the number of MS classifications each in a with attacking the weighted sum of the number of MS classifications weighted by the probability that is it silently and Alpha it is computed as half log of 1 - sin and b is a measure of the error and Alpha is a measure of the accuracy of the particular hypothesis HT and in the ensemble when we are combining the votes of the different hypothesis we are combining based on it so this is the mechanism which is followed by the adaboost,65_Lecture_40___Bagging_and_Boosting.wav
66,which is the very popular ensemble learning about a little bit more about,66_Lecture_40___Bagging_and_Boosting.wav
67,this ensembling works so if you are give,67_Lecture_40___Bagging_and_Boosting.wav
68,a number of be classified as they can be combined to get a strong classify this we classify as must have error better than London ypsilanti must be less than 4.5 for a 2 + problem it can be shown that this adaboost will achieve zero training error exponentially fast under the assumption that the errors and independent errors of the different hypothesis,68_Lecture_40___Bagging_and_Boosting.wav
69,it is shown that one bye,69_Lecture_40___Bagging_and_Boosting.wav
70,Delta X I not equal to Y ko 21 came this is the average error it can be shown the average error of this ensemble produced by and adaboost is less than equal to the product,70_Lecture_40___Bagging_and_Boosting.wav
71,over all the members of the answer to the product of all the learners that we are using in the answer book x ZTE which is less than equal to exponential of minus 2 equal to 12 capital 30 is the total number of iterations in adaboost half - EP 78 has to be less than 4.5 but whatever it is this is a fraction less than 1 and as you take this product this in this will become as it increases this will prob 20 exponentially fast and therefore adaboost can this algorithm if the learners are independent it can become very good as you increase the number of iterations so we are not going into the theory but this is a result which has been proved now we will show please look at the slide we will in a straight other most how it works so suppose this is the initial data we have,71_Lecture_40___Bagging_and_Boosting.wav
72,data points 35 of them are positive which is given by Green and given by blue a positive and the 5 green ones and negative and all of them each one of them has two ability point 1 the sum of the probabilities is,72_Lecture_40___Bagging_and_Boosting.wav
73,when for boosting you select the first set of training examples and the ones within squares get selected the data points that you use for training and based on this you get a classified and this classified letters,73_Lecture_40___Bagging_and_Boosting.wav
74,these three as positive and these A7 as negative so we see that this correctly labelled as positive but here these two are wrongly label does negative so now this probability distribution is updated so these to wear adaboost makes an error there probability is boosted using the formula of boosting and they get boosted 2.46 23 and to balance this the probability of these eight examples are reduced and they become points 0094 based on these values the value of Epsilon and Alpha can be calculated and Alpha happens to be 1.94,74_Lecture_40___Bagging_and_Boosting.wav
75,now what happens is the next round takes place in the next round we have this new distribution from which the data points or sample letters in now these six points which are in square samples and supports the learn the to come up with this be to now be to again Maxim errors and based on that error in a bi to make celeron these three examples and paste on,75_Lecture_40___Bagging_and_Boosting.wav
76,the probabilities of these examples are increased and the other circuit used and Alpha in this case is computed to be 2.9,76_Lecture_40___Bagging_and_Boosting.wav
77,third round takes place on this new distribution and letters a third round choose some samples and B3 is this classify and there is an error on these values this middle values and so there are these values based on that the probabilities are recomputed and Alpha is now computer to be 3.8,77_Lecture_40___Bagging_and_Boosting.wav
78,finally these three classified ads are combined with Alpha 1 alpha 2 alpha 3 and overall classified now,78_Lecture_40___Bagging_and_Boosting.wav
79,able to label designer positive and negative so this in this particular case boosting is able to combine district classified into a classified which is fully activated it may not always happen that you get it something but what is interesting to see that this is classified as one in your classified as they just made a division of this interval where was the ensemble classifier does not belong to the same type of classify it is able to identify their positive examples here and negative exam so this is an illustration of the adipose with this I stop today's lecture and their on boosting,79_Lecture_40___Bagging_and_Boosting.wav
