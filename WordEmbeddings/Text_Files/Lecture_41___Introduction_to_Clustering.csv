,content,topics
0,,0_Lecture_41___Introduction_to_Clustering.wav
1,good morning so today we will talk about clustering in the earlier classes so far in this course we have mainly talked about supervised learning and unsupervised learning we have some data which are labelled and we try to learn a function we want we try to come up with a method to label and seen instances correctly today we will know that and supervisor,1_Lecture_41___Introduction_to_Clustering.wav
2,and we will look,2_Lecture_41___Introduction_to_Clustering.wav
3,one specific type of unsupervised learning which is called cluster,3_Lecture_41___Introduction_to_Clustering.wav
4,people introduce what clustering is and in the subsequent classes are give illustrations of different specific clustering methods in unsupervised learning as we have discussed in the first class of this course that we have data but those data have no labels so we have unlabeled data instance and in unsupervised learning we want to find hidden structure in the unlabeled data we want to explore the data to find some structure in there but there is no old standard or there is no no labels to say that this is what you get now why would we be interested in this when we have a large amount of data and less we are able to group the data we will not pay able to do studies and to proper exploration of the date for example let us take the example of the plant and animal kingdom biologists have studied the different species of plants and animals and come up with,4_Lecture_41___Introduction_to_Clustering.wav
5,grouping and these groups can be called clusters simplest animals into vertebrates and invertebrates and vertebrates of mammals special 17 these labels were not given to the early scientists but they have come up with this groupings based on species which share similar and based on that this grouping,5_Lecture_41___Introduction_to_Clustering.wav
6,clustering is the most popular type of unsupervised learning that a few other types of unsupervised learning which will not talk about in this class in clustering the task is given a set of data instances so here you do not have earlier our sample content XI why the sample will contain only its 1X 2x 3x only the data points are given and no output or no output label is given the given a set of these instances of objects in clustering we want to group the objects into clusters so we can find the cluster C1 C2 C3 so C1 will contain,6_Lecture_41___Introduction_to_Clustering.wav
7,number x x 2 x 3 x 7 go in C 1 X 1 x 4 x 10 x 11 going C2 and then x 5,7_Lecture_41___Introduction_to_Clustering.wav
8,8 x 9 x 12,8_Lecture_41___Introduction_to_Clustering.wav
9,suppose we were three different clusters with come up with a given this we come up with groups and how do we come up with groups so instances which belong to the same group would be in some ways similar to each other 62387 should have some similarity among Each Other X 1 x 4 x 10 x 11 should have some similarity with each other and its 3 and X 4 elements which belong to different clusters will be in some way this similarity Each Other based on this we can come up it grow,9_Lecture_41___Introduction_to_Clustering.wav
10,Dr stream is useful for many applications for example it can be used to automatically organise data for example the plant species or animal species or news documents for book,10_Lecture_41___Introduction_to_Clustering.wav
11,it can be used for understanding hidden structure in data and sometimes clustering is used as preprocessing for further analysis of the data now look let us look and the slide to look at an application of news plus some of you will have used Google news in Google news will notice that the different new stories a grouped for examples of this is one new story is that about the bombing of Istanbul Airport on 29th June 2016 and the new stories under that they are grouped together so this is unsupervised clustering because this particular new story was not given as a label to this but the related news items were grouped together so Google news is an example of a cyst atom which does news + 3 then this is an example of clustering of gene expression data so these jeans are clustered based on the different the gene expression and other applications for example as I've talked about in Biology the classification of Plant and Animal kingdom given their features in marketing customer segmentation based on the database of customer data containing their properties and passed by records is a very useful to marketing companies because based on the grouping of the customers they can decide what type of promotions to target to its first,11_Lecture_41___Introduction_to_Clustering.wav
12,sample is clustering of web log data to discover groups of similar access patterns and The Fourth example is to recognise communities in social networks based on their similarities let us look at in order to talk about clustering algorithms let us look at some example data so these are the data points only the data points are given their labels are not given but in this particular data using visually that there are four natural clusters and what are clustering algorithm is required to do is take this and come up with this for testis ideally this is what the clustering algorithm,12_Lecture_41___Introduction_to_Clustering.wav
13,different aspects of clustering first of all there is a clustering algorithm and there are different types of clustering out we will discuss a few of the clustering algorithms in the next few classes there are two terms which are partition Allah divisional in nature which takes the data and advise them into a number of groups K means is an example of a partitional clustering and then there are hierarchical clustering algorithms which hadagali divide the data into clusters Haider ke khiladi baby based,13_Lecture_41___Introduction_to_Clustering.wav
14,top down method or bottom up method which is done adaptive hierarchical clustering again we will see that in the next class 13 other methods like module based methods for example mixture of apostles density based method like this condition so that is a clustering and secondly there is the distance of similarity function which the clustering and user tries to optimise we told earlier that in a cluster the elements in the cluster a similar to each other and the elements belonging to different clusters are different from each other and to measure how similar or dissimilar to elements are we have to use a metric of similarity of the similarity measure from possible measures are euclidean distance cosine distance Pearson correlation coefficient,14_Lecture_41___Introduction_to_Clustering.wav
15,only one have to have a way of evaluating how good the clusters is for that one can look at different methods for example one can try to,15_Lecture_41___Introduction_to_Clustering.wav
16,minimise the intracluster distance of elements which belong to the same cluster and maximize the intercluster just elements that belong to different clusters the quality of a clustering results depends on the algorithm the distance function used and the application form which you are you,16_Lecture_41___Introduction_to_Clustering.wav
17,son of the major clustering approaches partitioning based method which involves constructing radius partitions hierarchical methods which creates the hierarchical decomposition of the set of objects model based methods with hypothesize a model for each cluster and find the best fit of model to data density based clustering algorithms which are guided by connectivity and density function graph theoretic clustering based on the under construction of a graph and looking at some draught your itec measures like me but these are some of the different clustering approach this some few of them will talk about it,17_Lecture_41___Introduction_to_Clustering.wav
18,partitioning and construct the partition of given and data sample,18_Lecture_41___Introduction_to_Clustering.wav
19,partitioning algorithm will construct the partition of these into K clusters so ke is given to the it will find K clusters that optimises the chosen,19_Lecture_41___Introduction_to_Clustering.wav
20,may come up with the Global optimum of the chosen criteria for Yuva Kyon rustic method and come up with the local Optima so we will explore in detail the k-means algorithm which comes up with a local optimum based on certain,20_Lecture_41___Introduction_to_Clustering.wav
21,the second type of clustering algorithms is hierarchical clustering for example animals may be grouped into vertebrates and invertebrates and vertebrates may be broken into fish reptiles amphibians mammals also want this is an example of using a tree or had a chemical + thing we will look at some methods had a ketoplast which produces a nested sequence of cluster,21_Lecture_41___Introduction_to_Clustering.wav
22,you can for example you can use a partition algorithm and recursively applied it to get a hierarchical clustering for you made to a bottom-up clustering when you start with a large number with each cluster containing one item and repeatedly go on marking the clusters and you get oneplus and as a result you can get understood free will talk more about it in a letter class third type of clustering is a model-based cluster where given the data points you hypothesize a model for example you can think of each cluster being represented by a Gaussian distribution with mean and standard deviation,22_Lecture_41___Introduction_to_Clustering.wav
23,and you try to fit the data to the model and for example you can come up it is 3,23_Lecture_41___Introduction_to_Clustering.wav
24,type of algorithm is called density based clustering will not talk about it in this class we don't have time but it is based on the similarity in a based on the inner based on the density of a region the density of a region is a number of instances in a region in feature space it locates regions of high density and connects those points together so DB scan is a popular density based clustering then we have graph theoretic clustering which takes notes to represent the different items and the weights of the ages is based on the similarity of the items and based on this a graph is constructed and certain graph how to do so used to find connected find the strong links from connected components for example looking for minimum cut Nagar again we will not talk about this type of algorithms,24_Lecture_41___Introduction_to_Clustering.wav
25,third aspect of a class to now to them is the metric that we use a distance metric or a similarity metric so there are certain distance Matrix for example the main course ki family of distance measures were given two items x i x,25_Lecture_41___Introduction_to_Clustering.wav
26,the mean course K distance between VXI and exchange is computed,26_Lecture_41___Introduction_to_Clustering.wav
27,the submission,27_Lecture_41___Introduction_to_Clustering.wav
28,what the training example,28_Lecture_41___Introduction_to_Clustering.wav
29,it takes for submission over Sorry the number of input attributes lete se n is the input attribute excise - xjs to the power P the whole thing to the power 1 by this is the main cause ke Matric so1 popular enough to side ft equal to 2 you get the euclidean distance in euclidean distance what you have is euclidean distance of exchange is mean by Oscar distance with equal to 2 which gives you root over Sigma is equal to 12,29_Lecture_41___Introduction_to_Clustering.wav
30,Siwan mine,30_Lecture_41___Introduction_to_Clustering.wav
31,XI -1 yes you can have the Manhattan distance between an exchange,31_Lecture_41___Introduction_to_Clustering.wav
32,summation over is equal to 12 and excise -62 self equal to 1 you get Manhattan distance which is defined like this speak to you get euclidean distance and you can also cause people to free phone so this is one type of distance Patrick S metric which is often used when you work with text data in the bag of word model is the cosine distance metric so given to,32_Lecture_41___Introduction_to_Clustering.wav
33,object 6nx chair the vectors of these two objects you find the cosine between these two vectors and the cosine between these two vectors can be computed so the cause of XI XC can be computed as the dot product of two vectors Exide or texture /,33_Lecture_41___Introduction_to_Clustering.wav
34,Root over the sum of the coefficients of these which we can write,34_Lecture_41___Introduction_to_Clustering.wav
35,montex idot motixx the normalisation factor and this is that so basically what you're doing is that you are finding the cosine between the two vectors,35_Lecture_41___Introduction_to_Clustering.wav
36,Matrix 13 correlation coefficients which are scale invariant and there a difference such measures for example mahalanobis distant Pearson correlation coefficient,36_Lecture_41___Introduction_to_Clustering.wav
37,real man relation and so on distance is taken as root over XI -1 x j Sigma inverse x,37_Lecture_41___Introduction_to_Clustering.wav
38,this Sigma is the covariance matrix if their Independence then this is equal to the euclidean distance and then we have the Pearson correlation Coefficient which is given by covariance of x i x j / Sigma excise Sourcing bike side stand for how far each of the items in a cluster is from the mean of the cluster and this is covariance of exercise these are some of the similarity metrics that are used in the cluster,38_Lecture_41___Introduction_to_Clustering.wav
39,next aspect which is of importance to US is how to measure the quality of a clustering two types of measures of quality it could be internal evaluation or external evaluation in internal evaluation you evaluate the cluster quality by the clusters and the data that you have various external evaluation you use edition,39_Lecture_41___Introduction_to_Clustering.wav
40,sofa example in internal evaluation metric to look at how close the points in a cluster at each other and how far they are from members,40_Lecture_41___Introduction_to_Clustering.wav
41,there are many several such measures one of them is the day was built in Intex that is built in index is computed as 1 by N summation over the number of clusters,41_Lecture_41___Introduction_to_Clustering.wav
42,you take,42_Lecture_41___Introduction_to_Clustering.wav
43,items belonging to the same cluster which are not equal to I will look at Sigma i + signature / this is the distance of CID CJ and this is the spread of Sigma,43_Lecture_41___Introduction_to_Clustering.wav
44,measure of the quality,44_Lecture_41___Introduction_to_Clustering.wav
45,if you have some label data but you ignored the labels can come up with the clustering of the data you can compare the class that you have got with the labels,45_Lecture_41___Introduction_to_Clustering.wav
46,and escapes you external,46_Lecture_41___Introduction_to_Clustering.wav
47,for example there are different external evaluation metrics like f-measure jaccard index Rand index,47_Lecture_41___Introduction_to_Clustering.wav
48,we have earlier talked about define what we mean by true positive true negative false positive false negative to refresh your memory let me draw this table,48_Lecture_41___Introduction_to_Clustering.wav
49,so,49_Lecture_41___Introduction_to_Clustering.wav
50,actual Label of the data it and the actual label is plus or actual label is minus and this is predicted by your,50_Lecture_41___Introduction_to_Clustering.wav
51,predicted + predictor,51_Lecture_41___Introduction_to_Clustering.wav
52,,52_Lecture_41___Introduction_to_Clustering.wav
53,gift items for which capsule level is Plus and you also predicted plus their actual and predicted - it is called to negative but if the actual level is plus you are also predicted - this is called false negative and actual label is -2 the predicted plus it is called false positive now and index TP and densities are the two regions where are you also has worked correctly and index given by,53_Lecture_41___Introduction_to_Clustering.wav
54,TP,54_Lecture_41___Introduction_to_Clustering.wav
55,tion divided by the sum of all 4 so the fraction of examples for which output as predicted the correct class,55_Lecture_41___Introduction_to_Clustering.wav
56,the jaccard index,56_Lecture_41___Introduction_to_Clustering.wav
57,is given by jacket index of two sets a b is given by a intersection b / a union B in this case if you take the intersection of the predicted that the actual what you get is true positive are the ones which is the intersection of those which both of the algorithms are predicted correctly divided by the union of those that any of the predicted correctly so this is divided by TP plus F3 Plus,57_Lecture_41___Introduction_to_Clustering.wav
58,possible metrics and is another common metric called f measure which is a harmonic mean of Precision and,58_Lecture_41___Introduction_to_Clustering.wav
59,true positive / true positive Plus,59_Lecture_41___Introduction_to_Clustering.wav
60,true positive / plus negative and measure is the harmonic mean between them some measures that are used for external evaluation with stop today's lecture in the next class we will start with K means which is an example of a partitional clustering and then we will talk about one hierarchy,60_Lecture_41___Introduction_to_Clustering.wav
61,and women talk about one model based clustering algorithms which is mixture of coffee,61_Lecture_41___Introduction_to_Clustering.wav
62,,62_Lecture_41___Introduction_to_Clustering.wav
