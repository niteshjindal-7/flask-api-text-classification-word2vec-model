,content,topics
0,,0_Lecture_06___Introduction_to_Decision_Trees.wav
1,welcome to the next lecture of this class in the model of decision trees in the last class introduced decision tree today we will look at some learning algorithm to learn decision trees so let us recapitulate the basic outline of the greedy decision tree algorithm that we discussed in the last class as we said that we start with a number of training examples letter city is the set of training examples now we want to decide at test for the root node of the decision tree so we come up with,1_Lecture_06___Introduction_to_Decision_Trees.wav
2,find out one of the suppose a is the set of attributes,2_Lecture_06___Introduction_to_Decision_Trees.wav
3,Tere Piche attribute so we want to check we want to find one of the attributes based on which we will make decisions at the root node so let a be the attribute that we have selected now based on that attribute I will look at the different values of the attributes suppose a text to values and then corresponding to that we will have two children of the set of training in samples with the associated initially with a route that will be split into D1 and D2 suppose this particular branch corresponds to a equal to to this corresponds to a call to false so divine will contain those examples from which a 1 equal to 22 will contain those examples for which date,3_Lecture_06___Introduction_to_Decision_Trees.wav
4,down at this case when we look at Diwan we will again have to select,4_Lecture_06___Introduction_to_Decision_Trees.wav
5,I decide that we will stop growing the decision Tree at this point or we have to select an attribute and we recursively and the basic outline of the album is given in the,5_Lecture_06___Introduction_to_Decision_Trees.wav
6,as we discussed in the last class charactor to decisions we have to take now out of these to open nodes we have to decide which one we should start with and then we have to decide if you decide to start with a particular node we have to decide at that note whether we should stop or whether we should grow the tree if he grows a tree we have to decide which attribute to split on these are the choices that one has to make when one goes for a decision tree now let us see how we take those decisions first for letters look at the decision about when used,6_Lecture_06___Introduction_to_Decision_Trees.wav
7,several cases where you may have to stop first of all if you consider that there are three features of three attributes and at a particular point suppose you have chosen a one the feature a one and based on that you have an equal to fir se got this place and then suppose you have chosen a 3 and based on three equal to true you have got this plate and here suppose you have chosen a to and based on a true equal to true or false you got the splits proposed here we have De to them here we have D3 and here we have D4 now if it is the case that all the three attributes have been used up so this is contained a 1 equal to false the two equal to true to equal to to solve things are given here so we have no more attributes to split on because we have exhausted that repeats if you have exhausted that we have to stop here we have the,7_Lecture_06___Introduction_to_Decision_Trees.wav
8,if you find that this place that all the examples in Deewan have the value plus or have the value minus if all the examples of the one have the same value for the target attribute we can immediately stop there and label this known as a leaf not all of them my plus we will able this as a leaf not with the class of the attribute that we have we may stop if the examples that we have got here are too few suppose the five is a set of examples we have and the five is a Saturday 62522 examples then also we should stop these are the three of the possible stop,8_Lecture_06___Introduction_to_Decision_Trees.wav
9,the decision about which notes to split,9_Lecture_06___Introduction_to_Decision_Trees.wav
10,which attribute to split on,10_Lecture_06___Introduction_to_Decision_Trees.wav
11,practice that will apply at a particular,11_Lecture_06___Introduction_to_Decision_Trees.wav
12,what is the ample here we have the choice of using attribute it to or a three years we have the choice of using attributes A1 A2 A3 which attributes should be,12_Lecture_06___Introduction_to_Decision_Trees.wav
13,so earlier we have talked about a Biased criteria which says that we want to choose a very simple function we prefer simpler functions or we can say in the context of decision trees we prefer smaller decision trees so we could think of a heuristic method of choosing the attribute so that based on this choice of the attribute the decision tree is expected to be,13_Lecture_06___Introduction_to_Decision_Trees.wav
14,another thing that we could do is that we could think of you have some examples here that is it to so and suppose it's hard to class problem did you may have a mixture of positive and negative examples to has only one type of example we can stop at that not every to has a mixture of positive and negative examples we can look that it is said that if it did stop 82 we will output the majority class supposed to has 60 positive examples and 40 negative examples so if we had stopped the De to we could have said the two is positive and then we would have made an error and that error is 40% so we can stop so that the splits give the smallest error,14_Lecture_06___Introduction_to_Decision_Trees.wav
15,and there are slightly more sophisticated methods based on which we can do this which we will discuss,15_Lecture_06___Introduction_to_Decision_Trees.wav
16,and if you have multi-valued features of the things that we can do this now in this example the attributes has two values if the attributes have multiple values then there are two choices one is that we could take that attribute and suppose that attribute has four values we could have four children but in a sometimes that you can be multiple can be real values and the number of children cannot be can be extremely large so we could have if you have few children then we could have multiple value attribute or we could split rows values into have so that we have two children suppose a particular attribute has the value low Medium High so we could have three children corresponding to these three values low Medium High aur we could have two children and then we could say this is low and medium and this is hi so if you use attribute A1 as low and medium comes here in a future below this note we could also against it on a one letter se AVN is slower when is meeting sofa multivalued attribute when we use less number of splits we can use the attribute again to split that,16_Lecture_06___Introduction_to_Decision_Trees.wav
17,now let's look at how we choose an attribute some more principals criteria about which attribute to split,17_Lecture_06___Introduction_to_Decision_Trees.wav
18,Sab client to look at two examples we have some training examples lete 64 training examples here and if we choose attribute A12 split on a one is a binary attribute having true and false then if Y1 equal to true we get 26 examples on this site and 38 example for a wonderful to falls out of this 26 examples 21 is positive 5 is negative for a 1 equal to false 8 a positive 30 and negative however if it is placed on a to for a two equal to true we have 18 positive 33 negative for a two equal to false we have 11 positive and negative now based on this information we want to decide whether a one or a 2 is a better candidate for splitting to give an answer to this there are multiple methods that one could use and we will introduce one very popular method which is based on entropy and information entropy as you know is a measure of disorder in a system if it a particular node all the examples are positive for all the examples and negative that is all examples belong to the same class then it is a h********** set of examples and entropy is entropy is low however if we have two classes and all the examples half belong to one class half belong to another class then entropy is higher,18_Lecture_06___Introduction_to_Decision_Trees.wav
19,Alif note is one I really there all the examples belong to the same class that is entropy is low so we would prefer that notes quickly quickly get notes which floor,19_Lecture_06___Introduction_to_Decision_Trees.wav
20,based on this we have a heuristic to decide which attribute,20_Lecture_06___Introduction_to_Decision_Trees.wav
21,when you want to select an attribute we want to choose the most useful attribute and one of the criteria we use to decide what is the most important attribute is the criteria of information gain,21_Lecture_06___Introduction_to_Decision_Trees.wav
22,now what is the information for if at a particular place we have some training examples for half a positive half an egg,22_Lecture_06___Introduction_to_Decision_Trees.wav
23,if we are given a random example and asked to predict its class we have to make a random guys have no information if the examples are equally divided among the classes there is no information there but if all the examples belong to a class and we identify that set example with that class information,23_Lecture_06___Introduction_to_Decision_Trees.wav
24,so we want to have a definition of Information and we want to prefer a situation when information is now when we decide which attribute to split on we will use the principle we may use the principle of information gain I am this principle if you can look at the slide this principle the information gain measures how well a given attribute separates the training examples according to their target classification so if all the examples have same target classification information is high and information gain is,24_Lecture_06___Introduction_to_Decision_Trees.wav
25,if majority support 90% belong to one class 10 also information gain is information is right but if 50% belongs to one class 50% and information is the measure of information gain which will define is used to select among the candidate attributes at each step while growing the decision tree and the gain is a measure of how much we can reduce uncertainty if the examples belong to the same class there is no uncertainty in the examples are spread among the classes almost uniformly there is uncertainty now based on this we will define entropy loss will define entropy and in terms of Entropy we can define information entropy is a measure of purity pure exit of examples will have entropy zero it can also be thought about as a measure of uncertainty or a measure of information content entropy is a very standard the term which is used in various domains in thermodynamics in information theory etcetera in information theory sisters in a site in information theory the optimum length code assigns - log P B so if you have p positive examples and P negative exam so you have tea positive examples,25_Lecture_06___Introduction_to_Decision_Trees.wav
26,and 1 - 3 negative example,26_Lecture_06___Introduction_to_Decision_Trees.wav
27,and you want to come up with the court then in information theory the optimum code will use - log P number of bits to send a message having a probability P right so we use the idea of Entropy for decision trees and entropy is defined as the average optimum number of bits to encode information about certainty uncertainty about us this is a background of Entropy and entropy is defined as entropy of s f is a sample is a sample that is a set of examples it is defined as,27_Lecture_06___Introduction_to_Decision_Trees.wav
28,C plus plus is the fraction of positive examples in this Sample C + - log to the base 2 p +,28_Lecture_06___Introduction_to_Decision_Trees.wav
29,plus minus b minus is the fraction of negative examples in the sample minus negative,29_Lecture_06___Introduction_to_Decision_Trees.wav
30,log to P - aur we can write this as - 3 + log 2 of P +,30_Lecture_06___Introduction_to_Decision_Trees.wav
31,minus t minus,31_Lecture_06___Introduction_to_Decision_Trees.wav
32,log 2 of P - this is the definition of Entropy so if it is a h********** set then suppose P Plus is one and T minus is zero in that case entropy will be,32_Lecture_06___Introduction_to_Decision_Trees.wav
33,CP Plus 20 - is 1 then also entropy will be zero and entropy that is the value of this function is highest 20 + equal to P minus equal to,33_Lecture_06___Introduction_to_Decision_Trees.wav
34,if you look at the sub slide,34_Lecture_06___Introduction_to_Decision_Trees.wav
35,slideshows with the value of p What is the value of Entropy we can take equal to P plus in that case we minus equal to 1 minus 3 equal to zero entropy zero when people to 10 and 20 equal to half and half year what do you get minus have log to half,35_Lecture_06___Introduction_to_Decision_Trees.wav
36,,36_Lecture_06___Introduction_to_Decision_Trees.wav
37,and this is equal to 1 the highest value of entropy is 1 when P + equal to P minus equal to half and this is the shape of the entropy,37_Lecture_06___Introduction_to_Decision_Trees.wav
38,entropy zero is the outcome is certain that entropy is maximum if you have no knowledge of this,38_Lecture_06___Introduction_to_Decision_Trees.wav
39,now based on entropy and information gain information gain,39_Lecture_06___Introduction_to_Decision_Trees.wav
40,of a sample if with respect to an attribute a is defined to be this is the original entropy,40_Lecture_06___Introduction_to_Decision_Trees.wav
41,new entropy of the system if you split an attribute to split an attribute a and you get two different values of a then for every value of a,41_Lecture_06___Introduction_to_Decision_Trees.wav
42,major general for all the multi values for be included in all the values that it takes the entropy for a particular class this is the fraction,42_Lecture_06___Introduction_to_Decision_Trees.wav
43,sbbs is the fraction of examples that have value for which the attribute has some value bi f bi bi S entropy of SI,43_Lecture_06___Introduction_to_Decision_Trees.wav
44,so what is it mean supposed,44_Lecture_06___Introduction_to_Decision_Trees.wav
45,the number of training examples at this node of the decision tree and you split on attribute a and a has two values true or false and you have S1 here is to hear and suppose the fraction of so S1 suppose S1 equal to size of S1 equal to one third of S and size of H2 is two third office then if one has an entropy is too has a name property and the resultant entropy is one third into the entropy of S1 plus two third into entropy of H2 and information gain is the original and copy of s -,45_Lecture_06___Introduction_to_Decision_Trees.wav
46,in this particular case that we had seen that if we split on attribute A1 and if we split an attribute to which one we should prefer if we use this measure of information gain so entropy of us in this case as has 29 positive and 35 negative examples and if you do the computation and trophies point 99 now we have to find out what is the information gain if you use a one and information gain if you use it and this is worked out here in the case of a one the entropy of S1 is point 71 entropy of S2 is point 74 and gain as a one can be computed as using the formula I am not working it out but you can follow the slide information gain is point 27 when does if you use the attribute a to for splitting the entropy of s 1.94 entropy of H2 is point 62 and the game is point 12 so where is the game highest the game is highest for even compared it so according to this measure we will use a one for splitting rather,46_Lecture_06___Introduction_to_Decision_Trees.wav
47,,47_Lecture_06___Introduction_to_Decision_Trees.wav
48,a one get selected because it has higher information gain that is the reduction in entropy is more for even you want to reduce the entropy because you want smaller decision trees let's look at an example the slide shows a set of training examples are yesterday in the last class we had introduced a decision tree to decide whether the proponent wants to play tennis given the different parameters of the day Outlook temperature humidity wind and this is a data set in this data set if you want to decide at the root node which attribute we want to split on letters a humidity and when the two possibilities that game for humidity is point 151 game for Windows 4.048 so we will prefer to split on humidity rather than on,48_Lecture_06___Introduction_to_Decision_Trees.wav
49,similarly the outlook for Outlook mail train is point 247 it is word out in the slide you can look at it so outlook is preferred to either humidity or went so among these three Outlook seems to be the most promising so if you do it for the all for attributes from should be for temperature temperature has a gain of 4.0 to 9 so this light shows for the that training example the game for various attributes and we see the gain for outlook is the highest and so we will if your using this measure of information came we will use Outlook as the root test for the decision tree so this id3 algorithm given by Queen Le it will select Outlook as the route on this 15 training example and then out look at three different values and then you take outlook is equal to Sunny for outlook is equal to sunny again you have to find out which not to split cells Outlook has already been used up we have a choice of three different attributes humidity temperature and wind and this slide shows the computation of the gain of these three attributes we see the gain of humidity is 4.97 for temperature point 57 and for range 2.01 so we will use humidity at the test in this note for Outlook equal to overcast we see all the examples here a positive so we will make it Alif not an Outlook equal to rain is there are three positive to negative examples again we have to do the computation to decide which attribute to speak out so if you grow the full decision tree this is what we will get and you can,49_Lecture_06___Introduction_to_Decision_Trees.wav
50,now other than the information gain there are other measures of,50_Lecture_06___Introduction_to_Decision_Trees.wav
51,deciding that attribute for decision tree one popular measure is death and Taxes and other measure of note impurity we will not going to the details but just to tell you the Gini Index of a node is computed as 1 - Sigma of probability of C whole square was a c are the different classes and PC is the probability of the class of the which can be estimated by the fraction of examples belonging to the class 10 found the Gini Index of a door,51_Lecture_06___Introduction_to_Decision_Trees.wav
52,you can to the Gini Index of a split for an attribute Sajini a is Sigma again the fraction of training is Sigma over the values of the attribute the fraction belonging to that value any of that not so based on that you can compute the Gini index engine indexes and other measure which is you another heuristic which can be used for session now what we have discussed is decision tree which has two or three or four values that is nominal value attribute what is the training example contains an attribute which is,52_Lecture_06___Introduction_to_Decision_Trees.wav
53,if it is a real valued attribute what you can do is that you can split the attribute values into two halves so for example height I can say height less than 5 height greater than 5 you can divide it into two halves all you can divide it into a few discrete ranges and then you can grow that,53_Lecture_06___Introduction_to_Decision_Trees.wav
54,suppose you want to divide the attribute into two ranges for a continuous attribute you have to decide what is the value on which we will split suppose that the different Heights are there and you want to decide whether you want to split at four or five of 5.5 or 5.3 or 6.2 if you decide where it now for this also what you can do is that we can identify possible values for splitting and for each value that we split on with the range on we can find out where the information gate is of course this is a computationally intensive and it would require some time but one can do it intelligence sofa continuous attribute one can do binaries plate and in order to binary string if you want to do it optimally you find all possible splits and find out where the information gained his highest,54_Lecture_06___Introduction_to_Decision_Trees.wav
55,now we have covered the basic algorithm for decision tree is there are certain other things that you need to worry about when working with decision trees whether a decision trees or other learning outcomes also underfitting and overfitting missing values cost of classification it's at which we will cover in the letter class,55_Lecture_06___Introduction_to_Decision_Trees.wav
56,stop over lecture for this particular module and then will contact you within the next class,56_Lecture_06___Introduction_to_Decision_Trees.wav
