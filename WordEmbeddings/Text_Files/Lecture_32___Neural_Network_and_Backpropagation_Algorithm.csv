,content,topics
0,,0_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
1,today we will talk about multilayer neural network and the backpropagation algorithm to refresh your memory let us see what we can do by a single layer neural network if you use a single layer perceptron which we have already seen in a single layer perceptron we have a linear summation of the input units followed by a nonlinear function and the nonlinear function in the case of a perception is a thresholding function but we could also use other functions such as the sigmoid function that and its function of the Year,1_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
2,dosti ki maa of,2_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
3,Sigma WH using the sigmoid function or in general we can use a function 5 if we have a single layer neural network or single layer perceptron,3_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
4,can the decision boundary is represented by a straight line supposed X1 and X2 are the two features that you have the decision boundary is a straight line and such units will work to represent functions where the two classes examples belonging to two classes can be separated by a straight line for example is a look at the slide here we have two classes 1 class 1 is not a bi red plus plus two denoted by blue circle and there is a straight line function that separates them on the right VC to other examples where we have class 1 which is red class to which is blue and there is a line separating class 1 from class 2 and there exists a linear decision boundary with separate class 1,4_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
5,this fourth example in the bottom left here we have to read points to read plus points to Blue 0 points now these examples we cannot have a linear boundary with separates the bluepoints from the red point source such a machine learning problem cannot be represented by a single layer,5_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
6,example of a function which can be represented by a single layer perceptron and this particular function is the Boolean or function which most of you are familiar with the Boolean or function outputs one if any of the inputs S1 and output thereof only if all the inputs are not one not this function can be represented by a single layer perceptron and learning the function means learning the weights on the corresponding ages so there are three with its associated with this percent draught w1 from x12 the unit w2 from x to the unit and 220 which is the,6_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
7,colours diagram shows you that some specific values of 0011 and w2 for which we can implement this function this function can be implemented by some sets of that this is an example of a set of values which can implement this function ok so this is the representation of the earth function and given this particular values of 0011 and w2 we get a linear decision boundary with separate the space into two regions so that the plus points are in one region the - 4 -,7_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
8,this is another example of a function which can be represented by a single layer perceptron this is the Boolean and function where the output is one only if both the inputs are one otherwise the output is 0 it is represented by this diagram and this decision their corresponding to some specific weight so for example if I said the blue unequal 2122 equal to 1 and 220 equal to minus 1.5 it is it will denote a decision surface which works for this and function there could be other combinations of weights also which work for the and function but this is one combination of weight which implements the and,8_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
9,but when we have the xor function which is one if exactly one of the inputs is 1 and 0 if either both of them are both of them and that is the xor function and as you can see there is no linear decision boundary that separates 10 points from the one point So in order to represent this function we can go for multi layer,9_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
10,an example of implementation of the Boolean function so we have initially we have the first layer we have to perceptrons the first perceptron H1 and H2 and then the second layer we have one perceptron hope and together these three units into layers they can represent the Boolean xor function for certain combination of weights for example we can have the first the left units left H1 and the first player to represent the odd function by putting the weights as 11 and -4.5 we can have the second node at the first layer represent the and function by putting the weights as 11 -1.5 and we can have the note and the second layer,10_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
11,the finalists of function by setting the weights as 1 - 1 - 4.5 this is one example implementation of the xor function by using a two layer perceptron 100 cannot be implemented by a one layer perceptron but it can be represented by a two layer perceptron,11_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
12,general if you look at multilayer neural networks we can say this thing about the Representation capability of neural networks if you have single layer neural networks they have limited representation power and we ask you have already discussed they can represent linear decision surfaces and therefore if the examples of two classes are linearly separable then only that can be represented by a single,12_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
13,nonlinear functions if have to go for multiple,13_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
14,as you can see if we had only linear units combination of linear units would be yet another linear unit so in order for multilayer neural networks to the present not linear function it is important that the functions implemented at the individual units are not only that is why we go for nonlinear units at the threshold unit of sigmoid unit or time is unit of,14_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
15,when did go for multi layer network if we go for a to LAN network so suppose this is the input unit and we have one hidden layer neural network so suppose this is that you can put X1 X2 X3 M at the inputs and this is the output and we can have one or more hidden layers suppose this is the first hidden layer comprising three no,15_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
16,importance of connection from the input to the first,16_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
17,and we can have connections on the first layer to the second layer so,17_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
18,network this is a network with one hidden layer if we take a network with one hidden layer it is normally called the two layer neural network such neural networks can represent all,18_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
19,all Boolean functions can be represented by a neural network with the Singh,19_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
20,that it is possible because you know that any Boolean function can be represented using Nand gates using two layer of 90s and I'll leave it for you to figure out that we can have a single layer perceptron represent the nun function we have aliassime that neural network and represent a single layer perceptron can represent the and function you can I leave it you as an exercise to see that it can represent an and function which is the inverse of the and function and by cascading two layers of 9 and you can represent any Boolean function and any Boolean function can therefore be represented by a neural network with a single,20_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
21,despite of yes secondly we can say every bounded continuous function can be approximated with arbitrarily small error by neural network with one hidden layer so not just Union function if you take a continuous function if the continuous function is bounded that that is it doesn't go to infinity it it is within a bound then any continuous function can be approximated by arbitrarily small error using a single hidden,21_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
22,if you have a neural network with two hidden layer like this so this is the first layer this is the second layer H2 and is a connection from the nodes in H1 to the nodes in H2,22_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
23,and then there is the output,23_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
24,calder to hidden layer neural network it can be shown that any function at all can be approximated to arbitrary accuracy by a network with two hidden layers if you're using a network with two hidden layers such a network can represent any arbitrary function which is a very powerful,24_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
25,how to wear the catch just because a given network can represent of function does not mean that the function will be learnable in the sense that as we will see in a neural network we do not know so we say that there exists a neural network which can represent this function but that neural network comprises of a number of nodes in the different layers and the number of bits right so we know that a function can be represented by a to attend a neural network but we do not know how many notes should put what should be the weight speed and we do not know how many notes will be put so that to figuring out how many notes will put and what would be the weight may turn out to be hard for different problems and that when by said that any Boolean function can be represented by a network with one hidden layer I did not mention anything about the number of of notes that you require that can be some Boolean function for which the number of nodes that require can be very large so just because a function is represent table may not mean that it is Lord never now we will see how we can learn in a multilayer neural network using the back proper,25_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
26,this like this is the schematics of a multilayer neural network,26_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
27,so we have the inputs we have the first didn't left of this shows at two hidden layer neural network the input the yellow not for the first hidden layer the blue note that the second hidden layer and the green notes are the output,27_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
28,as we have already discussed that when you give the input and you observe the output and if you have a training set the training set will tell you for a given input what should be the ideal output and from the training set you can find out what is the error for a neural network unit for a particular Input and we can update the weights so that this error is reduced now the error is only observed at the output so if you have a neural network where this is the input layer,28_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
29,this is the output layer and these are some hidden layer,29_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
30,CD connectivity between this and this this this this this for the notes and the output layer we can find out the error for a given input so if I take the first input X1 we can find out what is the output waiver and we can find out what is the output that you are getting using the neural network so we can find out at every node for the given input what is the actual error and we can try to change the weights so that the error,30_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
31,but the output we do not know for a training example what should be the value of a note here or a note here,31_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
32,going to do is that the care that we find at this layer we are going to propagate the error and estimate the error at the inside hidden layers we are going to take their which we observe and the output layer backpropagation the error to the previously so we say that the error here is because of the error which was,32_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
33,The Blame of the error here partially depends on the error this notes which in turn depends on their address so we will take this error and back propagated to the other notes from which it takes Input and if you look at the weight of this and the weight of this age if this has the higher magnitude of weight this node has higher contribution here if it is a smaller magnitude of weight it has a smaller contribution so when we oppose that words we apportion the error proportional to the weight if the earth has larger weight we put larger error report Santoor,33_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
34,so backpropagation works in this way when we apply the neural network on a particular input the input signal propagates in this way is computed in this way so that we can get output but when we find the error we find the error at this layer and the error is that propagated to the previous layers and based on the notional error out after backpropagation based on that note error we do the weight updating at this place so here we update the weights based on the directly off,34_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
35,after we are back propagated there we find the notional error at this level and based on that we change this way,35_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
36,propagate this error for the year and based on that which is this so that is why we call this method,36_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
37,method to train multilayer neural network the updating of the weights of the neural network is done in such a way so that the error observed can be reduced the error is only directly observed that the output layer that error is back propagated to the previous layers and with that notional error which has been back propagated we do the weight update in previous,37_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
38,we have looked that the derivation of the error derivation of the update rule of a neural,38_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
39,based on the error we saw how we could use gradient Descent to find out the error gradient at a unit and we can't change the error based on going to the negative of the error just to recapitulate if you have one output neurones the error function,39_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
40,equal to half Y minus whole square for a particular input why is,40_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
41,out why is the gold standard output is the actual output so why minus gives you the error for half Y - whole square is the particular measure of error that,41_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
42,unit Jay the output is defined as equal to the function 5 applied on net when net is the sum of the the weighted sum of the units at the previously Sonet Jay is Sigma wkg ok and fine is the nonlinear function that you are using as your mention we could use 5 as a sigmoid function not NH your value or some such,42_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
43,wkg correspond to those ages which are coming from the previous unit 2,43_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
44,net Chitra neurone is the weighted sum of outputs ok of the previous and neurones which connects to the scorer and following the method which we followed in the last class now given this we can find out given that formula equal to half Y minus b whole square and O is as it is defined here we can now try to find out the derivative of this error function with respect to the different weights so as we have in different weights correspond in plus one different with corresponding to the previous inputs and the buyer,44_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
45,take the partial derivative of this error function with respect to each wait and,45_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
46,quantity partial derivative of the error function with respect to the Blue Eyes which is one specific weight can be rewritten as by the Chain rule Delhi b l ok X Dela J B L net Jay x delnet j b l w i j Sonet Jay call to Sigma double ok ji ok and ok equal to 5 minutes and we can write this Delhi b l w a g in this,46_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
47,,47_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
48,we have written this is their function this is ok and Delhi b l w i think this is what we did in the previous slide now this can be written as Delhi by the laws can be written as summation over l Delhi b l oil oil by delnet settle x w,48_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
49,corresponds to the out this output is coming from the other and other outputs other notes which are indexed by l and this is file of nature into 1 minus 5 of nature which is the corresponding to the sigmoid function and this is why this follows from the derivation that we worked out in the last class so simplifying we can find out the daily b l w i j is Delta j y when this quantity is called,49_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
50,the detail derivation we have done in the last class so Delhi by Delta w it is delta delta check comes from the nodes of the next layer from the errors in the Next Layer is back propagated to the previous,50_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
51,where Delta T is given by Delhi by delo delo j b l net which is equal to minus y j into Ajay X 1 - ok if there is an output neurone if I am at the last layer otherwise it is recursively computed as Sigma over said Delta z lwjgl x ok X 1 - ok if there is a neurone in the,51_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
52,Delta Jay can be computed directly at the output layer and once you have computer Delta for each of the output units you can compute delta of the previous commit after your computer all the deltas of the previous unit you can compute the delta of the previous two previous image so that is called the back propagation and based on that this is the recursive computation of delta starting from the last of the output layer and going one Level backward up to the beginning sunao once your figure this out you know what is Delhi b l tablet and then you change the weights using gradient Descent Delta WH ekwal 2 - ETA Delhi by Tel w i so this is the gradient this feature is the learning factor small letter means slow converter large it means faster rate and minus because we are doing gradient,52_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
53,so based on this we can write the back propagation algorithm which is actually very simple as a given in this light we take a neural network we have a structure we have one hidden layer to it and there whatever you want you decide the number of players in the neural network number of units image layer and 2 collection from the input to the first lead in the first the second second to the out and then you have a number of weights initially universalize all the weights to small Random numbers after that to carry out an iterative process which is given as here until satisfied what you do is you input you have a set of training examples input the first training example to the network and compute the network output so you give X you find my love for each you find o u give X1 you fine oven do you find O2 now for each output unit ke you may have only one output on multiple out each output unit ke you compute Delta ke at the output layer as ok into 1 minus ok into vi ke,53_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
54,go to the previous hidden layer for each hidden Unit 8 to compute Delta age as equal to 8 into 1 minus A into Sigma over whk Delta ke for all ke Vichar ine,54_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
55,after that you update it network with w i just as w w i z plus Delta w i z and Delta whs minus theta Delta Jay exercise as we have already seen so this is the backpropagation how to do we have an input we have the output that we get from the network and we have the target output to find the error from the target output based on that we update the weights feedback propagate the ways to the previous day by propagating the Delta value and continue we continue for all the hidden layers this is the backpropagation out but it's very sad,55_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
56,back propagation we do gradient Descent over the network,56_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
57,do the example we have shown is for a layered graph we can do backpropagation over any direct,57_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
58,setting to observe is that by doing backpropagation we are not guaranteed to find the Global we only get a local minima so we have a very complex error surface comprising of the weights at all the layers by doing backpropagation we are updating the ways to do better and better and we come to you doing it until the network converters but when the network converges it will converge to a local minima which need not be our global,58_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
59,there are certain tricks that one can use to prevent getting trapped in a local minima for example one such tric is to include a Momentum factor called,59_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
60,is that if you are going and trying to hit a local minima you try to prevent that by maintaining the previous direction of movement by the general direction of movement you don't want to deviate and get stuck so Momentum what it does is that when you change Delta w h a u not only look at it are Delta exchange exercises which we had arrived earlier but we also keep another factor which keeps track of the direction of weight change at the previous iterations Delta w i n is the weight change at the Inn At iteration which is equal to eat at Delta Jack size + Alpha X direction of a change in the previous iterations if you apply the momentum training my business,60_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
61,you are less likely to hit a local minima bad local minima but one thing to note is that in neural network when you use multiple layers even if training is slow after your learn the weights applying the new,61_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
62,observation cycle,62_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
63,beta update we can do a batch update that is given a particular configuration and given a set of training examples with respect to the all the training examples we can compute the partial derivative of the error and find the best way of updating it or we can do it for one input at a time so the first method is called batch gradient Descent the second method is called the store cast,63_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
64,take one input at a time based on that you change the way now,64_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
65,gradient Descent is less likely to get stuck in a local minima because if there is a local minima it is unlikely that for all examples it will be that minimum so if your stochastic gradient Descent the neural network is and is more likely to get what's the,65_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
66,and that is something in between which is called Mini batch gradient Descent so instead of taking all the examples at a time or a single example at a time you take a batch of examples at a time and with respect to that you too great,66_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
67,gradient Descent calculus outputs for the entire data set accumulate the errors then back propagate and makes a single update it is too slow to convert and it may get stuck in local minima stalker stick or online gradient Descent on the other,67_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
68,will take one training example at a time with respect to that it will find the error gradient it converts to solution faster and often helps get the system out of local minima and in between we have Mini batch gradient,68_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
69,,69_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
70,training process can be divided into epochs if you have a number of training example 1 Hip Hop will look at all the training examples once then will have the second Hip Hop then the third sector so when you are learning in epochs when do you stop so we keep training the neural network on the entire training set over and over again and each episode is called,70_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
71,we can stop where the training error is not is getting a saturation or we can use cross validation while we are training the neural network we can also keep validating it on a held outside and when we see that the training and validation errors are closed then we can stop or we can stop when we reached a maximum number,71_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
72,it works like other machine learning out those overfitting can occur and this overfitting is illustrated by this diagram,72_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
73,on the x-axis we have the number of iterations and the y axis we have root mean square error as is typical of many machine learning algorithm as we increase the number of iteration the error on the training set keeps on,73_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
74,even become zero not become zero but the error on my held outset typically will initially decrease and then it will increase over fitting has occurred at this is the zone where overfitting has occurred ideal you should stop before the validation errors starts to increase so if you can keep track of the validation error you will know that this is the place where you must stop and not continue anymore iteration because they on that the network is likely to overfit and the accuracy of the network milk,74_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
75,elastic local minima as he said neural networks gets can get stuck in local minima for small network can we also said that if you use to castor gradient Descent it is less likely to get,75_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
76,is when you have a large network with many weights local minima is not so come on because we could we have so many ways it is unlikely that and what doing every with paper at least it is unlikely that the same local minima will be the minimum,76_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
77,in conclusion we can say that artificial neural network lets you use highly expressive nonlinear function that can represent almost all functions it comprises of a parallel network of Logistic function units or other types of units are also possible the principal words by minimising the sum of squared training errors there are also neural networks with different other loss functions but we will not talk about it in this class you are we have looked that neural network to minimise the root mean square,77_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
78,we can add a regularization come to neural network which I did not talk about what you can do is that you can try to prevent the weights from getting large by penalizing networks with the way they have large values by adding a regularization come neural networks can get stuck in a local minima and it may exhibit over,78_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
79,come to the conclusion in the next class will give a very brief introduction on Deep learning,79_Lecture_32___Neural_Network_and_Backpropagation_Algorithm.wav
