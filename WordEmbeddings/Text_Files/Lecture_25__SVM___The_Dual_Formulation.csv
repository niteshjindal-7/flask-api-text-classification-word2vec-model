,content,topics
0,,0_Lecture_25__SVM___The_Dual_Formulation.wav
1,abhi will now talk about part C of this lecture where we will look at the dual formulation of support vector machine in the last class we did the looked at the formulation of Optimisation problem corresponding to support vector machine where we have to minimise half w square this is the convex quadratic Optimisation function subject to this linear constraints why wtx I + be greater than equal to one for all examples,1_Lecture_25__SVM___The_Dual_Formulation.wav
2,look at how to get the dual of this particular formulation let us very briefly talk about Lagrangian dual ATI suppose we take our general Primal general problem and its Primal formulation is given by this is an Optimisation problem where you want to optimise meaning for you want to minimise f w w are the parameters you want to find values of w so that to minimise FW and you have a set of linear constraints that to type of linear constraints equality constraints we have l equality constraints h i w equal to zero and ke inequality constraints grw less than equal to zero all this constraint Selenium corresponding to this problem the generalized Lagrangian is given by a function of w alphabet,2_Lecture_25__SVM___The_Dual_Formulation.wav
3,w + summation over 12 ke for all the number of non linear constraints equal to 12 ke Alpha Plus permission equal to 12 l b h i h i w a h i w are the equality constraints the item to other inequality constraints the alpha and beta are called lagrange multipliers and the value of Alpha is greater than equal to feel so this is the Lagrangian of this Optimisation for,3_Lecture_25__SVM___The_Dual_Formulation.wav
4,what we want to do is that we want to this particular lagrange,4_Lecture_25__SVM___The_Dual_Formulation.wav
5,inner if you take the values of a w alpha and beta such that where the if the Primal constraints are not satisfied then the value of this Lagrangian will be Infinity if the constraints are not satisfied the value of the Lagrangian is infinity and it will be equal to FW if the constraints are satisfied so we want to find out,5_Lecture_25__SVM___The_Dual_Formulation.wav
6,the values of Alpha Beta for which,6_Lecture_25__SVM___The_Dual_Formulation.wav
7,so if you if you look at the maximum of the value of l w alpha beta then if w satisfies the Primal constraints it will be equal to FW and it will be Infinity otherwise and we can rewrite the primer as finding the value of this expression Max of LW alpha beta we want to find out those so we first to maximize keeping the blue fixed for a particular w we can maximize over alpha beta and find the expression Max LW alpha beta and max LW alpha beta is either fwr infinity and if you find the minimum of this it will be giving us the solution,7_Lecture_25__SVM___The_Dual_Formulation.wav
8,otherwise it has a value of infinity to the Primal can be written as,8_Lecture_25__SVM___The_Dual_Formulation.wav
9,take the Lagrangian and you find out alpha beta for a fixed w you can take maximization over alpha beta and then you can do minimum over w so minimum over w maximum over alpha beta LW alpha beta is a rewriting of the Primal fear,9_Lecture_25__SVM___The_Dual_Formulation.wav
10,Primal problem and it has a solution letters in the solution is p star Easter is the solution of the Primal problem minimum of a w maximum over alpha beta of the Lagrangian and the 2 problem is we are just putting the minimum here and maximum here so we take first we take Max offer Alpha Alpha Beta minimum over wlw alpha beta this is the dual formulation and it has the solution,10_Lecture_25__SVM___The_Dual_Formulation.wav
11,if this is the Primus problem solution this is the dual problem solution and we have to theorems this theory and says that first of all you should take if you change the order of maths mean and mean maths it is a General it is a General expression that maths over mean of this expression expression is less than equal to mean of maths of this expression and d star is the max of mean of this expression and therefore this star is less than equal to p,11_Lecture_25__SVM___The_Dual_Formulation.wav
12,star is always less than equal to P start now if there exists a saddle point of this Lagrangian where they are equal that is called the saddle point and that is the optimum value of both the optimum value of the Primal formulation and the optimum value of the dual formulation will be identical when there is a,12_Lecture_25__SVM___The_Dual_Formulation.wav
13,Kajal point exist then the saddle point satisfies the following condition called kkt condition or parents don't,13_Lecture_25__SVM___The_Dual_Formulation.wav
14,the condition says that the partial derivative of this Lagrangian with respect to w i and with respect to b i will be equal,14_Lecture_25__SVM___The_Dual_Formulation.wav
15,according to the kkt conditions and from these to you will find out that What you get is that Alpha IGI w will be equal to zero for equal to 12,15_Lecture_25__SVM___The_Dual_Formulation.wav
16,w is less than equal to zero and Alpha I Greater than equal to zero so these are the conditions that you get when the saddle point exists and the theory and says if w star alpha star and beta star satisfy the kkt conditions then it is also a solution to the Primal and dual problems with this brief in a description brief outline of Lagrangian dual ATI let us go back to SBM and see how it can be applied the details of this theory is beyond the scope of this class you can read some material on convex Optimisation if you want to learn more,16_Lecture_25__SVM___The_Dual_Formulation.wav
17,if you look at our SVM formulation what we have is we have FW as half the square and we have the gw Vs WI w X + be greater than equal to 1 we don't have the age we don't have the equality constraints we have only the objective function and the only dealing with Alpha I not the beta so we are dealing with FW + Alpha ji,17_Lecture_25__SVM___The_Dual_Formulation.wav
18,,18_Lecture_25__SVM___The_Dual_Formulation.wav
19,,19_Lecture_25__SVM___The_Dual_Formulation.wav
20,kkt conditions also says,20_Lecture_25__SVM___The_Dual_Formulation.wav
21,Alpha I Ti w equal to zero and w is less than zero so if Alpha,21_Lecture_25__SVM___The_Dual_Formulation.wav
22,speakers Alpha ITI we01 Alpha is zero then gw can be non zero and otherwise ji,22_Lecture_25__SVM___The_Dual_Formulation.wav
23,face that only the few of the Alpha is can be,23_Lecture_25__SVM___The_Dual_Formulation.wav
24,the training data points whose Alpha is a non Zero Hour cal the syp,24_Lecture_25__SVM___The_Dual_Formulation.wav
25,some of the Alpha is are non zero and the training data corresponding to the support Alpha is better than equal to zero if I better than zero,25_Lecture_25__SVM___The_Dual_Formulation.wav
26,implication so this is the original Optimisation problem and when we take this is SVM Optimisation problem we take the Lagrangian which gives us minimization of lwb Alpha we have written lpp Din exprimer so minimise lpw be Alpha half w square minus Sigma Alpha i y w i x i plus B minus one subject to the constraints Alpha I Greater than equal to zero this is by getting the Lagrangian of the Optimisation,26_Lecture_25__SVM___The_Dual_Formulation.wav
27,,27_Lecture_25__SVM___The_Dual_Formulation.wav
28,take the partial derivative of this LP with respect to wnb what we get here is w equal to Sigma Alpha i y XI and from the second one by taking partial derivative of LP with respect to be and setting it 20 we get Sigma Alpha why,28_Lecture_25__SVM___The_Dual_Formulation.wav
29,what it means is that if I substitute this value of w in this expression here is a substitute this value of w signify XI in this expression hear what I get is l pwb Alpha equal to Sigma Alpha minus Sigma Alpha Phi Alpha Jave XT exchange I am sorry so we put w is here so this w becomes half of Alpha Alpha j y y x x 2 - 20 Sigma Alpha bhaiya so this is my LP when I substitute this value of w but we know that signify Y equal to zero from this constraint so this expression on the right side can be ignored and finally we get lwb alpha as this,29_Lecture_25__SVM___The_Dual_Formulation.wav
30,lwb Alpha Sigma Alpha minus half of its equal to 12 m Alpha Phi Alpha why XI transpose exchange now this is a very important formulation and we will look at the properties of this formulation to get certain properties of the support vector machine,30_Lecture_25__SVM___The_Dual_Formulation.wav
31,the dual problem that before we go to that let's look at the dual problem that no problem is maximizing of Jaya Jaya Jaya also is the expression we saw earlier and these are the constraints Alpha greater than equal to zero and Sigma Alpha i y equal to zero this is the word problem which is a portrait programming problem and from this quadratic programming problem we can solve and find the Global maximum value of Alpha I we can find out the values of Alpha by solving this quadratic programming problem and this quadratic programming problem is much easier to solve than the Primal formulation of this is much simpler are because the constraints of simpler and we will see it has certain nice Pro,31_Lecture_25__SVM___The_Dual_Formulation.wav
32,once you solve and get the lagrange multipliers Alpha we can reconstruct the parameter vectors we can find w as Sigma Alpha wire,32_Lecture_25__SVM___The_Dual_Formulation.wav
33,we noticed that Alpha is non zero only for few of the examples those examples are the one which are the ones which are the support vectors so w is obtained from,33_Lecture_25__SVM___The_Dual_Formulation.wav
34,Alpha XI well I Rangers among the support vectors and usually the support vectors of you Vin number and w can be computed from the coordinates of those,34_Lecture_25__SVM___The_Dual_Formulation.wav
35,also when we get a new data point Z in order to find out the output corresponding to this week and compute wtz Jazzy B which is alpha Sigma Alpha i x i z plus bi and we classify it as class 1 if the sum is positive and class to gift otherwise now you know that w need not be form found explicitly we can just use this expression and this expression as a very nice property when you put Z what you are right doing is this is alpha this is why this is excited that so you are taking the dot product,35_Lecture_25__SVM___The_Dual_Formulation.wav
36,vector with your,36_Lecture_25__SVM___The_Dual_Formulation.wav
37,function is given by this dot product of excited and X right so you need not Complex computation reduces to mainly finding this product so you have the dot product between the test point x and the support vector Exide why is this such an exciting thought now Exide is a vector and this can be a high dimensional vector but with if you take the dot product of these two linear vectors what you get is a scalar to the dot product is a scalar,37_Lecture_25__SVM___The_Dual_Formulation.wav
38,look at what are the implications later,38_Lecture_25__SVM___The_Dual_Formulation.wav
39,Optimisation problem also if you look at this formulation very solve the Optimisation,39_Lecture_25__SVM___The_Dual_Formulation.wav
40,what we have is the dot product of the training points solve 5 Alpha why there is either plus one minus one so these are very simple to compute multiply and exciting exchange the dot product of Excise so when we solve the Optimisation problems it involves computing the dot product between all the pairs of training points and the optimal w is linear combination of a small number of data points so these are some of the features about this SVM formulation we stop here today in the next class we will look at certain properties of SVM and how these properties can be used for those formulations of Asian with this I am today's,40_Lecture_25__SVM___The_Dual_Formulation.wav
41,,41_Lecture_25__SVM___The_Dual_Formulation.wav
