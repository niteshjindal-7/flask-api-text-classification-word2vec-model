,content,topics
0,,0_Lecture_18__Bayesian_Learning.wav
1,welcome to today's lecture today we will take talk about baby and learning which is part B of module 4,1_Lecture_18__Bayesian_Learning.wav
2,,2_Lecture_18__Bayesian_Learning.wav
3,in the last class We gave a crash course on probability and today we will see how probability is used for learning especially for classification so probability how it is used for modelling concepts,3_Lecture_18__Bayesian_Learning.wav
4,,4_Lecture_18__Bayesian_Learning.wav
5,,5_Lecture_18__Bayesian_Learning.wav
6,bayesian probability is the notion of probability,6_Lecture_18__Bayesian_Learning.wav
7,,7_Lecture_18__Bayesian_Learning.wav
8,which talks about partial believes,8_Lecture_18__Bayesian_Learning.wav
9,,9_Lecture_18__Bayesian_Learning.wav
10,,10_Lecture_18__Bayesian_Learning.wav
11,today's improbability talks about probability interpretation as partial belief and bayesian estimation,11_Lecture_18__Bayesian_Learning.wav
12,,12_Lecture_18__Bayesian_Learning.wav
13,,13_Lecture_18__Bayesian_Learning.wav
14,it calculates the validity of a proposal,14_Lecture_18__Bayesian_Learning.wav
15,,15_Lecture_18__Bayesian_Learning.wav
16,,16_Lecture_18__Bayesian_Learning.wav
17,the validity of preposition is calculated based on two things number one priority estimate it is based on the prior estimate of its probability,17_Lecture_18__Bayesian_Learning.wav
18,secondly new evidence new relevant evidence,18_Lecture_18__Bayesian_Learning.wav
19,based on this the posterior bayesian estimation is done and the key to this is an important theorem call to base which we will introduce now,19_Lecture_18__Bayesian_Learning.wav
20,,20_Lecture_18__Bayesian_Learning.wav
21,it is with how to find the probability of a hypothesis given that it have different possible competing hypotheses and you can find out the probability of the individual hypothesis given that data so that you can find out which is the most probable or most likely hypothesis according to the base theorem probability of hypothesis given data is given by probability be given a times prior probability of the hypothesis 8 / this is very easy to you know that by the law of products you can see that probability is the equal to Probability is X probability be given 8 and you can also because it is commutative this is also equal to Probability the h which is equal to probability that the X probability 8 given the so if you consider these two are equal,21_Lecture_18__Bayesian_Learning.wav
22,manipulating them you can come up with base base role is the most important formula from which we can look for at base learning so pH is the prior probability of the hypothesis for this is the prior probability,22_Lecture_18__Bayesian_Learning.wav
23,probability is the probability of the data is hypothesis is true what is the likelihood of the data being generated if it was true what is the probability of being generated and PD is the likelihood of the data from this we have based,23_Lecture_18__Bayesian_Learning.wav
24,application of Bayes Theorem for the simple look at the sky,24_Lecture_18__Bayesian_Learning.wav
25,want to know whether a patient has cancer or not a patient take so this particular example is taken from from Mitchell's book on machine learning a patient it's a lab test and the result is positive now that test returns the correct positive result in only 98% of the cases in which the diseases actually present and the correct negative result in only 97% of the cases in which the disease is not,25_Lecture_18__Bayesian_Learning.wav
26,do you know that point 008 of the entire population have this cancer so we can write down this as probability of Cancer that is the prior probability of Cancer among the population is equal to 2.008 and therefore probability of not cancer,26_Lecture_18__Bayesian_Learning.wav
27,equal to 1 minus point 008 that is point 99,27_Lecture_18__Bayesian_Learning.wav
28,probability of the test being positive given that cancer is,28_Lecture_18__Bayesian_Learning.wav
29,this is given as 4.98 by the statement of the problem also probability,29_Lecture_18__Bayesian_Learning.wav
30,therefore the probability of the test be negative given cancer,30_Lecture_18__Bayesian_Learning.wav
31,1 -4.9 8 that is 4.02 what is the probability of test being positive given not tense,31_Lecture_18__Bayesian_Learning.wav
32,by 1 minus point 97 that is 4.03 similarly probability of not given not cancer is given to be point 97 values that are supplied to you in the problem this we can use base theorem we want to find out the probability of Cancer that the patient has cancer,32_Lecture_18__Bayesian_Learning.wav
33,given that the ability that the patient has cancer,33_Lecture_18__Bayesian_Learning.wav
34,given that the test is positive is given by probability of,34_Lecture_18__Bayesian_Learning.wav
35,positive given cancer,35_Lecture_18__Bayesian_Learning.wav
36,X probability of Cancer / probability of the,36_Lecture_18__Bayesian_Learning.wav
37,you can write,37_Lecture_18__Bayesian_Learning.wav
38,probability of not cancer given that the test is positive equal to probability of,38_Lecture_18__Bayesian_Learning.wav
39,just being negative given cancer X probability of not cancer,39_Lecture_18__Bayesian_Learning.wav
40,X probability of the,40_Lecture_18__Bayesian_Learning.wav
41,I know you can put the values what is probability of plus it is point 98.9 8.00 to Probability and negative given cancer is point 97 or not sorry point zero 3.03,41_Lecture_18__Bayesian_Learning.wav
42,times point 99,42_Lecture_18__Bayesian_Learning.wav
43,given the test is positive is point 98 2.002 / Probability and not cancer given the test is positive is point 0 3 X 992 given PT / probability of having the,43_Lecture_18__Bayesian_Learning.wav
44,cone of base learning how can be applied to find a hypothesis in material we can find out the most likely hypothesis which is called the maximum a posteriori hypothesis,44_Lecture_18__Bayesian_Learning.wav
45,show the map hypothesis is given by,45_Lecture_18__Bayesian_Learning.wav
46,start value of a for which,46_Lecture_18__Bayesian_Learning.wav
47,probability that given data is maximized now by based,47_Lecture_18__Bayesian_Learning.wav
48,this is same as,48_Lecture_18__Bayesian_Learning.wav
49,that hypothesis we just expand this by taste theorem we get probability be given 8 times probability 8 /,49_Lecture_18__Bayesian_Learning.wav
50,capital of the hypothesis space and smallest out of all hypothesis and hypothesis space you want to find that hypothesis for which this expression is maximized now PD is independent of the particular hypothesis so we can say this is the same hypothesis for which this part is maximize,50_Lecture_18__Bayesian_Learning.wav
51,show the posterior headquarter posterior probability is given by probability given age proportional to Probability given a chance PH and the maximum a posteriori is hypothesis is the one for which probability be given h x pH is maximum and we get abhi to hypothesis based on their post,51_Lecture_18__Bayesian_Learning.wav
52,event if for all hypothesis the probability that,52_Lecture_18__Bayesian_Learning.wav
53,,53_Lecture_18__Bayesian_Learning.wav
54,you choose that hypothesis for which probability is the maximum likelihood hypothesis it is applicable in those cases where the prior probability of all hypothesis and equal that is initially before you have any data all the hypothesis equally probable in that case you choose the hypothesis for which probability be given ages so the application of bayesian theorem in order to find out maximum a posteriori hypothesis and the maximum likelihood hypothesis now we will see an example of how in finding the least square line we can apply the base theorem to find out the most likely hypothesis,54_Lecture_18__Bayesian_Learning.wav
55,,55_Lecture_18__Bayesian_Learning.wav
56,so suppose of the learner real valued function,56_Lecture_18__Bayesian_Learning.wav
57,we have already talked about linear regression which can be used to learn a real value,57_Lecture_18__Bayesian_Learning.wav
58,suppose the data is generated in the following fashion so that is a target function f is the target,58_Lecture_18__Bayesian_Learning.wav
59,,59_Lecture_18__Bayesian_Learning.wav
60,the individual data generated so the data is given as its id I other individual data points and there is generated as effect size + absalom absalom is the error and we assume that this error followers on normal distribution with mean 0 and standard deviation so we can I think that there is coming from a normal distribution whose meaning is effects I and whose Errors is given by Sigma square b square is the variance corresponding to this error term so this is how the data is being generated and let us assume that a person and I is independently generator for the individual instances website and I are independent for in different instances and it's a Gaussian with zero mean and variance Sigma square and therefore we can say that data is generated as normal distribution as excise,60_Lecture_18__Bayesian_Learning.wav
61,what we have is that this is our X and this is our,61_Lecture_18__Bayesian_Learning.wav
62,suppose this is the,62_Lecture_18__Bayesian_Learning.wav
63,function this is effect,63_Lecture_18__Bayesian_Learning.wav
64,and the data that we get on letters a generator,64_Lecture_18__Bayesian_Learning.wav
65,Bhiwadi data points that,65_Lecture_18__Bayesian_Learning.wav
66,find a function which estimates,66_Lecture_18__Bayesian_Learning.wav
67,now how do we find this function let us be used by maximum likelihood hypothesis so what is HTML HTML is the maximum likelihood hypothesis which is given by that hypothesis for which probability the given it is Max,67_Lecture_18__Bayesian_Learning.wav
68,now what is this this is,68_Lecture_18__Bayesian_Learning.wav
69,Max and probability given age is given by product over all the training examples 1 by root over to pi Sigma square,69_Lecture_18__Bayesian_Learning.wav
70,e to the power minus half,70_Lecture_18__Bayesian_Learning.wav
71,XI,71_Lecture_18__Bayesian_Learning.wav
72,whole square by sing because they follow the boss,72_Lecture_18__Bayesian_Learning.wav
73,can be written about this portion of the board so that we can write this formula here so this turns out to be,73_Lecture_18__Bayesian_Learning.wav
74,alt Max 8 so that function which maximizes this product is the same as which maximizes the sum of the logs so we convert it into the lock domain which is summation I equal to 12 m m is the number of training examples,74_Lecture_18__Bayesian_Learning.wav
75,Ellen Sofia taking logarithm of this part so it is half gallon so minus half minus half LN to buy Sigma square minus,75_Lecture_18__Bayesian_Learning.wav
76,I - 8 x by Sigma,76_Lecture_18__Bayesian_Learning.wav
77,are we can get so by simply find what we get it is that function for which Sigma I will 212,77_Lecture_18__Bayesian_Learning.wav
78,I - 8 x whole square is minimise,78_Lecture_18__Bayesian_Learning.wav
79,because this part is constant when I am taking the hypothesis for which this expression is maximized this part doesn't play a role because this is constant so this part laser road which is argmax hypothesis - of half the -8 it's because whatever maximizes minus half of that also maximizes only this but so it's not if you want to maximize negative of this it is the same as minimising the pores,79_Lecture_18__Bayesian_Learning.wav
80,that hypothesis the maximum likelihood hypothesis for this linear regression problem is that hypothesis for which the -8 and this is exactly the least,80_Lecture_18__Bayesian_Learning.wav
81,based on this will get a function and that function could be something like this but this is that function for which the sum of squared Errors is maximized so this is a bayesian explanation to why we would choose a sum of squared error to minimise in order to find out the linear regression,81_Lecture_18__Bayesian_Learning.wav
82,,82_Lecture_18__Bayesian_Learning.wav
83,,83_Lecture_18__Bayesian_Learning.wav
84,survival study about what is base optimal classifier,84_Lecture_18__Bayesian_Learning.wav
85,,85_Lecture_18__Bayesian_Learning.wav
86,,86_Lecture_18__Bayesian_Learning.wav
87,,87_Lecture_18__Bayesian_Learning.wav
88,question is we are suppose we are given some training data,88_Lecture_18__Bayesian_Learning.wav
89,which each of the training instances are given the class that it belongs to then we are given a test instance and we ask what is the optimum classification of X the life answer would be that you find out the most probable hypothesis using the map criteria and then you apply the hypothesis to the test example but this is not necessary l,89_Lecture_18__Bayesian_Learning.wav
90,if you are given the training data from the training data we learnt it,90_Lecture_18__Bayesian_Learning.wav
91,ek baat is the most probable hypothesis but 8 map is not the most web,91_Lecture_18__Bayesian_Learning.wav
92,example suppose 81 82 83 are three candidate hypothesis belonging to the hypothesis space and suppose probability 81 given the point for probability that two given the is 4.3 and probability 83 given D is for,92_Lecture_18__Bayesian_Learning.wav
93,so which is the map classified H1 is the map classified because it has the maximum posterior probability suppose you are given a new data,93_Lecture_18__Bayesian_Learning.wav
94,I am suppose H1 X is positive h2x,94_Lecture_18__Bayesian_Learning.wav
95,is negative 3 X is negative,95_Lecture_18__Bayesian_Learning.wav
96,what is the most probable classification of the most probable hypothesis of H1 H1 is saying that X is positive but H2 and H3 both are saying that it is,96_Lecture_18__Bayesian_Learning.wav
97,the most in this case the most probable classification would be actually negative because the sum of the probabilities of these two hypothesis is 4.6 which is larger than the probability of this hypothesis point 4 so what we have is we have what we call the base optimal classification in base of thermal classification for a particular example we take the class 2,97_Lecture_18__Bayesian_Learning.wav
98,capital V if the set of all possible classes you hypothesis will your algorithm will output that class included in all the classes for which the summation over all the hypothesis included in the hypothesis space probability Vijay given h i x probability HIV given De is so this is called the base optimal,98_Lecture_18__Bayesian_Learning.wav
99,is optimal classifier in output that class for classification problem for which you take summation over the entire hypothesis space of probability Vijay given a,99_Lecture_18__Bayesian_Learning.wav
100,i x ability h i give ND that will be maximum to find out the base optimal classifier you have to,100_Lecture_18__Bayesian_Learning.wav
101,you have to apply all the possible hypothesis on the test is done in order to find out the base of classification this is optimal classified but this turns out to be in,101_Lecture_18__Bayesian_Learning.wav
102,so we can quickly looked at the slide show the basis of classification is given by that Vijay for which Sigma age do din capital 8 pro back this is maximum and this is an example that we can work out which was in trouble 881 given days point 4821 30 points 383 given to people there for probability negative given h10 negative given H2 is 1882 negative given its ease 1 and 2 were pretty + Kevin h 10 and if you apply the base aptamil classify the see that probability of plus 6.4 probability of -4.6 so why is this called optimal it is optimal in the sense that no other classify using the same hypothesis space and same prior knowledge can outperform this on the earth average so this is called the base optimal classified but as you can see since typically the size of the hypothesis spaces he would it is not possible to apply the base optimal classify so we have to use some approximation of the base of the classified and for that we can use Gibbs sampling so what we do in get sampling,102_Lecture_18__Bayesian_Learning.wav
103,is that instead of,103_Lecture_18__Bayesian_Learning.wav
104,applying all possible hypothesis on X week sample from the hypothesis,104_Lecture_18__Bayesian_Learning.wav
105,choose the hypothesis random,105_Lecture_18__Bayesian_Learning.wav
106,according to,106_Lecture_18__Bayesian_Learning.wav
107,probability 8 given,107_Lecture_18__Bayesian_Learning.wav
108,super is hypothesis we have a probability associated with it so we have a probability distribution over the hypothesis space based on our training data that is our evidence we get a posterior probability distribution over the hypothesis space in the base of the MI classify each of the hypothesis according to the probability will apply on the each other for this is applied on the test instance and weights their contributions according to their posterior probabilities but in get something will choose a randomly our hypothesis according to pH by the and use it to classify the news,108_Lecture_18__Bayesian_Learning.wav
109,we just,109_Lecture_18__Bayesian_Learning.wav
110,one hyper system the distribution and use it to classify the new in fortunately it is surprising results that it has been found that the error for gives our firm is quite bounded so if the expected man who is taken over the target hypothesis drawn at random according to the prior probability distribution then the expected error of the gets classifier is less than equal to twice the error of the base optimal classified so that gets classify which is very much tractable in the sense that you can eat only to apply one hypothesis and if the one Sebastian distribution has been computed the Gibbs sampling can be used to choose the one hypothesis which can be used to classify the instance and it gets an error which is no more that twice the error of the base,110_Lecture_18__Bayesian_Learning.wav
111,Sabi come to conclusion of today's lecture,111_Lecture_18__Bayesian_Learning.wav
