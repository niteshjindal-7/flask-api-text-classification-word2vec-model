,content,topics
0,,0_Lecture_05___Linear_Regression.wav
1,morning welcome to the course on introduction to machine learning today we will start the second module of the course which is Introduction to Linear regression and decision trees these are some of the simplest machine learning algorithms and with this we will start looking at the machine learning algorithms in the first part of this module we will talk about linear regression we have already explained what is regression so regression is a supervised learning problem,1_Lecture_05___Linear_Regression.wav
2,where you are given examples of instances whose X and Y values are given and you have to learn a function so that given an unknown X you have to predict why so you want a function which predicts given X predicts Y and for regression why is country,2_Lecture_05___Linear_Regression.wav
3,so the equation problem and there are many models that can be used for regression that is many types of functions can be used the simplest type of function is a linear,3_Lecture_05___Linear_Regression.wav
4,now X can comprise of a single feature or multiple features for simplicity will first talk about simple regression when X is,4_Lecture_05___Linear_Regression.wav
5,feature will also discuss multiple regression comprises a number of feature source if x is a single teacher we can think of the training examples can be plotted suppose this is the value of x and this is the value of Y and let us assume that both x and y are continuous value so each training example is a point in this space for this value of x is the value of Y for this value of x is the value of Y so like that you may have different points in the feature space,5_Lecture_05___Linear_Regression.wav
6,and what you are required to do is find the function so that given an arbitrary unknown X you can predict why now the function can have different forms for example the function could be like this,6_Lecture_05___Linear_Regression.wav
7,on the function could be like this,7_Lecture_05___Linear_Regression.wav
8,what the function could be like this,8_Lecture_05___Linear_Regression.wav
9,different types of functions which are possible in linear regression we argue that the function is linear like this blue line here and out of the different linear functions possible we want to find one that optimises certain right,9_Lecture_05___Linear_Regression.wav
10,but apart from linear regression we could have other forms of regression for example if you look at the slide here in this particular picture in the first diagram the blue circles denote the different instances in the training data and suppose the green line is the actual function that actual function from which the points were generated so in regression you are given this blue points not the green line and you are asked to come up with a function now let us say that if you look at the diagram below suppose this blue points are the lines and you want to come up with a function suppose the red line is a function that you have come up it now we want to find out how good is the line with respect to the training exam so one of the ways we can measure how good is the line is 20 find the error of the line look at the Blue Point so here there are three blue points we can define the error one way of defining the error is we find the distance of the point from this red line and we can take the square distance from each point to the line and take the sum of squared errors the sum of squared Errors is one measure of error and this is one of the popular measures of error and we could try to find that function for which this sum of squared Errors is minimised argument that after we assume that this function comes from a particular classic and as you know that the function is linear or the function is quadratic,10_Lecture_05___Linear_Regression.wav
11,let's look at this right again,11_Lecture_05___Linear_Regression.wav
12,blue points which were generated from the green curve but the green card was unknown to us given the blue points we can learn this red line in figure 1 which is a linear function for this linear function in figure to these are two different linear functions sorry this is a linear function this is a General linear function this is a linear function which is parallel to the x-axis so that is it is of the for Y equal to constant is the first time graph for this function corresponds to Y equal to constant the second diagram the function corresponds to the equation Y equal to w X + constant in the third diagram we have a cubic function which is of the form Y equal to x cube + b x square + 3 X + 80 and in the fourth diagram it is a 90 degree polynomial this is a zero degree polynomial 1 Degree polynomial 3 degree polynomial 9th,12_Lecture_05___Linear_Regression.wav
13,the fit is not very good with,13_Lecture_05___Linear_Regression.wav
14,figure in the second figure the faith is slightly better with respect to if you look at the sum of squared errors this is highest in the first figure lower in the second figure lower in the third figure 0 in the ninth figure in the fourth figure in the fourth figure where is fit for 9th degree polynomial we are able to have the function pass through all the training examples so the sum of squared error of the training on the training example is zero but remember what we talked to the last class what we are interested in is finding the inner what is interested in is minimising the error on future examples for minimising the error for all examples according to the distribution now you can see in this fourth diagram even though we have fitted the points on the line the red line to all the points this function does not really corresponds to the Greenland sofa other points the error may be higher if you look at the third diagram this function seems to have set the points much better and we can expect that within this range the fit to the green line will be smart,14_Lecture_05___Linear_Regression.wav
15,,15_Lecture_05___Linear_Regression.wav
16,we have to keep this in mind when we try to come up with the function,16_Lecture_05___Linear_Regression.wav
17,regression models as we said in regression models we can talk about a single variable to extend their single variable then we call it simple regression or text can be multiple variables in the college multiple regression now for each of this the function that we define may be a linear function or nonlinear function today we will talk about linear regression where we use a linear function to in order to fit the training examples that we have got so in linear regression we are given an input x and we have to compute Y and we have training examples which are given to us so we have to find a straight line function so that give an unknown value of the given this value of x we have to find out what is the possible visor given this value of x if you have learnt the Yellow line we will find out this point and from this we can find out what is the value of Y As given by this function for example in linear regression we can try to fit a function from height of a person to the age of a person so that given a height you can predict the edge of the person or you can given the area of a house you can predict the price of the house or given the value of the sensors you want to predict the distance of the sensor from the wall so you are given this blue points and you are trying to fit a straight line function,17_Lecture_05___Linear_Regression.wav
18,linear function has certain parameters as you know that,18_Lecture_05___Linear_Regression.wav
19,align can be characterized by the slope,19_Lecture_05___Linear_Regression.wav
20,and if we extend the Slime plus intercept with the x-axis so we can specify a line by these two parameters the slope and intercept with the x on the y-axis so if it take the equation of the line as y equal to zero plus Bita 1X we can interpret beta zero as the point where it meets the y-axis and Beta 1 as the slope of the line so we want to given the points we want to find the Slime and finding a line means finding the parameters of the line and that are se the parameters of the line aur beta zero Plus,20_Lecture_05___Linear_Regression.wav
21,,21_Lecture_05___Linear_Regression.wav
22,Jamie not exist a complete fit was treated like suppose that the points are generated so we can assume that there is some noise in the data because a data may not be able to fit the data with the straight line so we can assume that there is an error so why you all too beta 0 + beta 1 X + ab silent this is the function from which the points are generated so so we can think of there is the underlying straight line and the points are generated after accounting for some error for example the said trying to predict distance from Sansar observation there may be some error associated with the sensing device and this error you can assume that this error has mean of zero and some standard deviation so we assume that this is the underline function from which the data is generated and given by data points you are trying to find out beta 0 and Beta 1 through which you can specify the equation of the line so beta zero can be beta zero is the y-intercept,22_Lecture_05___Linear_Regression.wav
23,of the population line the population line this is the actual line actually creation through which the data is generated so beta zero is the y-intercept of this actual line and we can say Beta 1 is the slope of the line or we can call that which one is the population slow and its silence is a random,23_Lecture_05___Linear_Regression.wav
24,,24_Lecture_05___Linear_Regression.wav
25,now let's look at an example of a source of this is a linear regression model where we have this relation between the variables which is the linear function Y equal to beta 0 + beta 1 X + b and in the next slide we look at an example of the training set so in this training set we are looking at the price of a house and the size of a house so we are given in this particular case we are given 15 examples for each example have a house number we have the area of the house and we have the selling price of the house and other object would be given the area of the house to predict its selling price so what we could do is that we could plot this so X is the area of the house and why why is the selling price we can plot these functions and we can try to find the fit to this function sau bees 15 points have been plotted on this graph here which the x-axis is the size of the house in terms of hundred square feet this is a 1500 square fit this is 2000 square feet 2500 and this is the price of the house in LAX so given these points we want to find the equation of a line and as we solve the equation of a line means finding the values of beta 0 and with our resources for simple linear regression for multiple linear regression we have in variable and independent variables on the independent variables and we can say that the equation of the line is beta 0 + beta 1 X + beta 2 X square +,25_Lecture_05___Linear_Regression.wav
26,BJP x to the power 3 + this is when we have tea predictor variables copy independent variables so we have P predictor variable,26_Lecture_05___Linear_Regression.wav
27,,27_Lecture_05___Linear_Regression.wav
28,ok so we have to come up with a model for finding out these values of beta 0 beta 1 Beta 2 Beta be accepted and use that so what we're assuming is that the expected value,28_Lecture_05___Linear_Regression.wav
29,of Y given X follows this equation to this equation is the equation of the population line that is the equation from which the examples are actually draw so expected value of Y given x,29_Lecture_05___Linear_Regression.wav
30,is given by the population,30_Lecture_05___Linear_Regression.wav
31,aur because it's Island is a random error and we assume that the mean of a website in his zero we can say that expected value of Y given X is beta 0 + beta 1 in case of linear simple and Babita 0 + beta + beta 2 x square plus beta p x ^ P when we have multiple now given the data points we are trying to find out the equation of the line that is an estimated value of each of these parameters so we are trying to come up with beta zero hat,31_Lecture_05___Linear_Regression.wav
32,beta 1 hat beta Tu hat beta,32_Lecture_05___Linear_Regression.wav
33,so that the equation that we get is like this,33_Lecture_05___Linear_Regression.wav
34,this is the equation that we are trying to come up with as an estimator for the actual function actual target function do the actual target function this is a function that you are trying to come up with and we will try to optimise certain things to come up with this function is Optimisation will be with respect to the training examples that we have so for example we can try to,34_Lecture_05___Linear_Regression.wav
35,find out,35_Lecture_05___Linear_Regression.wav
36,those values of beta 0 Beta 1 b a p so that,36_Lecture_05___Linear_Regression.wav
37,the sum of,37_Lecture_05___Linear_Regression.wav
38,squared error is minimised so if you want to minimise,38_Lecture_05___Linear_Regression.wav
39,the sum of squared errors and paste on that we come up with values of beta 0 hat M Van hath me tattoo Heartbeat AP this particular equation is called the least Airlines so we will see that given the training points how we can come up with the least square,39_Lecture_05___Linear_Regression.wav
40,so let me just love the board,40_Lecture_05___Linear_Regression.wav
41,data that we have may not form a perfect line so what we will do is that we will make some assumptions about the assumptions that,41_Lecture_05___Linear_Regression.wav
42,,42_Lecture_05___Linear_Regression.wav
43,latest test for simplicity take the simple linear regression by equal to M 0 + beta 1 X + a silent and assumption that we make about the error is so we are getting different data points letters a 1D 2D n and corresponding so Deewan we have Y1 equal to beta 0 + beta 1 X 1 + it silent 12 comprises of y2x to where Y2 equal to beta 0 + beta 1 X 2 + excited to show with respect to every example there is a value of the error at LINE 1 excellent to epsolin 300 MB make is that expect value of Epsilon I want to feel that is the errors that is added is has mean of zero want to find the equation so that whatever the residual error that will have a mean of the and letters in the standard deviation of these errors is taken to be sigmoid silent the sigmoid sinus so the errors come from a distribution whose mean is zero and it has a some standard deviation and standard deviation is unknown to us for the assumption that the errors,43_Lecture_05___Linear_Regression.wav
44,,44_Lecture_05___Linear_Regression.wav
45,that is epsolin 112 epsolin and they are independent of each other and we can also assume that these errors are normally distributed,45_Lecture_05___Linear_Regression.wav
46,normally distributed with mean 0 and standard deviation Sigma,46_Lecture_05___Linear_Regression.wav
47,this sort of noise is called gossen noise or white,47_Lecture_05___Linear_Regression.wav
48,now,48_Lecture_05___Linear_Regression.wav
49,that now given the training points the blue hour the training points in this picture we are come up with the line and for that line we can find out what is the sum of squared errors with respect to the blue training points out of all possible lines so the different lines are parameter rise by the values of beta 0 and Vitamin E rich fair values M M M when will have one line we want to find that line for which the sum of squared errors,49_Lecture_05___Linear_Regression.wav
50,square that is called the least squares regression line the least squares regression line give the unique line such that the sum of the squares vertical distances between the data points and the line is the smallest possible so we will find out how to choose this line and that is the algorithm that we will develop so we want a line which so we are given the training points XI,50_Lecture_05___Linear_Regression.wav
51,sodium equal to x i y and we have training points like this and we want to come up with a line so that why are you -,51_Lecture_05___Linear_Regression.wav
52,beta 0 + beta 1 exercise this is so why is the actual that's why I should be the actual value and for a particular value of beta 0 and beta one that we have estimated this is the predicted value so we won't support this particular example the squared error is this right and over all the examples the sum of squared Errors is this so this is for all the included in the trading said this is the sum of the squared errors and we want to minimise please the sum of squared errors given the data points Excel,52_Lecture_05___Linear_Regression.wav
53,this is the Residue that we have when we make this assumption of the values of a beta zero and Vitamin and we want to find beta 0 and beta 1 so that the sum of the squares is a SIM,53_Lecture_05___Linear_Regression.wav
54,find out how to learn the parameters,54_Lecture_05___Linear_Regression.wav
55,how to learn the parameters are the parameters are beta and Vitamin and we want to find out the estimated value of this parameter this problem can be solved in different ways we can find the closed form solution given the training examples we can find a closed-form solution of to get the values of beta 0 Beta 1 in order to satisfy this criteria or we can come up with the iterative so we will look at both in this,55_Lecture_05___Linear_Regression.wav
56,,56_Lecture_05___Linear_Regression.wav
57,2 dimensional problem when X is a single variable we have to beta 0 + beta 1 X and we want to find the values of m 0 unbeatable which minimises the objective function then what we can do is that we can write in order to minimise this function we can use a standard procedure of taking partial derivative of the objective function that the one that we have written on board here we want to if you take the partial derivative of this function with respect to the coefficients between 0 and without oven and said this 205 solving we will get the values of M and B are as given in this light sobita zero is Sigma Y minus beta 16 Mai / n and which oven is and Sigma XI -1 bike Sigma why it's so this division I am not doing in the class but this is what you can do and this is the closed from solution that you can get,57_Lecture_05___Linear_Regression.wav
58,come to multiple linear regression in multiple linear regression you have equal to b l 0 + 1 X 1 + between X and what you can write a text equal to Sigma batayi excite you can hear also find the closed form solution and the closed form solution will involve Matrix operations for the matrix inversion at 17 which are involved and alternative to this is to use some iterative algorithm which will iteratively update the weight by looking at the training example There are several iterative Method 1 popular well known method is the using the Delta rule which is also called the element method is the same method LMS which stands for,58_Lecture_05___Linear_Regression.wav
59,list minimum slope so this element method can be used this element method of The Delta method will update the feature Zero B water weight values to minimise the sum of squared errors so it is the function and we want to learn the parameters are you are so we want to make sure you want to learn a function want to learn why so we want to learn ajax which is better 0 + Sigma batayi Exide and we define a cost function JS heater based on minimising the sum of squared theta is Sigma h x minus y whole square overall the training exam cost function which we want to minimise and this action is a parameter of it is a language and we want to find because it can be taken to minimise this function,59_Lecture_05___Linear_Regression.wav
60,Navin the elements of the what we do is that we start with the initial value of theta that is initial value of M then we take a training example or all training examples and update the values of M ab so as to reduce the sum of squared errors so we can find out what we can do is that this function is actually a convex function and this convex function it's a quadratic function and a quadratic function as a single Optima this case this product function will have a single minimum what we do is that we start at any point in this function space and then we update the values of m 0 Beta 1 so as to,60_Lecture_05___Linear_Regression.wav
61,reduce this value and keep on reducing this will ultimately reach the minimum of the function so the method that we follow is called the gradient Descent method proposes a function which start with some some place here and then what we do is that we find the gradient at this point we find the gradient of the curve at this point and then we take a small step in the negative direction of the gradient colour gradient Descent and we continue doing this with small stand if it is small enough steps ultimately we will go to the minimum of this now so given this function we can take a partial derivative of this function and work out that Del Del beta JJ theta is the gradient of the function and we want to take a small step in the negative direction and of the gradient and Alpha here is the step size Alpha is small and Alpha determines if Alpha is larger we take larger steps if Alpha is small widget smallest so this is the initial value of M and we take a small change we make a small change to beta in the negative direction of the partial derivative of J theta with respect,61_Lecture_05___Linear_Regression.wav
62,and since there is a convex quality function it has a single Global minima and so gradient Descent will eventually converge to the globe,62_Lecture_05___Linear_Regression.wav
63,so the silence update rule if you have a single training example you can work out so you can work out,63_Lecture_05___Linear_Regression.wav
64,as,64_Lecture_05___Linear_Regression.wav
65,and there you cannot use in a week and just look at the slide here and so Delta theta so actually one thing I forgot to tell you that in this function we have put half year easy to see that whatever minimises the rest of the function is also minimises the half of this function and just 1 and half for convenience it's not very important so because of taken half year when we do I take a derivative we can work it out its if you take care make a change of variable x minus y then we can take so this is a z square so this the derivative of its to INR to Z so we have to into half into that x minus y x the partial derivative of x minus,65_Lecture_05___Linear_Regression.wav
66,and Bison ki fine what we get is 8 x minus y into exchange sofa single training example we get the update rule as beta Jay equal to area value of theta + Alpha times the Y - 8 x i x the value of exchange so this is the Delta rule and this Delta rule you can easily interpret is that is why and a check for the same you do not change beta but if they are different suppose why is greater than X then what you have to do you have to increase beta so that it has come closer to Y if Y is smaller than HX you have to decrease beta and that is what the thalamus update role of The Delta rule is to be so you can do this for a single training example or you can update for all the training examples at a time so you find out summation of this and then you taken up update you take all the training examples and update all of them together this is called batch gradient Descent or you can update based on a single example of which is called incremental gradient is so bad gradient descent,66_Lecture_05___Linear_Regression.wav
67,text text the right steps in the right direction but it is very slow because you make one step after processing all the in,67_Lecture_05___Linear_Regression.wav
68,what is usually done is that we use stochastic gradient Descent where we take examples one at a time and make local made changes based on the training example if you do that the entire process is fast and it has also been shown that stochastic gradient Descent has very nice property is so that it does converts and between batch gradient Descent and stochastic gradient Descent we can also have Mini batch gradient Descent when we take few examples at a time and based on that we decide the update of the parameters with this we come to the end of this lecture thank,68_Lecture_05___Linear_Regression.wav
